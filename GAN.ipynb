{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNYtuTEaQgU+/KEjqMatnq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nanashi-bot/GANs/blob/main/GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "KTc-N_gyyBug"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import optim\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining transform\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
      ],
      "metadata": {
        "id": "Wr3DrL_Dy6uw"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing CIFAR-10\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4Rnlo5ZzMaN",
        "outputId": "99f38229-0ec8-4863-ef91-28fd41aa59b9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = next(iter(train_dataloader))"
      ],
      "metadata": {
        "id": "FR-DtaKPOLXz"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPMTGiFJOSH0",
        "outputId": "b9533459-27dc-4fae-89e4-ed3b7c29f6f1"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 3, 32, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(x[0][0].permute(1, 2, 0))\n",
        "plt.axis('off')\n",
        "plt.figure(figsize=(5,3))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "Na143x7HOuoS",
        "outputId": "3a5995d0-71d1-4217-f853-354dea7398c3"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "\n",
        "latent_dim = 100\n",
        "lr = 0.0001\n",
        "beta1 = 0.5\n",
        "beta2 = 0.999\n",
        "num_epochs = 10"
      ],
      "metadata": {
        "id": "2y3lHhqOzfTi"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create generator:\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 256 * 8 * 8),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (256, 8, 8)),\n",
        "            # nn.Upsample(scale_factor = 2),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=5, stride=1, padding=2), # For padding = same we use padding = stride // 2\n",
        "            nn.BatchNorm2d(128, momentum = 0.78),\n",
        "            nn.ReLU(),\n",
        "            # nn.Upsample(scale_factor = 2),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
        "            nn.BatchNorm2d(64, momentum=0.78),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 3, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
        "            nn.Tanh()\n",
        "            )\n",
        "    def forward(self, x):\n",
        "        img = self.model(x)\n",
        "        return img"
      ],
      "metadata": {
        "id": "CNqPHFz0zzSP"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create discriminator\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=5, stride=2, padding=2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # nn.Dropout(0.25),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # nn.ZeroPad2d((0, 1, 0, 1)),\n",
        "            nn.BatchNorm2d(128, momentum=0.82),\n",
        "            # nn.LeakyReLU(0.25),\n",
        "            # nn.Dropout(0.25),\n",
        "            # nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
        "            # nn.BatchNorm2d(128, momentum=0.82),\n",
        "            # nn.LeakyReLU(0.2),\n",
        "            # nn.Dropout(0.25),\n",
        "            # nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            # nn.BatchNorm2d(256, momentum=0.8),\n",
        "            # nn.LeakyReLU(0.25),\n",
        "            # nn.Dropout(0.25),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * 8 * 8, 1),\n",
        "            nn.Sigmoid()\n",
        "            )\n",
        "    def forward(self, img):\n",
        "        validity = self.model(img)\n",
        "        return validity"
      ],
      "metadata": {
        "id": "h71ym7IL0Fyw"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = Generator(latent_dim).to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "# Loss function\n",
        "adversarial_loss = nn.BCELoss()\n",
        "# Optimizers\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, beta2))"
      ],
      "metadata": {
        "id": "LxoDuF1A7kbm"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing discriminator\n",
        "test_input_img = torch.rand(32, 3, 32, 32).to(device)\n",
        "output = discriminator(test_input_img)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "XnPlaoiF7xOl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8adad175-a9b5-4cc5-9748-aaafd899cea8"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing generator\n",
        "test_input_noise = torch.rand(32, latent_dim).to(device)\n",
        "output = generator(test_input_noise)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyNPsiDlVOVX",
        "outputId": "050addbb-55f4-48a8-8196-572d2b23607a"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = adversarial_loss(real_output, torch.ones_like(real_output))\n",
        "    fake_loss = adversarial_loss(fake_output, torch.zeros_like(fake_output))\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "gf3OZR5jV9wd"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generator_loss(fake_output):\n",
        "    return adversarial_loss(fake_output, torch.ones_like(fake_output))"
      ],
      "metadata": {
        "id": "rmF2-XEaW1JH"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(real_images_batch):\n",
        "    batch_size = 32\n",
        "    noise_dim = 100\n",
        "    real_images_batch = next(iter(train_dataloader))\n",
        "    noise = torch.randn(batch_size, noise_dim)\n",
        "\n",
        "    # # Training discriminator\n",
        "    optimizer_D.zero_grad()\n",
        "    fake_images = generator(noise)\n",
        "    real_output = discriminator(real_images_batch[0])\n",
        "    fake_output = discriminator(fake_images.detach())\n",
        "    disc_loss = discriminator_loss(real_output, fake_output)\n",
        "    disc_loss.backward()\n",
        "    optimizer_D.step()\n",
        "\n",
        "    # Training generator\n",
        "    optimizer_G.zero_grad()\n",
        "    generated_images = generator(noise)\n",
        "    fake_output = discriminator(generated_images)\n",
        "    gen_loss = generator_loss(fake_output)\n",
        "    gen_loss.backward()\n",
        "    optimizer_G.step()\n",
        "    print(f\"Generator loss: {gen_loss.item()}, Discriminator loss: {disc_loss.item()}\")\n",
        "    return gen_loss.item(), disc_loss.item()"
      ],
      "metadata": {
        "id": "qEmGbUMXXJIG"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_dataloader, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        for image_batch in train_dataloader:\n",
        "            train_step(image_batch)"
      ],
      "metadata": {
        "id": "jbiFsIJ4W7Su"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(train_dataloader,2)\n",
        "# Takes 32 minutes without gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owc8K10SekDF",
        "outputId": "2acb1f51-b8f5-4a5b-eb42-28f68b502d76"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator loss: 0.5990892052650452, Discriminator loss: 2.0022706985473633\n",
            "Generator loss: 0.5379568338394165, Discriminator loss: 2.0946738719940186\n",
            "Generator loss: 0.5476500391960144, Discriminator loss: 2.1872787475585938\n",
            "Generator loss: 0.46638616919517517, Discriminator loss: 2.4412481784820557\n",
            "Generator loss: 0.50625079870224, Discriminator loss: 2.332075357437134\n",
            "Generator loss: 0.5981793999671936, Discriminator loss: 2.232046127319336\n",
            "Generator loss: 0.7152092456817627, Discriminator loss: 1.940292477607727\n",
            "Generator loss: 0.6282390356063843, Discriminator loss: 2.2394113540649414\n",
            "Generator loss: 0.697138786315918, Discriminator loss: 2.1170458793640137\n",
            "Generator loss: 0.7235879302024841, Discriminator loss: 2.114246368408203\n",
            "Generator loss: 0.7256515026092529, Discriminator loss: 1.9981653690338135\n",
            "Generator loss: 0.7262833118438721, Discriminator loss: 2.0956661701202393\n",
            "Generator loss: 0.7398228645324707, Discriminator loss: 2.003434896469116\n",
            "Generator loss: 0.8194093108177185, Discriminator loss: 2.0170862674713135\n",
            "Generator loss: 0.7709462642669678, Discriminator loss: 2.1205930709838867\n",
            "Generator loss: 0.7915922403335571, Discriminator loss: 2.171729564666748\n",
            "Generator loss: 0.7802680730819702, Discriminator loss: 2.1863479614257812\n",
            "Generator loss: 0.8299328088760376, Discriminator loss: 2.152644157409668\n",
            "Generator loss: 0.7972078323364258, Discriminator loss: 2.25417423248291\n",
            "Generator loss: 0.7827613353729248, Discriminator loss: 2.303199291229248\n",
            "Generator loss: 0.6092017292976379, Discriminator loss: 2.6133785247802734\n",
            "Generator loss: 0.5273285508155823, Discriminator loss: 2.891590118408203\n",
            "Generator loss: 0.5306655168533325, Discriminator loss: 2.807199239730835\n",
            "Generator loss: 0.7496190667152405, Discriminator loss: 2.4723737239837646\n",
            "Generator loss: 0.9968693852424622, Discriminator loss: 2.0391104221343994\n",
            "Generator loss: 1.0496010780334473, Discriminator loss: 1.9594829082489014\n",
            "Generator loss: 1.0709681510925293, Discriminator loss: 1.8934810161590576\n",
            "Generator loss: 0.9284878969192505, Discriminator loss: 2.1174190044403076\n",
            "Generator loss: 0.9541433453559875, Discriminator loss: 2.111241102218628\n",
            "Generator loss: 0.825973391532898, Discriminator loss: 2.4707260131835938\n",
            "Generator loss: 0.8602532744407654, Discriminator loss: 2.3732523918151855\n",
            "Generator loss: 0.7580586671829224, Discriminator loss: 2.6337602138519287\n",
            "Generator loss: 0.8788905143737793, Discriminator loss: 2.3312623500823975\n",
            "Generator loss: 0.9248623847961426, Discriminator loss: 2.3864457607269287\n",
            "Generator loss: 0.8804559707641602, Discriminator loss: 2.391723155975342\n",
            "Generator loss: 0.9268807768821716, Discriminator loss: 2.3461503982543945\n",
            "Generator loss: 0.9550060629844666, Discriminator loss: 2.248680353164673\n",
            "Generator loss: 1.075029969215393, Discriminator loss: 2.1736464500427246\n",
            "Generator loss: 1.0518488883972168, Discriminator loss: 2.1178200244903564\n",
            "Generator loss: 1.0472444295883179, Discriminator loss: 2.1999998092651367\n",
            "Generator loss: 0.9989722967147827, Discriminator loss: 2.2878565788269043\n",
            "Generator loss: 1.0033681392669678, Discriminator loss: 2.225029468536377\n",
            "Generator loss: 1.0238986015319824, Discriminator loss: 2.3592586517333984\n",
            "Generator loss: 1.0587507486343384, Discriminator loss: 2.260371446609497\n",
            "Generator loss: 1.1993523836135864, Discriminator loss: 2.068114995956421\n",
            "Generator loss: 1.2382152080535889, Discriminator loss: 1.939096212387085\n",
            "Generator loss: 1.2270421981811523, Discriminator loss: 1.9128026962280273\n",
            "Generator loss: 1.0352038145065308, Discriminator loss: 2.277719497680664\n",
            "Generator loss: 0.9212032556533813, Discriminator loss: 2.4480443000793457\n",
            "Generator loss: 0.8982706069946289, Discriminator loss: 2.510094404220581\n",
            "Generator loss: 0.9176934957504272, Discriminator loss: 2.513840913772583\n",
            "Generator loss: 1.0678715705871582, Discriminator loss: 2.2297523021698\n",
            "Generator loss: 1.2512214183807373, Discriminator loss: 2.0422472953796387\n",
            "Generator loss: 1.2839783430099487, Discriminator loss: 1.8734166622161865\n",
            "Generator loss: 1.4046218395233154, Discriminator loss: 1.7608177661895752\n",
            "Generator loss: 1.3031926155090332, Discriminator loss: 1.8638523817062378\n",
            "Generator loss: 1.3054089546203613, Discriminator loss: 1.812587022781372\n",
            "Generator loss: 1.2085058689117432, Discriminator loss: 1.8859007358551025\n",
            "Generator loss: 1.320499062538147, Discriminator loss: 1.895644187927246\n",
            "Generator loss: 1.4254592657089233, Discriminator loss: 1.7247695922851562\n",
            "Generator loss: 1.473275065422058, Discriminator loss: 1.7121813297271729\n",
            "Generator loss: 1.3478403091430664, Discriminator loss: 1.9306739568710327\n",
            "Generator loss: 1.2221662998199463, Discriminator loss: 2.0865421295166016\n",
            "Generator loss: 1.1643977165222168, Discriminator loss: 2.349384307861328\n",
            "Generator loss: 1.374819040298462, Discriminator loss: 2.0752346515655518\n",
            "Generator loss: 1.5380195379257202, Discriminator loss: 1.8945001363754272\n",
            "Generator loss: 1.6250253915786743, Discriminator loss: 1.8078995943069458\n",
            "Generator loss: 1.6818366050720215, Discriminator loss: 1.6874130964279175\n",
            "Generator loss: 1.667765736579895, Discriminator loss: 1.676497459411621\n",
            "Generator loss: 1.5997118949890137, Discriminator loss: 1.6453839540481567\n",
            "Generator loss: 1.560279130935669, Discriminator loss: 1.7194972038269043\n",
            "Generator loss: 1.6230897903442383, Discriminator loss: 1.7827599048614502\n",
            "Generator loss: 1.80859375, Discriminator loss: 1.5166919231414795\n",
            "Generator loss: 1.775073528289795, Discriminator loss: 1.3592963218688965\n",
            "Generator loss: 1.6154662370681763, Discriminator loss: 1.627349853515625\n",
            "Generator loss: 1.6281983852386475, Discriminator loss: 1.684507966041565\n",
            "Generator loss: 1.733575701713562, Discriminator loss: 1.5106159448623657\n",
            "Generator loss: 1.7069233655929565, Discriminator loss: 1.5943825244903564\n",
            "Generator loss: 1.6116923093795776, Discriminator loss: 1.8483397960662842\n",
            "Generator loss: 1.5639640092849731, Discriminator loss: 1.865649938583374\n",
            "Generator loss: 1.6055988073349, Discriminator loss: 1.7697666883468628\n",
            "Generator loss: 1.507723331451416, Discriminator loss: 1.907036542892456\n",
            "Generator loss: 1.4416073560714722, Discriminator loss: 1.9027988910675049\n",
            "Generator loss: 1.5294568538665771, Discriminator loss: 1.8566519021987915\n",
            "Generator loss: 1.8150681257247925, Discriminator loss: 1.4721108675003052\n",
            "Generator loss: 1.9282052516937256, Discriminator loss: 1.282771348953247\n",
            "Generator loss: 1.89714777469635, Discriminator loss: 1.249794602394104\n",
            "Generator loss: 1.7864104509353638, Discriminator loss: 1.3102123737335205\n",
            "Generator loss: 1.6722121238708496, Discriminator loss: 1.5636147260665894\n",
            "Generator loss: 1.7771177291870117, Discriminator loss: 1.4550940990447998\n",
            "Generator loss: 1.8321642875671387, Discriminator loss: 1.4036718606948853\n",
            "Generator loss: 1.839970588684082, Discriminator loss: 1.349210262298584\n",
            "Generator loss: 1.714048981666565, Discriminator loss: 1.55245041847229\n",
            "Generator loss: 1.7269378900527954, Discriminator loss: 1.4527596235275269\n",
            "Generator loss: 1.798302173614502, Discriminator loss: 1.4902673959732056\n",
            "Generator loss: 1.620521068572998, Discriminator loss: 1.617724061012268\n",
            "Generator loss: 1.5348809957504272, Discriminator loss: 1.797739028930664\n",
            "Generator loss: 1.5874996185302734, Discriminator loss: 1.8213927745819092\n",
            "Generator loss: 1.6524263620376587, Discriminator loss: 1.6337902545928955\n",
            "Generator loss: 1.7244758605957031, Discriminator loss: 1.497984528541565\n",
            "Generator loss: 1.8136879205703735, Discriminator loss: 1.452298641204834\n",
            "Generator loss: 1.9070533514022827, Discriminator loss: 1.2489686012268066\n",
            "Generator loss: 1.9585349559783936, Discriminator loss: 1.1962916851043701\n",
            "Generator loss: 1.9612468481063843, Discriminator loss: 1.1013753414154053\n",
            "Generator loss: 1.9266512393951416, Discriminator loss: 1.1344783306121826\n",
            "Generator loss: 1.863052487373352, Discriminator loss: 1.2095295190811157\n",
            "Generator loss: 1.906721830368042, Discriminator loss: 1.2065279483795166\n",
            "Generator loss: 1.9430121183395386, Discriminator loss: 1.2072467803955078\n",
            "Generator loss: 1.891615629196167, Discriminator loss: 1.192842721939087\n",
            "Generator loss: 1.974199652671814, Discriminator loss: 1.2613003253936768\n",
            "Generator loss: 1.9981307983398438, Discriminator loss: 1.1433722972869873\n",
            "Generator loss: 1.9677484035491943, Discriminator loss: 1.131111741065979\n",
            "Generator loss: 1.9116874933242798, Discriminator loss: 1.3492379188537598\n",
            "Generator loss: 1.8775159120559692, Discriminator loss: 1.3620128631591797\n",
            "Generator loss: 1.9071252346038818, Discriminator loss: 1.3546069860458374\n",
            "Generator loss: 1.9189491271972656, Discriminator loss: 1.37485671043396\n",
            "Generator loss: 1.908400535583496, Discriminator loss: 1.275270938873291\n",
            "Generator loss: 2.010185718536377, Discriminator loss: 1.280616044998169\n",
            "Generator loss: 2.0381500720977783, Discriminator loss: 1.0983829498291016\n",
            "Generator loss: 2.019242286682129, Discriminator loss: 1.1933536529541016\n",
            "Generator loss: 1.9294402599334717, Discriminator loss: 1.3636023998260498\n",
            "Generator loss: 1.7981343269348145, Discriminator loss: 1.6775524616241455\n",
            "Generator loss: 1.9542739391326904, Discriminator loss: 1.581913709640503\n",
            "Generator loss: 2.119405746459961, Discriminator loss: 1.40298593044281\n",
            "Generator loss: 2.1059212684631348, Discriminator loss: 1.2683018445968628\n",
            "Generator loss: 1.909306287765503, Discriminator loss: 1.394228219985962\n",
            "Generator loss: 1.817889928817749, Discriminator loss: 1.5278600454330444\n",
            "Generator loss: 1.8205060958862305, Discriminator loss: 1.5661766529083252\n",
            "Generator loss: 1.8219423294067383, Discriminator loss: 1.424420952796936\n",
            "Generator loss: 1.9551408290863037, Discriminator loss: 1.2652651071548462\n",
            "Generator loss: 2.059309482574463, Discriminator loss: 1.141862154006958\n",
            "Generator loss: 2.0332531929016113, Discriminator loss: 1.0809144973754883\n",
            "Generator loss: 1.9767075777053833, Discriminator loss: 1.256990671157837\n",
            "Generator loss: 1.828228235244751, Discriminator loss: 1.4266449213027954\n",
            "Generator loss: 1.8305186033248901, Discriminator loss: 1.629817247390747\n",
            "Generator loss: 1.9637844562530518, Discriminator loss: 1.549952507019043\n",
            "Generator loss: 2.11479115486145, Discriminator loss: 1.3293213844299316\n",
            "Generator loss: 2.194556713104248, Discriminator loss: 1.1929223537445068\n",
            "Generator loss: 2.150095224380493, Discriminator loss: 1.2571297883987427\n",
            "Generator loss: 2.216228723526001, Discriminator loss: 1.0874935388565063\n",
            "Generator loss: 2.1961443424224854, Discriminator loss: 1.0088415145874023\n",
            "Generator loss: 2.2408645153045654, Discriminator loss: 0.8930891156196594\n",
            "Generator loss: 2.253804922103882, Discriminator loss: 0.8178786039352417\n",
            "Generator loss: 2.139746904373169, Discriminator loss: 0.9131428003311157\n",
            "Generator loss: 2.08012056350708, Discriminator loss: 0.8632173538208008\n",
            "Generator loss: 2.042837142944336, Discriminator loss: 1.014868974685669\n",
            "Generator loss: 2.139636516571045, Discriminator loss: 0.9852429628372192\n",
            "Generator loss: 2.2066874504089355, Discriminator loss: 0.9772868156433105\n",
            "Generator loss: 2.356882095336914, Discriminator loss: 0.7784871459007263\n",
            "Generator loss: 2.282839298248291, Discriminator loss: 0.8060290813446045\n",
            "Generator loss: 2.138382911682129, Discriminator loss: 1.0433666706085205\n",
            "Generator loss: 2.0757269859313965, Discriminator loss: 1.0619654655456543\n",
            "Generator loss: 2.193603515625, Discriminator loss: 1.1235322952270508\n",
            "Generator loss: 2.3156614303588867, Discriminator loss: 0.9156498908996582\n",
            "Generator loss: 2.314772844314575, Discriminator loss: 0.9552432894706726\n",
            "Generator loss: 2.267378807067871, Discriminator loss: 0.9956613183021545\n",
            "Generator loss: 2.298137664794922, Discriminator loss: 1.0244202613830566\n",
            "Generator loss: 2.4253621101379395, Discriminator loss: 0.8321080207824707\n",
            "Generator loss: 2.3488640785217285, Discriminator loss: 0.9123480319976807\n",
            "Generator loss: 2.3816299438476562, Discriminator loss: 0.8261860609054565\n",
            "Generator loss: 2.348170518875122, Discriminator loss: 0.857279360294342\n",
            "Generator loss: 2.372915744781494, Discriminator loss: 0.9231001138687134\n",
            "Generator loss: 2.3480825424194336, Discriminator loss: 1.0659363269805908\n",
            "Generator loss: 2.5152807235717773, Discriminator loss: 0.987973153591156\n",
            "Generator loss: 2.640148401260376, Discriminator loss: 0.7392908930778503\n",
            "Generator loss: 2.4904768466949463, Discriminator loss: 0.7733020186424255\n",
            "Generator loss: 2.4388833045959473, Discriminator loss: 0.7863712310791016\n",
            "Generator loss: 2.466191053390503, Discriminator loss: 0.8879889845848083\n",
            "Generator loss: 2.48345685005188, Discriminator loss: 0.7733908891677856\n",
            "Generator loss: 2.4834237098693848, Discriminator loss: 0.8036059141159058\n",
            "Generator loss: 2.4098353385925293, Discriminator loss: 0.8435538411140442\n",
            "Generator loss: 2.35538649559021, Discriminator loss: 0.9632216691970825\n",
            "Generator loss: 2.3303961753845215, Discriminator loss: 0.9643593430519104\n",
            "Generator loss: 2.4931888580322266, Discriminator loss: 0.9232535362243652\n",
            "Generator loss: 2.645101308822632, Discriminator loss: 0.7639597058296204\n",
            "Generator loss: 2.632770538330078, Discriminator loss: 0.6018401980400085\n",
            "Generator loss: 2.5501976013183594, Discriminator loss: 0.6805603504180908\n",
            "Generator loss: 2.396047592163086, Discriminator loss: 0.7351900935173035\n",
            "Generator loss: 2.37846302986145, Discriminator loss: 0.7393754720687866\n",
            "Generator loss: 2.331186532974243, Discriminator loss: 0.7783606052398682\n",
            "Generator loss: 2.261789321899414, Discriminator loss: 1.0131272077560425\n",
            "Generator loss: 2.324347496032715, Discriminator loss: 1.0559017658233643\n",
            "Generator loss: 2.3651013374328613, Discriminator loss: 1.044465184211731\n",
            "Generator loss: 2.530879020690918, Discriminator loss: 0.9991307258605957\n",
            "Generator loss: 2.7279601097106934, Discriminator loss: 0.7043112516403198\n",
            "Generator loss: 2.630946397781372, Discriminator loss: 0.6962577104568481\n",
            "Generator loss: 2.425907611846924, Discriminator loss: 0.8433669805526733\n",
            "Generator loss: 2.2597808837890625, Discriminator loss: 1.0157328844070435\n",
            "Generator loss: 2.2941412925720215, Discriminator loss: 1.1633738279342651\n",
            "Generator loss: 2.5096652507781982, Discriminator loss: 1.1405651569366455\n",
            "Generator loss: 2.598675489425659, Discriminator loss: 0.9807816743850708\n",
            "Generator loss: 2.6021018028259277, Discriminator loss: 1.0002431869506836\n",
            "Generator loss: 2.5669095516204834, Discriminator loss: 1.0143758058547974\n",
            "Generator loss: 2.4976041316986084, Discriminator loss: 1.0362612009048462\n",
            "Generator loss: 2.3939578533172607, Discriminator loss: 1.0859594345092773\n",
            "Generator loss: 2.406062602996826, Discriminator loss: 1.2261652946472168\n",
            "Generator loss: 2.3862545490264893, Discriminator loss: 1.3137776851654053\n",
            "Generator loss: 2.438509941101074, Discriminator loss: 1.114351749420166\n",
            "Generator loss: 2.4389286041259766, Discriminator loss: 1.0792946815490723\n",
            "Generator loss: 2.5191054344177246, Discriminator loss: 0.9534608125686646\n",
            "Generator loss: 2.5682077407836914, Discriminator loss: 0.8188069462776184\n",
            "Generator loss: 2.5908284187316895, Discriminator loss: 0.6841293573379517\n",
            "Generator loss: 2.5001378059387207, Discriminator loss: 0.6738364100456238\n",
            "Generator loss: 2.370847225189209, Discriminator loss: 0.6245056986808777\n",
            "Generator loss: 2.3603405952453613, Discriminator loss: 0.6300831437110901\n",
            "Generator loss: 2.3434126377105713, Discriminator loss: 0.7366460561752319\n",
            "Generator loss: 2.3783843517303467, Discriminator loss: 0.7321895360946655\n",
            "Generator loss: 2.3873980045318604, Discriminator loss: 0.618880033493042\n",
            "Generator loss: 2.427532196044922, Discriminator loss: 0.6617648005485535\n",
            "Generator loss: 2.4260988235473633, Discriminator loss: 0.7502634525299072\n",
            "Generator loss: 2.3740131855010986, Discriminator loss: 0.7421701550483704\n",
            "Generator loss: 2.467313766479492, Discriminator loss: 0.6650797724723816\n",
            "Generator loss: 2.4420878887176514, Discriminator loss: 0.7482519149780273\n",
            "Generator loss: 2.4995169639587402, Discriminator loss: 0.8977315425872803\n",
            "Generator loss: 2.6118342876434326, Discriminator loss: 0.7313500046730042\n",
            "Generator loss: 2.580913543701172, Discriminator loss: 0.8797098994255066\n",
            "Generator loss: 2.6075363159179688, Discriminator loss: 0.7403949499130249\n",
            "Generator loss: 2.6235151290893555, Discriminator loss: 0.6584165096282959\n",
            "Generator loss: 2.5554046630859375, Discriminator loss: 0.7361632585525513\n",
            "Generator loss: 2.5004260540008545, Discriminator loss: 0.762298047542572\n",
            "Generator loss: 2.512228488922119, Discriminator loss: 0.787726640701294\n",
            "Generator loss: 2.5385141372680664, Discriminator loss: 0.7774835824966431\n",
            "Generator loss: 2.623749256134033, Discriminator loss: 0.7263929843902588\n",
            "Generator loss: 2.7291440963745117, Discriminator loss: 0.680493950843811\n",
            "Generator loss: 2.662250280380249, Discriminator loss: 0.6677271127700806\n",
            "Generator loss: 2.5853116512298584, Discriminator loss: 0.6524511575698853\n",
            "Generator loss: 2.594808578491211, Discriminator loss: 0.6776631474494934\n",
            "Generator loss: 2.7003140449523926, Discriminator loss: 0.5567524433135986\n",
            "Generator loss: 2.7535295486450195, Discriminator loss: 0.5649975538253784\n",
            "Generator loss: 2.7406585216522217, Discriminator loss: 0.43560469150543213\n",
            "Generator loss: 2.664468288421631, Discriminator loss: 0.6267909407615662\n",
            "Generator loss: 2.6227636337280273, Discriminator loss: 0.5349842309951782\n",
            "Generator loss: 2.5371313095092773, Discriminator loss: 0.6185282468795776\n",
            "Generator loss: 2.602242946624756, Discriminator loss: 0.7121443152427673\n",
            "Generator loss: 2.717588424682617, Discriminator loss: 0.7605339288711548\n",
            "Generator loss: 2.7456400394439697, Discriminator loss: 0.7733038663864136\n",
            "Generator loss: 2.745030641555786, Discriminator loss: 0.7742490768432617\n",
            "Generator loss: 2.8103129863739014, Discriminator loss: 0.7018026113510132\n",
            "Generator loss: 2.755289077758789, Discriminator loss: 0.6965188980102539\n",
            "Generator loss: 2.7055768966674805, Discriminator loss: 0.7182928323745728\n",
            "Generator loss: 2.717576503753662, Discriminator loss: 0.7120272517204285\n",
            "Generator loss: 2.808574676513672, Discriminator loss: 0.7198413610458374\n",
            "Generator loss: 2.8220109939575195, Discriminator loss: 0.7439229488372803\n",
            "Generator loss: 2.8234407901763916, Discriminator loss: 0.6495628356933594\n",
            "Generator loss: 2.7877583503723145, Discriminator loss: 0.6744862794876099\n",
            "Generator loss: 2.7835774421691895, Discriminator loss: 0.6230504512786865\n",
            "Generator loss: 2.745327949523926, Discriminator loss: 0.7031483054161072\n",
            "Generator loss: 2.8385934829711914, Discriminator loss: 0.6694302558898926\n",
            "Generator loss: 2.9390101432800293, Discriminator loss: 0.6157914996147156\n",
            "Generator loss: 2.926938533782959, Discriminator loss: 0.5414696931838989\n",
            "Generator loss: 2.8426289558410645, Discriminator loss: 0.5312422513961792\n",
            "Generator loss: 2.851384401321411, Discriminator loss: 0.4455033540725708\n",
            "Generator loss: 2.6834635734558105, Discriminator loss: 0.5897908806800842\n",
            "Generator loss: 2.7225961685180664, Discriminator loss: 0.6468839049339294\n",
            "Generator loss: 2.8461194038391113, Discriminator loss: 0.5619669556617737\n",
            "Generator loss: 2.8147850036621094, Discriminator loss: 0.46861404180526733\n",
            "Generator loss: 2.830113410949707, Discriminator loss: 0.4763740003108978\n",
            "Generator loss: 2.7147204875946045, Discriminator loss: 0.48724716901779175\n",
            "Generator loss: 2.66312837600708, Discriminator loss: 0.4401269555091858\n",
            "Generator loss: 2.6363039016723633, Discriminator loss: 0.5500405430793762\n",
            "Generator loss: 2.6701900959014893, Discriminator loss: 0.5760927200317383\n",
            "Generator loss: 2.723949432373047, Discriminator loss: 0.5613813996315002\n",
            "Generator loss: 2.838531017303467, Discriminator loss: 0.5694003105163574\n",
            "Generator loss: 2.8741695880889893, Discriminator loss: 0.4988292455673218\n",
            "Generator loss: 2.888354539871216, Discriminator loss: 0.5661208033561707\n",
            "Generator loss: 2.8361880779266357, Discriminator loss: 0.4962020814418793\n",
            "Generator loss: 2.7455034255981445, Discriminator loss: 0.5251476168632507\n",
            "Generator loss: 2.6637611389160156, Discriminator loss: 0.5988420248031616\n",
            "Generator loss: 2.6880569458007812, Discriminator loss: 0.7428221702575684\n",
            "Generator loss: 2.7862918376922607, Discriminator loss: 0.8083040118217468\n",
            "Generator loss: 2.809690475463867, Discriminator loss: 0.7919361591339111\n",
            "Generator loss: 2.961998462677002, Discriminator loss: 0.6432386636734009\n",
            "Generator loss: 2.9816365242004395, Discriminator loss: 0.6028668880462646\n",
            "Generator loss: 2.9218027591705322, Discriminator loss: 0.46021032333374023\n",
            "Generator loss: 2.8025693893432617, Discriminator loss: 0.47575467824935913\n",
            "Generator loss: 2.7283496856689453, Discriminator loss: 0.5223749279975891\n",
            "Generator loss: 2.723397731781006, Discriminator loss: 0.4910401403903961\n",
            "Generator loss: 2.7894155979156494, Discriminator loss: 0.6948512196540833\n",
            "Generator loss: 2.9035191535949707, Discriminator loss: 0.6596819162368774\n",
            "Generator loss: 2.9787285327911377, Discriminator loss: 0.6172341108322144\n",
            "Generator loss: 3.071708917617798, Discriminator loss: 0.5776574611663818\n",
            "Generator loss: 3.15181565284729, Discriminator loss: 0.6532522439956665\n",
            "Generator loss: 3.163839817047119, Discriminator loss: 0.7751071453094482\n",
            "Generator loss: 3.22916316986084, Discriminator loss: 0.6880371570587158\n",
            "Generator loss: 3.3108270168304443, Discriminator loss: 0.5350786447525024\n",
            "Generator loss: 3.192279815673828, Discriminator loss: 0.5722262859344482\n",
            "Generator loss: 3.1293179988861084, Discriminator loss: 0.6117760539054871\n",
            "Generator loss: 3.1045713424682617, Discriminator loss: 0.5487116575241089\n",
            "Generator loss: 3.0664541721343994, Discriminator loss: 0.4858500361442566\n",
            "Generator loss: 2.957707643508911, Discriminator loss: 0.47178220748901367\n",
            "Generator loss: 2.9377076625823975, Discriminator loss: 0.48157745599746704\n",
            "Generator loss: 2.9411120414733887, Discriminator loss: 0.4481346011161804\n",
            "Generator loss: 2.9359703063964844, Discriminator loss: 0.41553834080696106\n",
            "Generator loss: 2.963444948196411, Discriminator loss: 0.42025119066238403\n",
            "Generator loss: 3.0013070106506348, Discriminator loss: 0.4003679156303406\n",
            "Generator loss: 2.909182548522949, Discriminator loss: 0.4698537588119507\n",
            "Generator loss: 2.9321181774139404, Discriminator loss: 0.4402223527431488\n",
            "Generator loss: 2.9372849464416504, Discriminator loss: 0.41561558842658997\n",
            "Generator loss: 2.828669548034668, Discriminator loss: 0.3956165909767151\n",
            "Generator loss: 2.7722055912017822, Discriminator loss: 0.5434224605560303\n",
            "Generator loss: 2.7372641563415527, Discriminator loss: 0.48478007316589355\n",
            "Generator loss: 2.7553741931915283, Discriminator loss: 0.5320764183998108\n",
            "Generator loss: 2.8754889965057373, Discriminator loss: 0.5444562435150146\n",
            "Generator loss: 2.879539966583252, Discriminator loss: 0.5586580634117126\n",
            "Generator loss: 2.939152240753174, Discriminator loss: 0.5067911744117737\n",
            "Generator loss: 3.000802516937256, Discriminator loss: 0.5901819467544556\n",
            "Generator loss: 3.125606060028076, Discriminator loss: 0.568274199962616\n",
            "Generator loss: 3.165067195892334, Discriminator loss: 0.45598143339157104\n",
            "Generator loss: 3.0797553062438965, Discriminator loss: 0.481112539768219\n",
            "Generator loss: 2.9700660705566406, Discriminator loss: 0.45913565158843994\n",
            "Generator loss: 2.9470136165618896, Discriminator loss: 0.47948339581489563\n",
            "Generator loss: 2.9970755577087402, Discriminator loss: 0.6457502245903015\n",
            "Generator loss: 3.092719793319702, Discriminator loss: 0.6907175779342651\n",
            "Generator loss: 3.1854825019836426, Discriminator loss: 0.592027485370636\n",
            "Generator loss: 3.259619951248169, Discriminator loss: 0.5575304627418518\n",
            "Generator loss: 3.186610698699951, Discriminator loss: 0.5517818927764893\n",
            "Generator loss: 3.0799875259399414, Discriminator loss: 0.7219681739807129\n",
            "Generator loss: 3.1596524715423584, Discriminator loss: 0.6944054365158081\n",
            "Generator loss: 3.1706438064575195, Discriminator loss: 0.5364500284194946\n",
            "Generator loss: 3.10567045211792, Discriminator loss: 0.5924512147903442\n",
            "Generator loss: 3.0651333332061768, Discriminator loss: 0.6270512342453003\n",
            "Generator loss: 3.1098721027374268, Discriminator loss: 0.754816472530365\n",
            "Generator loss: 3.358630895614624, Discriminator loss: 0.602952241897583\n",
            "Generator loss: 3.2704408168792725, Discriminator loss: 0.606297492980957\n",
            "Generator loss: 3.240373373031616, Discriminator loss: 0.6037854552268982\n",
            "Generator loss: 3.1168973445892334, Discriminator loss: 0.7653339505195618\n",
            "Generator loss: 3.1146509647369385, Discriminator loss: 0.8055480718612671\n",
            "Generator loss: 3.1343331336975098, Discriminator loss: 0.8172797560691833\n",
            "Generator loss: 3.177440643310547, Discriminator loss: 0.5540628433227539\n",
            "Generator loss: 3.164278984069824, Discriminator loss: 0.4656605124473572\n",
            "Generator loss: 3.039827823638916, Discriminator loss: 0.3843669891357422\n",
            "Generator loss: 2.8502373695373535, Discriminator loss: 0.5341266989707947\n",
            "Generator loss: 2.812326431274414, Discriminator loss: 0.5358981490135193\n",
            "Generator loss: 2.8772056102752686, Discriminator loss: 0.4460589587688446\n",
            "Generator loss: 2.866302251815796, Discriminator loss: 0.590114951133728\n",
            "Generator loss: 2.8994438648223877, Discriminator loss: 0.5965944528579712\n",
            "Generator loss: 2.9945781230926514, Discriminator loss: 0.5046064853668213\n",
            "Generator loss: 3.013112783432007, Discriminator loss: 0.5980446934700012\n",
            "Generator loss: 3.034335136413574, Discriminator loss: 0.5903103947639465\n",
            "Generator loss: 3.0915985107421875, Discriminator loss: 0.5094730854034424\n",
            "Generator loss: 3.1817963123321533, Discriminator loss: 0.4539833664894104\n",
            "Generator loss: 3.142557144165039, Discriminator loss: 0.43137747049331665\n",
            "Generator loss: 2.981456995010376, Discriminator loss: 0.4127630591392517\n",
            "Generator loss: 2.896962881088257, Discriminator loss: 0.5075032114982605\n",
            "Generator loss: 2.9663126468658447, Discriminator loss: 0.48008888959884644\n",
            "Generator loss: 2.927227020263672, Discriminator loss: 0.5519149899482727\n",
            "Generator loss: 2.9730920791625977, Discriminator loss: 0.5213935375213623\n",
            "Generator loss: 2.998866081237793, Discriminator loss: 0.5056866407394409\n",
            "Generator loss: 3.0061397552490234, Discriminator loss: 0.44838953018188477\n",
            "Generator loss: 2.901880979537964, Discriminator loss: 0.45316311717033386\n",
            "Generator loss: 2.9173290729522705, Discriminator loss: 0.38619861006736755\n",
            "Generator loss: 2.9107325077056885, Discriminator loss: 0.4358471632003784\n",
            "Generator loss: 2.9325623512268066, Discriminator loss: 0.5345145463943481\n",
            "Generator loss: 2.976470947265625, Discriminator loss: 0.5208407640457153\n",
            "Generator loss: 3.0774989128112793, Discriminator loss: 0.5037193298339844\n",
            "Generator loss: 3.0874063968658447, Discriminator loss: 0.4465985894203186\n",
            "Generator loss: 3.0983264446258545, Discriminator loss: 0.35703301429748535\n",
            "Generator loss: 2.9422616958618164, Discriminator loss: 0.3725544214248657\n",
            "Generator loss: 2.801011323928833, Discriminator loss: 0.5229393839836121\n",
            "Generator loss: 2.859100341796875, Discriminator loss: 0.5406786203384399\n",
            "Generator loss: 3.0131092071533203, Discriminator loss: 0.5968421101570129\n",
            "Generator loss: 3.154710292816162, Discriminator loss: 0.47817689180374146\n",
            "Generator loss: 3.136061668395996, Discriminator loss: 0.38809847831726074\n",
            "Generator loss: 3.0653250217437744, Discriminator loss: 0.3585103154182434\n",
            "Generator loss: 3.002310037612915, Discriminator loss: 0.35690274834632874\n",
            "Generator loss: 2.9590041637420654, Discriminator loss: 0.2567387521266937\n",
            "Generator loss: 2.9003281593322754, Discriminator loss: 0.38680943846702576\n",
            "Generator loss: 2.8308351039886475, Discriminator loss: 0.3708496689796448\n",
            "Generator loss: 2.842930316925049, Discriminator loss: 0.5029513239860535\n",
            "Generator loss: 2.9073779582977295, Discriminator loss: 0.3743477463722229\n",
            "Generator loss: 3.0490682125091553, Discriminator loss: 0.3127436637878418\n",
            "Generator loss: 3.0788590908050537, Discriminator loss: 0.32951781153678894\n",
            "Generator loss: 3.119981050491333, Discriminator loss: 0.3129293918609619\n",
            "Generator loss: 3.159120798110962, Discriminator loss: 0.3350680470466614\n",
            "Generator loss: 3.0494778156280518, Discriminator loss: 0.34168684482574463\n",
            "Generator loss: 3.072540760040283, Discriminator loss: 0.34274822473526\n",
            "Generator loss: 3.0134334564208984, Discriminator loss: 0.34398528933525085\n",
            "Generator loss: 2.9597434997558594, Discriminator loss: 0.37807971239089966\n",
            "Generator loss: 2.9278688430786133, Discriminator loss: 0.4493315815925598\n",
            "Generator loss: 3.0548980236053467, Discriminator loss: 0.545173168182373\n",
            "Generator loss: 3.2075600624084473, Discriminator loss: 0.5288132429122925\n",
            "Generator loss: 3.286762237548828, Discriminator loss: 0.5059763789176941\n",
            "Generator loss: 3.306589365005493, Discriminator loss: 0.38908445835113525\n",
            "Generator loss: 3.1979188919067383, Discriminator loss: 0.42127013206481934\n",
            "Generator loss: 3.107731342315674, Discriminator loss: 0.5201693177223206\n",
            "Generator loss: 3.1751489639282227, Discriminator loss: 0.67429518699646\n",
            "Generator loss: 3.3640568256378174, Discriminator loss: 0.7669475078582764\n",
            "Generator loss: 3.4849936962127686, Discriminator loss: 0.6200369000434875\n",
            "Generator loss: 3.443027973175049, Discriminator loss: 0.5358492136001587\n",
            "Generator loss: 3.342205286026001, Discriminator loss: 0.44654080271720886\n",
            "Generator loss: 3.181548833847046, Discriminator loss: 0.49864861369132996\n",
            "Generator loss: 3.1673166751861572, Discriminator loss: 0.5503873229026794\n",
            "Generator loss: 3.255666494369507, Discriminator loss: 0.4580784738063812\n",
            "Generator loss: 3.2202415466308594, Discriminator loss: 0.4046463072299957\n",
            "Generator loss: 3.1796960830688477, Discriminator loss: 0.3742627501487732\n",
            "Generator loss: 3.0252723693847656, Discriminator loss: 0.4436178207397461\n",
            "Generator loss: 3.013030529022217, Discriminator loss: 0.41789692640304565\n",
            "Generator loss: 3.1358184814453125, Discriminator loss: 0.40748685598373413\n",
            "Generator loss: 3.187110424041748, Discriminator loss: 0.42904964089393616\n",
            "Generator loss: 3.2295069694519043, Discriminator loss: 0.40560221672058105\n",
            "Generator loss: 3.247767448425293, Discriminator loss: 0.44787073135375977\n",
            "Generator loss: 3.2650647163391113, Discriminator loss: 0.4884498119354248\n",
            "Generator loss: 3.322699546813965, Discriminator loss: 0.42107197642326355\n",
            "Generator loss: 3.3434321880340576, Discriminator loss: 0.39266735315322876\n",
            "Generator loss: 3.298640012741089, Discriminator loss: 0.4463299512863159\n",
            "Generator loss: 3.3113150596618652, Discriminator loss: 0.48930835723876953\n",
            "Generator loss: 3.314673900604248, Discriminator loss: 0.4588276147842407\n",
            "Generator loss: 3.35048770904541, Discriminator loss: 0.43038904666900635\n",
            "Generator loss: 3.310023546218872, Discriminator loss: 0.390713632106781\n",
            "Generator loss: 3.220909833908081, Discriminator loss: 0.41838544607162476\n",
            "Generator loss: 3.1870834827423096, Discriminator loss: 0.4355681240558624\n",
            "Generator loss: 3.2475340366363525, Discriminator loss: 0.4464193284511566\n",
            "Generator loss: 3.283843517303467, Discriminator loss: 0.6024437546730042\n",
            "Generator loss: 3.4665284156799316, Discriminator loss: 0.5748525261878967\n",
            "Generator loss: 3.558387279510498, Discriminator loss: 0.4008464217185974\n",
            "Generator loss: 3.471776008605957, Discriminator loss: 0.39117538928985596\n",
            "Generator loss: 3.288821220397949, Discriminator loss: 0.34282952547073364\n",
            "Generator loss: 3.1689395904541016, Discriminator loss: 0.4172932803630829\n",
            "Generator loss: 3.1718533039093018, Discriminator loss: 0.42966949939727783\n",
            "Generator loss: 3.1994707584381104, Discriminator loss: 0.4447787404060364\n",
            "Generator loss: 3.2031519412994385, Discriminator loss: 0.44713008403778076\n",
            "Generator loss: 3.288503885269165, Discriminator loss: 0.43638595938682556\n",
            "Generator loss: 3.3452205657958984, Discriminator loss: 0.40635836124420166\n",
            "Generator loss: 3.3278493881225586, Discriminator loss: 0.4321645498275757\n",
            "Generator loss: 3.4352073669433594, Discriminator loss: 0.44983169436454773\n",
            "Generator loss: 3.4612231254577637, Discriminator loss: 0.4603695273399353\n",
            "Generator loss: 3.414325714111328, Discriminator loss: 0.3602469861507416\n",
            "Generator loss: 3.3690619468688965, Discriminator loss: 0.3951634168624878\n",
            "Generator loss: 3.394920825958252, Discriminator loss: 0.41918623447418213\n",
            "Generator loss: 3.354370355606079, Discriminator loss: 0.3857504427433014\n",
            "Generator loss: 3.337700366973877, Discriminator loss: 0.4851663112640381\n",
            "Generator loss: 3.4517805576324463, Discriminator loss: 0.42984986305236816\n",
            "Generator loss: 3.5005650520324707, Discriminator loss: 0.34456413984298706\n",
            "Generator loss: 3.3934390544891357, Discriminator loss: 0.38428595662117004\n",
            "Generator loss: 3.303741693496704, Discriminator loss: 0.396758496761322\n",
            "Generator loss: 3.378124713897705, Discriminator loss: 0.38633963465690613\n",
            "Generator loss: 3.3526556491851807, Discriminator loss: 0.331095814704895\n",
            "Generator loss: 3.280027151107788, Discriminator loss: 0.4128371477127075\n",
            "Generator loss: 3.2392120361328125, Discriminator loss: 0.4641013741493225\n",
            "Generator loss: 3.3150277137756348, Discriminator loss: 0.45250117778778076\n",
            "Generator loss: 3.434816360473633, Discriminator loss: 0.44252321124076843\n",
            "Generator loss: 3.538898468017578, Discriminator loss: 0.5010055303573608\n",
            "Generator loss: 3.5406181812286377, Discriminator loss: 0.3988074064254761\n",
            "Generator loss: 3.5001206398010254, Discriminator loss: 0.39015132188796997\n",
            "Generator loss: 3.4321682453155518, Discriminator loss: 0.4641055464744568\n",
            "Generator loss: 3.4463882446289062, Discriminator loss: 0.5744850635528564\n",
            "Generator loss: 3.4703903198242188, Discriminator loss: 0.4539182782173157\n",
            "Generator loss: 3.496011734008789, Discriminator loss: 0.3280229866504669\n",
            "Generator loss: 3.3129894733428955, Discriminator loss: 0.3792150318622589\n",
            "Generator loss: 3.2127346992492676, Discriminator loss: 0.32299482822418213\n",
            "Generator loss: 3.1742122173309326, Discriminator loss: 0.3736918568611145\n",
            "Generator loss: 3.2102251052856445, Discriminator loss: 0.4436614513397217\n",
            "Generator loss: 3.309221029281616, Discriminator loss: 0.46967577934265137\n",
            "Generator loss: 3.3820958137512207, Discriminator loss: 0.4934588074684143\n",
            "Generator loss: 3.414797306060791, Discriminator loss: 0.39742153882980347\n",
            "Generator loss: 3.4364190101623535, Discriminator loss: 0.5201032161712646\n",
            "Generator loss: 3.418764114379883, Discriminator loss: 0.33332109451293945\n",
            "Generator loss: 3.4055333137512207, Discriminator loss: 0.39157071709632874\n",
            "Generator loss: 3.4077436923980713, Discriminator loss: 0.300869345664978\n",
            "Generator loss: 3.4402477741241455, Discriminator loss: 0.3596329092979431\n",
            "Generator loss: 3.5024335384368896, Discriminator loss: 0.46589910984039307\n",
            "Generator loss: 3.4756038188934326, Discriminator loss: 0.39632755517959595\n",
            "Generator loss: 3.437854290008545, Discriminator loss: 0.41046246886253357\n",
            "Generator loss: 3.422976016998291, Discriminator loss: 0.470966637134552\n",
            "Generator loss: 3.5509519577026367, Discriminator loss: 0.6338209509849548\n",
            "Generator loss: 3.7016947269439697, Discriminator loss: 0.44353026151657104\n",
            "Generator loss: 3.6118197441101074, Discriminator loss: 0.39738258719444275\n",
            "Generator loss: 3.4633708000183105, Discriminator loss: 0.32237887382507324\n",
            "Generator loss: 3.3488991260528564, Discriminator loss: 0.442674458026886\n",
            "Generator loss: 3.337477922439575, Discriminator loss: 0.41109323501586914\n",
            "Generator loss: 3.3774302005767822, Discriminator loss: 0.40655118227005005\n",
            "Generator loss: 3.4048986434936523, Discriminator loss: 0.4453139901161194\n",
            "Generator loss: 3.4580600261688232, Discriminator loss: 0.4030659794807434\n",
            "Generator loss: 3.4307096004486084, Discriminator loss: 0.3262896239757538\n",
            "Generator loss: 3.3576624393463135, Discriminator loss: 0.27713829278945923\n",
            "Generator loss: 3.3297126293182373, Discriminator loss: 0.2750219702720642\n",
            "Generator loss: 3.3116607666015625, Discriminator loss: 0.3299427628517151\n",
            "Generator loss: 3.225463628768921, Discriminator loss: 0.31718379259109497\n",
            "Generator loss: 3.2095096111297607, Discriminator loss: 0.4151899218559265\n",
            "Generator loss: 3.178499221801758, Discriminator loss: 0.34619611501693726\n",
            "Generator loss: 3.2345118522644043, Discriminator loss: 0.42487770318984985\n",
            "Generator loss: 3.300471305847168, Discriminator loss: 0.4219709038734436\n",
            "Generator loss: 3.414487361907959, Discriminator loss: 0.45294296741485596\n",
            "Generator loss: 3.5479631423950195, Discriminator loss: 0.3964189291000366\n",
            "Generator loss: 3.634502649307251, Discriminator loss: 0.3697640597820282\n",
            "Generator loss: 3.6497769355773926, Discriminator loss: 0.38804373145103455\n",
            "Generator loss: 3.60195255279541, Discriminator loss: 0.3963073194026947\n",
            "Generator loss: 3.5420172214508057, Discriminator loss: 0.33389735221862793\n",
            "Generator loss: 3.5083794593811035, Discriminator loss: 0.5078107714653015\n",
            "Generator loss: 3.5234129428863525, Discriminator loss: 0.30228230357170105\n",
            "Generator loss: 3.4511473178863525, Discriminator loss: 0.28377020359039307\n",
            "Generator loss: 3.3653981685638428, Discriminator loss: 0.34246090054512024\n",
            "Generator loss: 3.318816661834717, Discriminator loss: 0.3194431960582733\n",
            "Generator loss: 3.353872537612915, Discriminator loss: 0.36804527044296265\n",
            "Generator loss: 3.4347357749938965, Discriminator loss: 0.2568420469760895\n",
            "Generator loss: 3.404723644256592, Discriminator loss: 0.20643630623817444\n",
            "Generator loss: 3.2623138427734375, Discriminator loss: 0.37267887592315674\n",
            "Generator loss: 3.220473289489746, Discriminator loss: 0.3195496201515198\n",
            "Generator loss: 3.28366756439209, Discriminator loss: 0.33481070399284363\n",
            "Generator loss: 3.313110113143921, Discriminator loss: 0.2608022093772888\n",
            "Generator loss: 3.2341227531433105, Discriminator loss: 0.3432902693748474\n",
            "Generator loss: 3.236921787261963, Discriminator loss: 0.2877843677997589\n",
            "Generator loss: 3.2291159629821777, Discriminator loss: 0.33978283405303955\n",
            "Generator loss: 3.176788806915283, Discriminator loss: 0.3519779443740845\n",
            "Generator loss: 3.2958240509033203, Discriminator loss: 0.46796390414237976\n",
            "Generator loss: 3.4555509090423584, Discriminator loss: 0.4019535481929779\n",
            "Generator loss: 3.602308511734009, Discriminator loss: 0.29604068398475647\n",
            "Generator loss: 3.49654221534729, Discriminator loss: 0.2863749861717224\n",
            "Generator loss: 3.393341064453125, Discriminator loss: 0.24668821692466736\n",
            "Generator loss: 3.194368600845337, Discriminator loss: 0.2542797327041626\n",
            "Generator loss: 3.1686155796051025, Discriminator loss: 0.33037006855010986\n",
            "Generator loss: 3.197643280029297, Discriminator loss: 0.40656784176826477\n",
            "Generator loss: 3.3223698139190674, Discriminator loss: 0.45128172636032104\n",
            "Generator loss: 3.472592830657959, Discriminator loss: 0.5007111430168152\n",
            "Generator loss: 3.5637731552124023, Discriminator loss: 0.4159575402736664\n",
            "Generator loss: 3.521327495574951, Discriminator loss: 0.43468934297561646\n",
            "Generator loss: 3.5139760971069336, Discriminator loss: 0.2976979613304138\n",
            "Generator loss: 3.4770870208740234, Discriminator loss: 0.3029496669769287\n",
            "Generator loss: 3.4257476329803467, Discriminator loss: 0.3313958942890167\n",
            "Generator loss: 3.4719886779785156, Discriminator loss: 0.33961451053619385\n",
            "Generator loss: 3.426103115081787, Discriminator loss: 0.3042234778404236\n",
            "Generator loss: 3.438192844390869, Discriminator loss: 0.334181010723114\n",
            "Generator loss: 3.4264779090881348, Discriminator loss: 0.34802567958831787\n",
            "Generator loss: 3.5357418060302734, Discriminator loss: 0.3819694221019745\n",
            "Generator loss: 3.5867786407470703, Discriminator loss: 0.33964017033576965\n",
            "Generator loss: 3.5220446586608887, Discriminator loss: 0.24019156396389008\n",
            "Generator loss: 3.45753812789917, Discriminator loss: 0.2764759659767151\n",
            "Generator loss: 3.3486666679382324, Discriminator loss: 0.2918381690979004\n",
            "Generator loss: 3.2884228229522705, Discriminator loss: 0.3040127158164978\n",
            "Generator loss: 3.2776105403900146, Discriminator loss: 0.41077595949172974\n",
            "Generator loss: 3.3763699531555176, Discriminator loss: 0.4493618309497833\n",
            "Generator loss: 3.4762115478515625, Discriminator loss: 0.3852670192718506\n",
            "Generator loss: 3.5226922035217285, Discriminator loss: 0.38184207677841187\n",
            "Generator loss: 3.498962163925171, Discriminator loss: 0.3390130400657654\n",
            "Generator loss: 3.4742815494537354, Discriminator loss: 0.3175710439682007\n",
            "Generator loss: 3.4717910289764404, Discriminator loss: 0.2618074417114258\n",
            "Generator loss: 3.4249799251556396, Discriminator loss: 0.239034503698349\n",
            "Generator loss: 3.378828763961792, Discriminator loss: 0.29618173837661743\n",
            "Generator loss: 3.3123269081115723, Discriminator loss: 0.23933695256710052\n",
            "Generator loss: 3.2744498252868652, Discriminator loss: 0.26878759264945984\n",
            "Generator loss: 3.3354926109313965, Discriminator loss: 0.3181670010089874\n",
            "Generator loss: 3.42279052734375, Discriminator loss: 0.2744481861591339\n",
            "Generator loss: 3.5218048095703125, Discriminator loss: 0.3038729727268219\n",
            "Generator loss: 3.5804405212402344, Discriminator loss: 0.30276867747306824\n",
            "Generator loss: 3.6423988342285156, Discriminator loss: 0.43018120527267456\n",
            "Generator loss: 3.7820851802825928, Discriminator loss: 0.3009604513645172\n",
            "Generator loss: 3.763214349746704, Discriminator loss: 0.2680073380470276\n",
            "Generator loss: 3.7014660835266113, Discriminator loss: 0.38205885887145996\n",
            "Generator loss: 3.6456823348999023, Discriminator loss: 0.3621617555618286\n",
            "Generator loss: 3.72955060005188, Discriminator loss: 0.3994477093219757\n",
            "Generator loss: 3.8428430557250977, Discriminator loss: 0.37172001600265503\n",
            "Generator loss: 3.8514654636383057, Discriminator loss: 0.47570687532424927\n",
            "Generator loss: 3.7954349517822266, Discriminator loss: 0.3015953302383423\n",
            "Generator loss: 3.664242744445801, Discriminator loss: 0.25042396783828735\n",
            "Generator loss: 3.4478659629821777, Discriminator loss: 0.198438361287117\n",
            "Generator loss: 3.2959744930267334, Discriminator loss: 0.2717786133289337\n",
            "Generator loss: 3.29669189453125, Discriminator loss: 0.2647308111190796\n",
            "Generator loss: 3.3793139457702637, Discriminator loss: 0.34980520606040955\n",
            "Generator loss: 3.479546546936035, Discriminator loss: 0.29197537899017334\n",
            "Generator loss: 3.5081634521484375, Discriminator loss: 0.2277849167585373\n",
            "Generator loss: 3.4591546058654785, Discriminator loss: 0.19388434290885925\n",
            "Generator loss: 3.443411111831665, Discriminator loss: 0.25421392917633057\n",
            "Generator loss: 3.4206717014312744, Discriminator loss: 0.19888222217559814\n",
            "Generator loss: 3.3670661449432373, Discriminator loss: 0.18390071392059326\n",
            "Generator loss: 3.286973714828491, Discriminator loss: 0.22243641316890717\n",
            "Generator loss: 3.327565908432007, Discriminator loss: 0.2477332055568695\n",
            "Generator loss: 3.3850595951080322, Discriminator loss: 0.31857824325561523\n",
            "Generator loss: 3.448157548904419, Discriminator loss: 0.2617241442203522\n",
            "Generator loss: 3.5089006423950195, Discriminator loss: 0.22495779395103455\n",
            "Generator loss: 3.5166900157928467, Discriminator loss: 0.2205166220664978\n",
            "Generator loss: 3.4703564643859863, Discriminator loss: 0.222808837890625\n",
            "Generator loss: 3.3857359886169434, Discriminator loss: 0.25436389446258545\n",
            "Generator loss: 3.378093957901001, Discriminator loss: 0.29965949058532715\n",
            "Generator loss: 3.422934055328369, Discriminator loss: 0.26885053515434265\n",
            "Generator loss: 3.4385275840759277, Discriminator loss: 0.26769089698791504\n",
            "Generator loss: 3.3948018550872803, Discriminator loss: 0.28083866834640503\n",
            "Generator loss: 3.4015567302703857, Discriminator loss: 0.32713502645492554\n",
            "Generator loss: 3.53102445602417, Discriminator loss: 0.3272598087787628\n",
            "Generator loss: 3.624206781387329, Discriminator loss: 0.27786514163017273\n",
            "Generator loss: 3.620543956756592, Discriminator loss: 0.2991922199726105\n",
            "Generator loss: 3.621412754058838, Discriminator loss: 0.31690242886543274\n",
            "Generator loss: 3.7130308151245117, Discriminator loss: 0.2759045362472534\n",
            "Generator loss: 3.6871893405914307, Discriminator loss: 0.2662777900695801\n",
            "Generator loss: 3.677917957305908, Discriminator loss: 0.25674930214881897\n",
            "Generator loss: 3.6831929683685303, Discriminator loss: 0.26940613985061646\n",
            "Generator loss: 3.7132418155670166, Discriminator loss: 0.24654506146907806\n",
            "Generator loss: 3.6158642768859863, Discriminator loss: 0.2769694924354553\n",
            "Generator loss: 3.648951768875122, Discriminator loss: 0.4246985614299774\n",
            "Generator loss: 3.7917728424072266, Discriminator loss: 0.34701359272003174\n",
            "Generator loss: 3.8461363315582275, Discriminator loss: 0.36952653527259827\n",
            "Generator loss: 3.872814416885376, Discriminator loss: 0.361595094203949\n",
            "Generator loss: 3.8286995887756348, Discriminator loss: 0.29507777094841003\n",
            "Generator loss: 3.75590181350708, Discriminator loss: 0.2564539611339569\n",
            "Generator loss: 3.630204439163208, Discriminator loss: 0.23137643933296204\n",
            "Generator loss: 3.575063467025757, Discriminator loss: 0.3440686762332916\n",
            "Generator loss: 3.6581954956054688, Discriminator loss: 0.33373934030532837\n",
            "Generator loss: 3.7180070877075195, Discriminator loss: 0.3066549301147461\n",
            "Generator loss: 3.765416383743286, Discriminator loss: 0.3576418161392212\n",
            "Generator loss: 3.7291860580444336, Discriminator loss: 0.2434689700603485\n",
            "Generator loss: 3.5788798332214355, Discriminator loss: 0.24508309364318848\n",
            "Generator loss: 3.532214403152466, Discriminator loss: 0.2583736777305603\n",
            "Generator loss: 3.4822747707366943, Discriminator loss: 0.28811290860176086\n",
            "Generator loss: 3.5170350074768066, Discriminator loss: 0.3527977764606476\n",
            "Generator loss: 3.6959657669067383, Discriminator loss: 0.38073527812957764\n",
            "Generator loss: 3.8072009086608887, Discriminator loss: 0.27490681409835815\n",
            "Generator loss: 3.6932764053344727, Discriminator loss: 0.30884259939193726\n",
            "Generator loss: 3.596006393432617, Discriminator loss: 0.25596004724502563\n",
            "Generator loss: 3.63385009765625, Discriminator loss: 0.33403295278549194\n",
            "Generator loss: 3.671522617340088, Discriminator loss: 0.36302465200424194\n",
            "Generator loss: 3.737560272216797, Discriminator loss: 0.26449596881866455\n",
            "Generator loss: 3.6980299949645996, Discriminator loss: 0.21946613490581512\n",
            "Generator loss: 3.558136224746704, Discriminator loss: 0.32799962162971497\n",
            "Generator loss: 3.5368478298187256, Discriminator loss: 0.3756502866744995\n",
            "Generator loss: 3.6630396842956543, Discriminator loss: 0.43256545066833496\n",
            "Generator loss: 3.76043701171875, Discriminator loss: 0.33817344903945923\n",
            "Generator loss: 3.7765204906463623, Discriminator loss: 0.301000714302063\n",
            "Generator loss: 3.650313138961792, Discriminator loss: 0.2674487233161926\n",
            "Generator loss: 3.497980833053589, Discriminator loss: 0.28552281856536865\n",
            "Generator loss: 3.4797310829162598, Discriminator loss: 0.4183025360107422\n",
            "Generator loss: 3.699921131134033, Discriminator loss: 0.4145839214324951\n",
            "Generator loss: 3.826772928237915, Discriminator loss: 0.3628207743167877\n",
            "Generator loss: 3.7834818363189697, Discriminator loss: 0.2680314779281616\n",
            "Generator loss: 3.6200385093688965, Discriminator loss: 0.3636941909790039\n",
            "Generator loss: 3.5063321590423584, Discriminator loss: 0.37676185369491577\n",
            "Generator loss: 3.56852388381958, Discriminator loss: 0.2290869653224945\n",
            "Generator loss: 3.6187286376953125, Discriminator loss: 0.24342796206474304\n",
            "Generator loss: 3.5857126712799072, Discriminator loss: 0.2645033001899719\n",
            "Generator loss: 3.481419324874878, Discriminator loss: 0.2788625955581665\n",
            "Generator loss: 3.45028018951416, Discriminator loss: 0.2970183491706848\n",
            "Generator loss: 3.4737935066223145, Discriminator loss: 0.2635095715522766\n",
            "Generator loss: 3.53920578956604, Discriminator loss: 0.17132726311683655\n",
            "Generator loss: 3.4888174533843994, Discriminator loss: 0.2062806934118271\n",
            "Generator loss: 3.5018248558044434, Discriminator loss: 0.20967097580432892\n",
            "Generator loss: 3.4497439861297607, Discriminator loss: 0.22886979579925537\n",
            "Generator loss: 3.4450600147247314, Discriminator loss: 0.20124879479408264\n",
            "Generator loss: 3.479685068130493, Discriminator loss: 0.23700940608978271\n",
            "Generator loss: 3.4374349117279053, Discriminator loss: 0.2778087854385376\n",
            "Generator loss: 3.473283052444458, Discriminator loss: 0.24730271100997925\n",
            "Generator loss: 3.5575501918792725, Discriminator loss: 0.21883542835712433\n",
            "Generator loss: 3.638721227645874, Discriminator loss: 0.26795804500579834\n",
            "Generator loss: 3.6008827686309814, Discriminator loss: 0.21449604630470276\n",
            "Generator loss: 3.6020901203155518, Discriminator loss: 0.27772951126098633\n",
            "Generator loss: 3.676478862762451, Discriminator loss: 0.25199881196022034\n",
            "Generator loss: 3.7649009227752686, Discriminator loss: 0.37289100885391235\n",
            "Generator loss: 3.892244338989258, Discriminator loss: 0.29882338643074036\n",
            "Generator loss: 3.974759101867676, Discriminator loss: 0.2219087779521942\n",
            "Generator loss: 3.940234661102295, Discriminator loss: 0.22078196704387665\n",
            "Generator loss: 3.843226671218872, Discriminator loss: 0.31161290407180786\n",
            "Generator loss: 3.805476427078247, Discriminator loss: 0.2633761465549469\n",
            "Generator loss: 3.889813184738159, Discriminator loss: 0.19784188270568848\n",
            "Generator loss: 3.810636281967163, Discriminator loss: 0.15890300273895264\n",
            "Generator loss: 3.661463499069214, Discriminator loss: 0.19129692018032074\n",
            "Generator loss: 3.5618128776550293, Discriminator loss: 0.19569598138332367\n",
            "Generator loss: 3.570748805999756, Discriminator loss: 0.22683483362197876\n",
            "Generator loss: 3.645709753036499, Discriminator loss: 0.2451256513595581\n",
            "Generator loss: 3.7430992126464844, Discriminator loss: 0.263503760099411\n",
            "Generator loss: 3.7904891967773438, Discriminator loss: 0.18995235860347748\n",
            "Generator loss: 3.727139472961426, Discriminator loss: 0.16796869039535522\n",
            "Generator loss: 3.6879220008850098, Discriminator loss: 0.17153587937355042\n",
            "Generator loss: 3.721190929412842, Discriminator loss: 0.21588630974292755\n",
            "Generator loss: 3.692168951034546, Discriminator loss: 0.13920894265174866\n",
            "Generator loss: 3.6179327964782715, Discriminator loss: 0.18456947803497314\n",
            "Generator loss: 3.556575298309326, Discriminator loss: 0.17511188983917236\n",
            "Generator loss: 3.5838370323181152, Discriminator loss: 0.21314691007137299\n",
            "Generator loss: 3.611825466156006, Discriminator loss: 0.22284668684005737\n",
            "Generator loss: 3.7181894779205322, Discriminator loss: 0.2289993166923523\n",
            "Generator loss: 3.730499029159546, Discriminator loss: 0.21597613394260406\n",
            "Generator loss: 3.7072832584381104, Discriminator loss: 0.18288865685462952\n",
            "Generator loss: 3.6410841941833496, Discriminator loss: 0.19093886017799377\n",
            "Generator loss: 3.588693857192993, Discriminator loss: 0.2894408106803894\n",
            "Generator loss: 3.623832941055298, Discriminator loss: 0.21049466729164124\n",
            "Generator loss: 3.59118914604187, Discriminator loss: 0.17842867970466614\n",
            "Generator loss: 3.536834239959717, Discriminator loss: 0.22656765580177307\n",
            "Generator loss: 3.5490005016326904, Discriminator loss: 0.20237576961517334\n",
            "Generator loss: 3.6105005741119385, Discriminator loss: 0.20030970871448517\n",
            "Generator loss: 3.6417129039764404, Discriminator loss: 0.2526077926158905\n",
            "Generator loss: 3.5871798992156982, Discriminator loss: 0.21150149405002594\n",
            "Generator loss: 3.565201997756958, Discriminator loss: 0.19766095280647278\n",
            "Generator loss: 3.530183792114258, Discriminator loss: 0.13297097384929657\n",
            "Generator loss: 3.436405658721924, Discriminator loss: 0.16832192242145538\n",
            "Generator loss: 3.462266206741333, Discriminator loss: 0.15423262119293213\n",
            "Generator loss: 3.451052665710449, Discriminator loss: 0.2250298261642456\n",
            "Generator loss: 3.4513847827911377, Discriminator loss: 0.1961607038974762\n",
            "Generator loss: 3.452623128890991, Discriminator loss: 0.20623859763145447\n",
            "Generator loss: 3.5375614166259766, Discriminator loss: 0.22269906103610992\n",
            "Generator loss: 3.6456871032714844, Discriminator loss: 0.22622868418693542\n",
            "Generator loss: 3.662682294845581, Discriminator loss: 0.23617933690547943\n",
            "Generator loss: 3.6006882190704346, Discriminator loss: 0.2289903163909912\n",
            "Generator loss: 3.6303462982177734, Discriminator loss: 0.22965136170387268\n",
            "Generator loss: 3.662461042404175, Discriminator loss: 0.23731932044029236\n",
            "Generator loss: 3.626142740249634, Discriminator loss: 0.21397069096565247\n",
            "Generator loss: 3.6129302978515625, Discriminator loss: 0.2501767873764038\n",
            "Generator loss: 3.632981061935425, Discriminator loss: 0.2674357295036316\n",
            "Generator loss: 3.6787667274475098, Discriminator loss: 0.3400309979915619\n",
            "Generator loss: 3.7182040214538574, Discriminator loss: 0.2696579694747925\n",
            "Generator loss: 3.7497403621673584, Discriminator loss: 0.21650546789169312\n",
            "Generator loss: 3.715855598449707, Discriminator loss: 0.368795245885849\n",
            "Generator loss: 3.6270296573638916, Discriminator loss: 0.24381019175052643\n",
            "Generator loss: 3.664771795272827, Discriminator loss: 0.2622796595096588\n",
            "Generator loss: 3.7246928215026855, Discriminator loss: 0.28231924772262573\n",
            "Generator loss: 3.851621627807617, Discriminator loss: 0.19688913226127625\n",
            "Generator loss: 3.8096365928649902, Discriminator loss: 0.25447237491607666\n",
            "Generator loss: 3.8640129566192627, Discriminator loss: 0.2544764280319214\n",
            "Generator loss: 3.8518483638763428, Discriminator loss: 0.3398458659648895\n",
            "Generator loss: 3.913954257965088, Discriminator loss: 0.29782629013061523\n",
            "Generator loss: 4.023468494415283, Discriminator loss: 0.358994722366333\n",
            "Generator loss: 4.131466865539551, Discriminator loss: 0.3423205018043518\n",
            "Generator loss: 4.017205715179443, Discriminator loss: 0.2318021059036255\n",
            "Generator loss: 3.800612449645996, Discriminator loss: 0.32759103178977966\n",
            "Generator loss: 3.852616786956787, Discriminator loss: 0.2810221016407013\n",
            "Generator loss: 3.870474100112915, Discriminator loss: 0.37168461084365845\n",
            "Generator loss: 3.9160614013671875, Discriminator loss: 0.32137376070022583\n",
            "Generator loss: 3.9456722736358643, Discriminator loss: 0.4422074556350708\n",
            "Generator loss: 3.932932138442993, Discriminator loss: 0.35766905546188354\n",
            "Generator loss: 3.966078281402588, Discriminator loss: 0.3049035966396332\n",
            "Generator loss: 3.9859251976013184, Discriminator loss: 0.3041136860847473\n",
            "Generator loss: 3.8157637119293213, Discriminator loss: 0.2999935746192932\n",
            "Generator loss: 3.718032121658325, Discriminator loss: 0.21133385598659515\n",
            "Generator loss: 3.70806884765625, Discriminator loss: 0.2039235383272171\n",
            "Generator loss: 3.7467753887176514, Discriminator loss: 0.2551758587360382\n",
            "Generator loss: 3.8046586513519287, Discriminator loss: 0.28487586975097656\n",
            "Generator loss: 3.8453638553619385, Discriminator loss: 0.2258441150188446\n",
            "Generator loss: 3.779395818710327, Discriminator loss: 0.21298672258853912\n",
            "Generator loss: 3.7589941024780273, Discriminator loss: 0.24232275784015656\n",
            "Generator loss: 3.7432734966278076, Discriminator loss: 0.32984715700149536\n",
            "Generator loss: 3.876852035522461, Discriminator loss: 0.2901444137096405\n",
            "Generator loss: 3.9225525856018066, Discriminator loss: 0.2390156090259552\n",
            "Generator loss: 3.945317029953003, Discriminator loss: 0.2170197069644928\n",
            "Generator loss: 3.9359514713287354, Discriminator loss: 0.18903927505016327\n",
            "Generator loss: 3.8491029739379883, Discriminator loss: 0.26046180725097656\n",
            "Generator loss: 3.8193392753601074, Discriminator loss: 0.18548864126205444\n",
            "Generator loss: 3.744260311126709, Discriminator loss: 0.2190934121608734\n",
            "Generator loss: 3.7480998039245605, Discriminator loss: 0.20770569145679474\n",
            "Generator loss: 3.763063669204712, Discriminator loss: 0.20411799848079681\n",
            "Generator loss: 3.80067777633667, Discriminator loss: 0.24045251309871674\n",
            "Generator loss: 3.8106987476348877, Discriminator loss: 0.16080504655838013\n",
            "Generator loss: 3.7781293392181396, Discriminator loss: 0.23377107083797455\n",
            "Generator loss: 3.713698625564575, Discriminator loss: 0.22547605633735657\n",
            "Generator loss: 3.666743278503418, Discriminator loss: 0.19006603956222534\n",
            "Generator loss: 3.6964030265808105, Discriminator loss: 0.22341123223304749\n",
            "Generator loss: 3.7494115829467773, Discriminator loss: 0.17243501543998718\n",
            "Generator loss: 3.766953706741333, Discriminator loss: 0.18662042915821075\n",
            "Generator loss: 3.659184694290161, Discriminator loss: 0.15591807663440704\n",
            "Generator loss: 3.65456223487854, Discriminator loss: 0.24261951446533203\n",
            "Generator loss: 3.6779096126556396, Discriminator loss: 0.1585347205400467\n",
            "Generator loss: 3.6976752281188965, Discriminator loss: 0.19738784432411194\n",
            "Generator loss: 3.6851367950439453, Discriminator loss: 0.20213764905929565\n",
            "Generator loss: 3.6817660331726074, Discriminator loss: 0.2058081030845642\n",
            "Generator loss: 3.7203316688537598, Discriminator loss: 0.2284272164106369\n",
            "Generator loss: 3.8885915279388428, Discriminator loss: 0.2479497492313385\n",
            "Generator loss: 4.083352088928223, Discriminator loss: 0.2649399936199188\n",
            "Generator loss: 4.164610862731934, Discriminator loss: 0.2612932026386261\n",
            "Generator loss: 4.165334701538086, Discriminator loss: 0.22477632761001587\n",
            "Generator loss: 4.037689685821533, Discriminator loss: 0.258524626493454\n",
            "Generator loss: 3.990250587463379, Discriminator loss: 0.22721992433071136\n",
            "Generator loss: 4.016676425933838, Discriminator loss: 0.2594316899776459\n",
            "Generator loss: 4.077104091644287, Discriminator loss: 0.2451636791229248\n",
            "Generator loss: 4.075305461883545, Discriminator loss: 0.3064497411251068\n",
            "Generator loss: 4.030096054077148, Discriminator loss: 0.2379225790500641\n",
            "Generator loss: 3.9968557357788086, Discriminator loss: 0.2515653073787689\n",
            "Generator loss: 3.985189437866211, Discriminator loss: 0.2907155454158783\n",
            "Generator loss: 3.882178783416748, Discriminator loss: 0.24932023882865906\n",
            "Generator loss: 3.830345869064331, Discriminator loss: 0.22735604643821716\n",
            "Generator loss: 3.804525852203369, Discriminator loss: 0.21032819151878357\n",
            "Generator loss: 3.885659694671631, Discriminator loss: 0.30066460371017456\n",
            "Generator loss: 3.9781434535980225, Discriminator loss: 0.25529706478118896\n",
            "Generator loss: 3.966515302658081, Discriminator loss: 0.2986673414707184\n",
            "Generator loss: 3.941570997238159, Discriminator loss: 0.2470494508743286\n",
            "Generator loss: 3.9336986541748047, Discriminator loss: 0.3226737082004547\n",
            "Generator loss: 3.9974963665008545, Discriminator loss: 0.19601035118103027\n",
            "Generator loss: 3.9664955139160156, Discriminator loss: 0.18580764532089233\n",
            "Generator loss: 3.918470621109009, Discriminator loss: 0.18818822503089905\n",
            "Generator loss: 3.8527579307556152, Discriminator loss: 0.1839447021484375\n",
            "Generator loss: 3.8276541233062744, Discriminator loss: 0.19946199655532837\n",
            "Generator loss: 3.90657639503479, Discriminator loss: 0.19305114448070526\n",
            "Generator loss: 3.9519500732421875, Discriminator loss: 0.1719987690448761\n",
            "Generator loss: 3.8742971420288086, Discriminator loss: 0.2196161448955536\n",
            "Generator loss: 3.785139799118042, Discriminator loss: 0.19719472527503967\n",
            "Generator loss: 3.8501229286193848, Discriminator loss: 0.21482519805431366\n",
            "Generator loss: 3.971511125564575, Discriminator loss: 0.260581374168396\n",
            "Generator loss: 4.062946319580078, Discriminator loss: 0.20190083980560303\n",
            "Generator loss: 4.126092433929443, Discriminator loss: 0.22351476550102234\n",
            "Generator loss: 4.156067848205566, Discriminator loss: 0.20180398225784302\n",
            "Generator loss: 4.1700358390808105, Discriminator loss: 0.22538703680038452\n",
            "Generator loss: 4.106894016265869, Discriminator loss: 0.1786826252937317\n",
            "Generator loss: 3.99031400680542, Discriminator loss: 0.20401239395141602\n",
            "Generator loss: 3.977692127227783, Discriminator loss: 0.19166812300682068\n",
            "Generator loss: 3.9632749557495117, Discriminator loss: 0.1524541676044464\n",
            "Generator loss: 3.929901123046875, Discriminator loss: 0.19609859585762024\n",
            "Generator loss: 3.82773494720459, Discriminator loss: 0.17794811725616455\n",
            "Generator loss: 3.7124669551849365, Discriminator loss: 0.2510753870010376\n",
            "Generator loss: 3.6334900856018066, Discriminator loss: 0.15223675966262817\n",
            "Generator loss: 3.6773624420166016, Discriminator loss: 0.20294325053691864\n",
            "Generator loss: 3.7565441131591797, Discriminator loss: 0.17041005194187164\n",
            "Generator loss: 3.792647361755371, Discriminator loss: 0.20654529333114624\n",
            "Generator loss: 3.749680519104004, Discriminator loss: 0.15029959380626678\n",
            "Generator loss: 3.7344393730163574, Discriminator loss: 0.21448326110839844\n",
            "Generator loss: 3.708652973175049, Discriminator loss: 0.2738358974456787\n",
            "Generator loss: 3.773092746734619, Discriminator loss: 0.24263164401054382\n",
            "Generator loss: 3.85036563873291, Discriminator loss: 0.25439712405204773\n",
            "Generator loss: 3.867539167404175, Discriminator loss: 0.1906558871269226\n",
            "Generator loss: 3.7452895641326904, Discriminator loss: 0.2307155728340149\n",
            "Generator loss: 3.7596969604492188, Discriminator loss: 0.22133271396160126\n",
            "Generator loss: 3.77657151222229, Discriminator loss: 0.19030891358852386\n",
            "Generator loss: 3.7396371364593506, Discriminator loss: 0.20803211629390717\n",
            "Generator loss: 3.7526636123657227, Discriminator loss: 0.2516643702983856\n",
            "Generator loss: 3.766846179962158, Discriminator loss: 0.2368212342262268\n",
            "Generator loss: 3.7877492904663086, Discriminator loss: 0.26452991366386414\n",
            "Generator loss: 3.9667162895202637, Discriminator loss: 0.2651202082633972\n",
            "Generator loss: 4.177774906158447, Discriminator loss: 0.25837552547454834\n",
            "Generator loss: 4.262398719787598, Discriminator loss: 0.2915765643119812\n",
            "Generator loss: 4.303144454956055, Discriminator loss: 0.3198170065879822\n",
            "Generator loss: 4.212017059326172, Discriminator loss: 0.3847369849681854\n",
            "Generator loss: 4.0223565101623535, Discriminator loss: 0.37533390522003174\n",
            "Generator loss: 3.873000383377075, Discriminator loss: 0.4188147783279419\n",
            "Generator loss: 3.933058500289917, Discriminator loss: 0.25086092948913574\n",
            "Generator loss: 3.933595895767212, Discriminator loss: 0.263165682554245\n",
            "Generator loss: 3.7658543586730957, Discriminator loss: 0.2241048365831375\n",
            "Generator loss: 3.7401397228240967, Discriminator loss: 0.23531067371368408\n",
            "Generator loss: 3.780163049697876, Discriminator loss: 0.17675888538360596\n",
            "Generator loss: 3.755561351776123, Discriminator loss: 0.1784697026014328\n",
            "Generator loss: 3.8458497524261475, Discriminator loss: 0.23178862035274506\n",
            "Generator loss: 3.773186683654785, Discriminator loss: 0.21189606189727783\n",
            "Generator loss: 3.706608772277832, Discriminator loss: 0.27711957693099976\n",
            "Generator loss: 3.811136245727539, Discriminator loss: 0.2153853476047516\n",
            "Generator loss: 3.8600924015045166, Discriminator loss: 0.2706265449523926\n",
            "Generator loss: 3.758216142654419, Discriminator loss: 0.22993463277816772\n",
            "Generator loss: 3.6592001914978027, Discriminator loss: 0.24594472348690033\n",
            "Generator loss: 3.5412278175354004, Discriminator loss: 0.3374892473220825\n",
            "Generator loss: 3.562842845916748, Discriminator loss: 0.26670873165130615\n",
            "Generator loss: 3.6032087802886963, Discriminator loss: 0.3103668987751007\n",
            "Generator loss: 3.728494167327881, Discriminator loss: 0.2536488175392151\n",
            "Generator loss: 3.7833282947540283, Discriminator loss: 0.2413358837366104\n",
            "Generator loss: 3.822031021118164, Discriminator loss: 0.2661895751953125\n",
            "Generator loss: 3.894939422607422, Discriminator loss: 0.2850688397884369\n",
            "Generator loss: 4.017557144165039, Discriminator loss: 0.31841567158699036\n",
            "Generator loss: 4.15087890625, Discriminator loss: 0.2670513391494751\n",
            "Generator loss: 4.256792068481445, Discriminator loss: 0.2861050069332123\n",
            "Generator loss: 4.3375444412231445, Discriminator loss: 0.2951108515262604\n",
            "Generator loss: 4.3641357421875, Discriminator loss: 0.46852585673332214\n",
            "Generator loss: 4.426095962524414, Discriminator loss: 0.27546629309654236\n",
            "Generator loss: 4.482280731201172, Discriminator loss: 0.34176015853881836\n",
            "Generator loss: 4.582804203033447, Discriminator loss: 0.3507181406021118\n",
            "Generator loss: 4.586893558502197, Discriminator loss: 0.47564390301704407\n",
            "Generator loss: 4.658216953277588, Discriminator loss: 0.35364261269569397\n",
            "Generator loss: 4.62493896484375, Discriminator loss: 0.26168525218963623\n",
            "Generator loss: 4.588681221008301, Discriminator loss: 0.25690627098083496\n",
            "Generator loss: 4.571190357208252, Discriminator loss: 0.2702401876449585\n",
            "Generator loss: 4.548435211181641, Discriminator loss: 0.27114754915237427\n",
            "Generator loss: 4.420541286468506, Discriminator loss: 0.29504525661468506\n",
            "Generator loss: 4.27427864074707, Discriminator loss: 0.28193360567092896\n",
            "Generator loss: 4.171319961547852, Discriminator loss: 0.16687673330307007\n",
            "Generator loss: 4.130690574645996, Discriminator loss: 0.24392351508140564\n",
            "Generator loss: 4.17333459854126, Discriminator loss: 0.23447716236114502\n",
            "Generator loss: 4.235140323638916, Discriminator loss: 0.36312586069107056\n",
            "Generator loss: 4.301872253417969, Discriminator loss: 0.26414015889167786\n",
            "Generator loss: 4.266871452331543, Discriminator loss: 0.21127018332481384\n",
            "Generator loss: 4.1740803718566895, Discriminator loss: 0.23045134544372559\n",
            "Generator loss: 4.125858783721924, Discriminator loss: 0.18057942390441895\n",
            "Generator loss: 4.153817653656006, Discriminator loss: 0.25520485639572144\n",
            "Generator loss: 4.142401695251465, Discriminator loss: 0.21289440989494324\n",
            "Generator loss: 4.171987533569336, Discriminator loss: 0.21632955968379974\n",
            "Generator loss: 4.131852149963379, Discriminator loss: 0.19552713632583618\n",
            "Generator loss: 4.17360258102417, Discriminator loss: 0.23872125148773193\n",
            "Generator loss: 4.230163097381592, Discriminator loss: 0.30120712518692017\n",
            "Generator loss: 4.25813627243042, Discriminator loss: 0.25317683815956116\n",
            "Generator loss: 4.254111289978027, Discriminator loss: 0.22962599992752075\n",
            "Generator loss: 4.291662216186523, Discriminator loss: 0.1882789433002472\n",
            "Generator loss: 4.298399448394775, Discriminator loss: 0.20178057253360748\n",
            "Generator loss: 4.165833950042725, Discriminator loss: 0.16110660135746002\n",
            "Generator loss: 4.026595592498779, Discriminator loss: 0.12902963161468506\n",
            "Generator loss: 3.943977117538452, Discriminator loss: 0.1767524629831314\n",
            "Generator loss: 3.953923463821411, Discriminator loss: 0.19034689664840698\n",
            "Generator loss: 4.0515055656433105, Discriminator loss: 0.16837821900844574\n",
            "Generator loss: 4.106736183166504, Discriminator loss: 0.19803942739963531\n",
            "Generator loss: 4.156023025512695, Discriminator loss: 0.21005427837371826\n",
            "Generator loss: 4.2503790855407715, Discriminator loss: 0.20318377017974854\n",
            "Generator loss: 4.2715888023376465, Discriminator loss: 0.14272870123386383\n",
            "Generator loss: 4.263949871063232, Discriminator loss: 0.1649187207221985\n",
            "Generator loss: 4.199751853942871, Discriminator loss: 0.1688658595085144\n",
            "Generator loss: 4.081136703491211, Discriminator loss: 0.16489073634147644\n",
            "Generator loss: 4.016228199005127, Discriminator loss: 0.19467350840568542\n",
            "Generator loss: 3.984593152999878, Discriminator loss: 0.20079922676086426\n",
            "Generator loss: 4.057314872741699, Discriminator loss: 0.20244894921779633\n",
            "Generator loss: 4.129079818725586, Discriminator loss: 0.3231481909751892\n",
            "Generator loss: 4.151599884033203, Discriminator loss: 0.22180451452732086\n",
            "Generator loss: 4.158017158508301, Discriminator loss: 0.19230304658412933\n",
            "Generator loss: 4.118458271026611, Discriminator loss: 0.21741341054439545\n",
            "Generator loss: 4.193454265594482, Discriminator loss: 0.21216127276420593\n",
            "Generator loss: 4.229569435119629, Discriminator loss: 0.3334507644176483\n",
            "Generator loss: 4.186602592468262, Discriminator loss: 0.1905047744512558\n",
            "Generator loss: 4.109788417816162, Discriminator loss: 0.17821058630943298\n",
            "Generator loss: 4.023344039916992, Discriminator loss: 0.23128816485404968\n",
            "Generator loss: 3.9982383251190186, Discriminator loss: 0.17625775933265686\n",
            "Generator loss: 4.046475410461426, Discriminator loss: 0.18498624861240387\n",
            "Generator loss: 4.117079734802246, Discriminator loss: 0.2571779489517212\n",
            "Generator loss: 4.160837650299072, Discriminator loss: 0.2819488048553467\n",
            "Generator loss: 4.263657093048096, Discriminator loss: 0.23868462443351746\n",
            "Generator loss: 4.432056903839111, Discriminator loss: 0.2217615693807602\n",
            "Generator loss: 4.5382184982299805, Discriminator loss: 0.18892383575439453\n",
            "Generator loss: 4.55030632019043, Discriminator loss: 0.15692204236984253\n",
            "Generator loss: 4.410079002380371, Discriminator loss: 0.14079542458057404\n",
            "Generator loss: 4.3065876960754395, Discriminator loss: 0.1997670829296112\n",
            "Generator loss: 4.25382137298584, Discriminator loss: 0.21018101274967194\n",
            "Generator loss: 4.29979133605957, Discriminator loss: 0.25032860040664673\n",
            "Generator loss: 4.406469345092773, Discriminator loss: 0.19491051137447357\n",
            "Generator loss: 4.509670734405518, Discriminator loss: 0.21029821038246155\n",
            "Generator loss: 4.462445259094238, Discriminator loss: 0.21105822920799255\n",
            "Generator loss: 4.527083396911621, Discriminator loss: 0.24591080844402313\n",
            "Generator loss: 4.527899742126465, Discriminator loss: 0.29553577303886414\n",
            "Generator loss: 4.425433158874512, Discriminator loss: 0.21117472648620605\n",
            "Generator loss: 4.20377254486084, Discriminator loss: 0.13478675484657288\n",
            "Generator loss: 4.011429309844971, Discriminator loss: 0.19116540253162384\n",
            "Generator loss: 3.951597213745117, Discriminator loss: 0.15877515077590942\n",
            "Generator loss: 3.912538528442383, Discriminator loss: 0.1793948858976364\n",
            "Generator loss: 3.933511734008789, Discriminator loss: 0.16893869638442993\n",
            "Generator loss: 3.9402308464050293, Discriminator loss: 0.17286081612110138\n",
            "Generator loss: 3.919130325317383, Discriminator loss: 0.16734543442726135\n",
            "Generator loss: 3.891984462738037, Discriminator loss: 0.170839324593544\n",
            "Generator loss: 3.939845561981201, Discriminator loss: 0.2714993357658386\n",
            "Generator loss: 4.0644850730896, Discriminator loss: 0.2310560941696167\n",
            "Generator loss: 4.185393333435059, Discriminator loss: 0.22949762642383575\n",
            "Generator loss: 4.249074459075928, Discriminator loss: 0.21736612915992737\n",
            "Generator loss: 4.254382133483887, Discriminator loss: 0.20253953337669373\n",
            "Generator loss: 4.230091571807861, Discriminator loss: 0.19445756077766418\n",
            "Generator loss: 4.2208099365234375, Discriminator loss: 0.18720364570617676\n",
            "Generator loss: 4.181983470916748, Discriminator loss: 0.15321171283721924\n",
            "Generator loss: 4.136833667755127, Discriminator loss: 0.22414511442184448\n",
            "Generator loss: 4.1829023361206055, Discriminator loss: 0.1597503423690796\n",
            "Generator loss: 4.266429901123047, Discriminator loss: 0.20038388669490814\n",
            "Generator loss: 4.371357440948486, Discriminator loss: 0.17650073766708374\n",
            "Generator loss: 4.40945291519165, Discriminator loss: 0.195445716381073\n",
            "Generator loss: 4.3409600257873535, Discriminator loss: 0.15615373849868774\n",
            "Generator loss: 4.291337966918945, Discriminator loss: 0.14039243757724762\n",
            "Generator loss: 4.187155246734619, Discriminator loss: 0.13701404631137848\n",
            "Generator loss: 4.065392971038818, Discriminator loss: 0.23087237775325775\n",
            "Generator loss: 4.061796188354492, Discriminator loss: 0.2037743628025055\n",
            "Generator loss: 4.143924713134766, Discriminator loss: 0.1538044661283493\n",
            "Generator loss: 4.10830020904541, Discriminator loss: 0.14204147458076477\n",
            "Generator loss: 4.002152442932129, Discriminator loss: 0.19272089004516602\n",
            "Generator loss: 3.957817792892456, Discriminator loss: 0.14195378124713898\n",
            "Generator loss: 3.9404296875, Discriminator loss: 0.18722134828567505\n",
            "Generator loss: 3.900082588195801, Discriminator loss: 0.25429853796958923\n",
            "Generator loss: 3.9113454818725586, Discriminator loss: 0.17006048560142517\n",
            "Generator loss: 3.9215452671051025, Discriminator loss: 0.16638702154159546\n",
            "Generator loss: 3.981306314468384, Discriminator loss: 0.15098269283771515\n",
            "Generator loss: 4.073016166687012, Discriminator loss: 0.14393384754657745\n",
            "Generator loss: 4.151980400085449, Discriminator loss: 0.11126995086669922\n",
            "Generator loss: 4.1459269523620605, Discriminator loss: 0.10069537162780762\n",
            "Generator loss: 3.93050479888916, Discriminator loss: 0.18652386963367462\n",
            "Generator loss: 3.8614346981048584, Discriminator loss: 0.11742456257343292\n",
            "Generator loss: 3.8440661430358887, Discriminator loss: 0.18610879778862\n",
            "Generator loss: 3.9216270446777344, Discriminator loss: 0.14812707901000977\n",
            "Generator loss: 3.9806864261627197, Discriminator loss: 0.14815890789031982\n",
            "Generator loss: 4.024728775024414, Discriminator loss: 0.12266643345355988\n",
            "Generator loss: 3.9804935455322266, Discriminator loss: 0.14701704680919647\n",
            "Generator loss: 4.002291202545166, Discriminator loss: 0.13433882594108582\n",
            "Generator loss: 4.015899181365967, Discriminator loss: 0.14179004728794098\n",
            "Generator loss: 4.019697189331055, Discriminator loss: 0.14450500905513763\n",
            "Generator loss: 4.050543785095215, Discriminator loss: 0.10535271465778351\n",
            "Generator loss: 3.9886415004730225, Discriminator loss: 0.1981048583984375\n",
            "Generator loss: 3.9657130241394043, Discriminator loss: 0.12838274240493774\n",
            "Generator loss: 3.936629295349121, Discriminator loss: 0.23625820875167847\n",
            "Generator loss: 3.951643705368042, Discriminator loss: 0.19518500566482544\n",
            "Generator loss: 4.052659511566162, Discriminator loss: 0.1623009294271469\n",
            "Generator loss: 4.084844589233398, Discriminator loss: 0.15880264341831207\n",
            "Generator loss: 4.048829078674316, Discriminator loss: 0.10099688917398453\n",
            "Generator loss: 3.948584794998169, Discriminator loss: 0.14483588933944702\n",
            "Generator loss: 3.9291036128997803, Discriminator loss: 0.10132502019405365\n",
            "Generator loss: 3.9394969940185547, Discriminator loss: 0.12025461345911026\n",
            "Generator loss: 3.9734678268432617, Discriminator loss: 0.13534054160118103\n",
            "Generator loss: 4.02485990524292, Discriminator loss: 0.1203126311302185\n",
            "Generator loss: 4.062597751617432, Discriminator loss: 0.11103425174951553\n",
            "Generator loss: 4.0337324142456055, Discriminator loss: 0.1150636225938797\n",
            "Generator loss: 4.045969009399414, Discriminator loss: 0.11508534103631973\n",
            "Generator loss: 4.031780242919922, Discriminator loss: 0.1171981692314148\n",
            "Generator loss: 4.0457868576049805, Discriminator loss: 0.0966305285692215\n",
            "Generator loss: 3.951983690261841, Discriminator loss: 0.1394668072462082\n",
            "Generator loss: 3.921420097351074, Discriminator loss: 0.1225769966840744\n",
            "Generator loss: 3.975555181503296, Discriminator loss: 0.14018535614013672\n",
            "Generator loss: 3.9133152961730957, Discriminator loss: 0.13627779483795166\n",
            "Generator loss: 3.893012523651123, Discriminator loss: 0.13459552824497223\n",
            "Generator loss: 3.9817776679992676, Discriminator loss: 0.15590089559555054\n",
            "Generator loss: 4.0347065925598145, Discriminator loss: 0.18744343519210815\n",
            "Generator loss: 4.164600849151611, Discriminator loss: 0.16631777584552765\n",
            "Generator loss: 4.2429585456848145, Discriminator loss: 0.22008442878723145\n",
            "Generator loss: 4.3008270263671875, Discriminator loss: 0.18947765231132507\n",
            "Generator loss: 4.280033111572266, Discriminator loss: 0.19794225692749023\n",
            "Generator loss: 4.177224159240723, Discriminator loss: 0.17571116983890533\n",
            "Generator loss: 4.148471832275391, Discriminator loss: 0.16756996512413025\n",
            "Generator loss: 4.1173481941223145, Discriminator loss: 0.14506468176841736\n",
            "Generator loss: 4.194458961486816, Discriminator loss: 0.23438343405723572\n",
            "Generator loss: 4.206315994262695, Discriminator loss: 0.15433749556541443\n",
            "Generator loss: 4.180403232574463, Discriminator loss: 0.15425267815589905\n",
            "Generator loss: 4.1283183097839355, Discriminator loss: 0.19060677289962769\n",
            "Generator loss: 4.1507086753845215, Discriminator loss: 0.1328740119934082\n",
            "Generator loss: 4.2233734130859375, Discriminator loss: 0.15982750058174133\n",
            "Generator loss: 4.263854026794434, Discriminator loss: 0.11952003836631775\n",
            "Generator loss: 4.215021133422852, Discriminator loss: 0.12085235118865967\n",
            "Generator loss: 4.028828144073486, Discriminator loss: 0.15084251761436462\n",
            "Generator loss: 3.966409683227539, Discriminator loss: 0.16437776386737823\n",
            "Generator loss: 3.971252918243408, Discriminator loss: 0.10383038222789764\n",
            "Generator loss: 3.9489521980285645, Discriminator loss: 0.15155285596847534\n",
            "Generator loss: 3.895627975463867, Discriminator loss: 0.13310673832893372\n",
            "Generator loss: 3.8776462078094482, Discriminator loss: 0.19811543822288513\n",
            "Generator loss: 3.955549955368042, Discriminator loss: 0.15176057815551758\n",
            "Generator loss: 3.9848785400390625, Discriminator loss: 0.14888635277748108\n",
            "Generator loss: 4.015309810638428, Discriminator loss: 0.16291072964668274\n",
            "Generator loss: 4.047532558441162, Discriminator loss: 0.14757701754570007\n",
            "Generator loss: 3.99160099029541, Discriminator loss: 0.1780489981174469\n",
            "Generator loss: 3.993313789367676, Discriminator loss: 0.13990415632724762\n",
            "Generator loss: 4.044097423553467, Discriminator loss: 0.21057875454425812\n",
            "Generator loss: 4.029378890991211, Discriminator loss: 0.1113792359828949\n",
            "Generator loss: 3.989396810531616, Discriminator loss: 0.1533472239971161\n",
            "Generator loss: 3.9581034183502197, Discriminator loss: 0.1738537847995758\n",
            "Generator loss: 4.065968990325928, Discriminator loss: 0.20106247067451477\n",
            "Generator loss: 4.273286819458008, Discriminator loss: 0.19361774623394012\n",
            "Generator loss: 4.4926886558532715, Discriminator loss: 0.18816441297531128\n",
            "Generator loss: 4.549618244171143, Discriminator loss: 0.18561026453971863\n",
            "Generator loss: 4.552849292755127, Discriminator loss: 0.12793554365634918\n",
            "Generator loss: 4.426449775695801, Discriminator loss: 0.15171286463737488\n",
            "Generator loss: 4.2730841636657715, Discriminator loss: 0.08868137001991272\n",
            "Generator loss: 4.127771377563477, Discriminator loss: 0.15991367399692535\n",
            "Generator loss: 4.078713417053223, Discriminator loss: 0.16203784942626953\n",
            "Generator loss: 4.07792854309082, Discriminator loss: 0.10700759291648865\n",
            "Generator loss: 4.085814476013184, Discriminator loss: 0.13843640685081482\n",
            "Generator loss: 4.046048164367676, Discriminator loss: 0.16575300693511963\n",
            "Generator loss: 4.045146942138672, Discriminator loss: 0.12793773412704468\n",
            "Generator loss: 4.062318325042725, Discriminator loss: 0.19789370894432068\n",
            "Generator loss: 4.062891960144043, Discriminator loss: 0.25680941343307495\n",
            "Generator loss: 4.037686347961426, Discriminator loss: 0.2756957411766052\n",
            "Generator loss: 4.07698917388916, Discriminator loss: 0.16734744608402252\n",
            "Generator loss: 4.090253829956055, Discriminator loss: 0.14592891931533813\n",
            "Generator loss: 4.089297294616699, Discriminator loss: 0.2373671531677246\n",
            "Generator loss: 4.117749214172363, Discriminator loss: 0.21532006561756134\n",
            "Generator loss: 4.223374843597412, Discriminator loss: 0.17918768525123596\n",
            "Generator loss: 4.300079822540283, Discriminator loss: 0.15515652298927307\n",
            "Generator loss: 4.275026798248291, Discriminator loss: 0.17673812806606293\n",
            "Generator loss: 4.23300313949585, Discriminator loss: 0.1933809369802475\n",
            "Generator loss: 4.270534992218018, Discriminator loss: 0.15008559823036194\n",
            "Generator loss: 4.22275972366333, Discriminator loss: 0.2215743064880371\n",
            "Generator loss: 4.191733360290527, Discriminator loss: 0.1710454821586609\n",
            "Generator loss: 4.216948509216309, Discriminator loss: 0.19510304927825928\n",
            "Generator loss: 4.297071933746338, Discriminator loss: 0.23322239518165588\n",
            "Generator loss: 4.321724891662598, Discriminator loss: 0.17591771483421326\n",
            "Generator loss: 4.274163246154785, Discriminator loss: 0.19469669461250305\n",
            "Generator loss: 4.21649169921875, Discriminator loss: 0.16090700030326843\n",
            "Generator loss: 4.194467544555664, Discriminator loss: 0.22204986214637756\n",
            "Generator loss: 4.368259429931641, Discriminator loss: 0.1798824667930603\n",
            "Generator loss: 4.462839603424072, Discriminator loss: 0.13059045374393463\n",
            "Generator loss: 4.330719947814941, Discriminator loss: 0.2163650393486023\n",
            "Generator loss: 4.273011684417725, Discriminator loss: 0.16135826706886292\n",
            "Generator loss: 4.232901096343994, Discriminator loss: 0.25989818572998047\n",
            "Generator loss: 4.188017845153809, Discriminator loss: 0.16855673491954803\n",
            "Generator loss: 4.181318759918213, Discriminator loss: 0.3719351589679718\n",
            "Generator loss: 4.182452201843262, Discriminator loss: 0.1356096863746643\n",
            "Generator loss: 4.110775470733643, Discriminator loss: 0.1128455325961113\n",
            "Generator loss: 4.032266139984131, Discriminator loss: 0.10238116979598999\n",
            "Generator loss: 3.9618022441864014, Discriminator loss: 0.15685990452766418\n",
            "Generator loss: 3.9500012397766113, Discriminator loss: 0.24534781277179718\n",
            "Generator loss: 3.9812207221984863, Discriminator loss: 0.1377979815006256\n",
            "Generator loss: 4.021785259246826, Discriminator loss: 0.17686405777931213\n",
            "Generator loss: 4.116530418395996, Discriminator loss: 0.148978590965271\n",
            "Generator loss: 4.137943267822266, Discriminator loss: 0.16042406857013702\n",
            "Generator loss: 4.156318187713623, Discriminator loss: 0.16641758382320404\n",
            "Generator loss: 4.070333003997803, Discriminator loss: 0.18931418657302856\n",
            "Generator loss: 3.990255832672119, Discriminator loss: 0.17032812535762787\n",
            "Generator loss: 4.020279407501221, Discriminator loss: 0.1491081565618515\n",
            "Generator loss: 3.976588487625122, Discriminator loss: 0.25740954279899597\n",
            "Generator loss: 4.108238697052002, Discriminator loss: 0.1625133752822876\n",
            "Generator loss: 4.130542755126953, Discriminator loss: 0.3159773647785187\n",
            "Generator loss: 4.267575740814209, Discriminator loss: 0.19042381644248962\n",
            "Generator loss: 4.347142219543457, Discriminator loss: 0.15869683027267456\n",
            "Generator loss: 4.2677836418151855, Discriminator loss: 0.20885220170021057\n",
            "Generator loss: 4.403491973876953, Discriminator loss: 0.243856281042099\n",
            "Generator loss: 4.584134578704834, Discriminator loss: 0.25692465901374817\n",
            "Generator loss: 4.61901330947876, Discriminator loss: 0.18337568640708923\n",
            "Generator loss: 4.467898368835449, Discriminator loss: 0.20577383041381836\n",
            "Generator loss: 4.44651460647583, Discriminator loss: 0.3128107190132141\n",
            "Generator loss: 4.527989387512207, Discriminator loss: 0.26072442531585693\n",
            "Generator loss: 4.543152809143066, Discriminator loss: 0.24406075477600098\n",
            "Generator loss: 4.587122440338135, Discriminator loss: 0.17069676518440247\n",
            "Generator loss: 4.629787921905518, Discriminator loss: 0.2827601432800293\n",
            "Generator loss: 4.7183942794799805, Discriminator loss: 0.18272659182548523\n",
            "Generator loss: 4.849490165710449, Discriminator loss: 0.23061424493789673\n",
            "Generator loss: 4.87220573425293, Discriminator loss: 0.15641716122627258\n",
            "Generator loss: 4.689610481262207, Discriminator loss: 0.15688587725162506\n",
            "Generator loss: 4.455010414123535, Discriminator loss: 0.12460100650787354\n",
            "Generator loss: 4.3255720138549805, Discriminator loss: 0.10587962716817856\n",
            "Generator loss: 4.284646987915039, Discriminator loss: 0.16971243917942047\n",
            "Generator loss: 4.3412346839904785, Discriminator loss: 0.19280149042606354\n",
            "Generator loss: 4.461484909057617, Discriminator loss: 0.12983505427837372\n",
            "Generator loss: 4.4952826499938965, Discriminator loss: 0.16296322643756866\n",
            "Generator loss: 4.48166036605835, Discriminator loss: 0.12712496519088745\n",
            "Generator loss: 4.470221996307373, Discriminator loss: 0.151500403881073\n",
            "Generator loss: 4.5044379234313965, Discriminator loss: 0.17758668959140778\n",
            "Generator loss: 4.596262454986572, Discriminator loss: 0.15746143460273743\n",
            "Generator loss: 4.606993675231934, Discriminator loss: 0.18100175261497498\n",
            "Generator loss: 4.582268714904785, Discriminator loss: 0.14842239022254944\n",
            "Generator loss: 4.545665264129639, Discriminator loss: 0.1987106055021286\n",
            "Generator loss: 4.574616432189941, Discriminator loss: 0.17773044109344482\n",
            "Generator loss: 4.49562406539917, Discriminator loss: 0.1559447944164276\n",
            "Generator loss: 4.400536060333252, Discriminator loss: 0.12031949311494827\n",
            "Generator loss: 4.295029640197754, Discriminator loss: 0.16894684731960297\n",
            "Generator loss: 4.285870552062988, Discriminator loss: 0.15991930663585663\n",
            "Generator loss: 4.329131126403809, Discriminator loss: 0.1492786705493927\n",
            "Generator loss: 4.333152770996094, Discriminator loss: 0.16887718439102173\n",
            "Generator loss: 4.346439838409424, Discriminator loss: 0.15379668772220612\n",
            "Generator loss: 4.350636005401611, Discriminator loss: 0.11804652214050293\n",
            "Generator loss: 4.280266284942627, Discriminator loss: 0.12776045501232147\n",
            "Generator loss: 4.197103023529053, Discriminator loss: 0.10764437913894653\n",
            "Generator loss: 4.150146484375, Discriminator loss: 0.11347107589244843\n",
            "Generator loss: 4.174079895019531, Discriminator loss: 0.1163863018155098\n",
            "Generator loss: 4.272239685058594, Discriminator loss: 0.12070022523403168\n",
            "Generator loss: 4.256896018981934, Discriminator loss: 0.2613517642021179\n",
            "Generator loss: 4.299009799957275, Discriminator loss: 0.14560510218143463\n",
            "Generator loss: 4.303450107574463, Discriminator loss: 0.16772550344467163\n",
            "Generator loss: 4.239965915679932, Discriminator loss: 0.12685278058052063\n",
            "Generator loss: 4.243378639221191, Discriminator loss: 0.11569748818874359\n",
            "Generator loss: 4.205054759979248, Discriminator loss: 0.12408296763896942\n",
            "Generator loss: 4.321525573730469, Discriminator loss: 0.14415448904037476\n",
            "Generator loss: 4.3702239990234375, Discriminator loss: 0.13767696917057037\n",
            "Generator loss: 4.380617618560791, Discriminator loss: 0.13809913396835327\n",
            "Generator loss: 4.362473011016846, Discriminator loss: 0.227412149310112\n",
            "Generator loss: 4.3945817947387695, Discriminator loss: 0.13756023347377777\n",
            "Generator loss: 4.377320766448975, Discriminator loss: 0.11159642785787582\n",
            "Generator loss: 4.327450752258301, Discriminator loss: 0.12810800969600677\n",
            "Generator loss: 4.337707996368408, Discriminator loss: 0.12653321027755737\n",
            "Generator loss: 4.36398983001709, Discriminator loss: 0.10721881687641144\n",
            "Generator loss: 4.431951999664307, Discriminator loss: 0.12846234440803528\n",
            "Generator loss: 4.490067958831787, Discriminator loss: 0.17373952269554138\n",
            "Generator loss: 4.50998592376709, Discriminator loss: 0.12853579223155975\n",
            "Generator loss: 4.478547096252441, Discriminator loss: 0.16350869834423065\n",
            "Generator loss: 4.454525947570801, Discriminator loss: 0.13531489670276642\n",
            "Generator loss: 4.404955863952637, Discriminator loss: 0.12238779664039612\n",
            "Generator loss: 4.35684871673584, Discriminator loss: 0.11014407873153687\n",
            "Generator loss: 4.371303081512451, Discriminator loss: 0.13318437337875366\n",
            "Generator loss: 4.493433475494385, Discriminator loss: 0.1634611189365387\n",
            "Generator loss: 4.56422233581543, Discriminator loss: 0.16186842322349548\n",
            "Generator loss: 4.581007957458496, Discriminator loss: 0.11980023980140686\n",
            "Generator loss: 4.589109420776367, Discriminator loss: 0.1368628442287445\n",
            "Generator loss: 4.644881725311279, Discriminator loss: 0.12311980128288269\n",
            "Generator loss: 4.675293922424316, Discriminator loss: 0.1307155191898346\n",
            "Generator loss: 4.618941307067871, Discriminator loss: 0.12081243097782135\n",
            "Generator loss: 4.598099708557129, Discriminator loss: 0.15089432895183563\n",
            "Generator loss: 4.659392833709717, Discriminator loss: 0.10681911557912827\n",
            "Generator loss: 4.576359748840332, Discriminator loss: 0.09784133732318878\n",
            "Generator loss: 4.47291898727417, Discriminator loss: 0.10273070633411407\n",
            "Generator loss: 4.392567157745361, Discriminator loss: 0.14775733649730682\n",
            "Generator loss: 4.638605117797852, Discriminator loss: 0.1937439739704132\n",
            "Generator loss: 4.759936809539795, Discriminator loss: 0.15136796236038208\n",
            "Generator loss: 4.722276210784912, Discriminator loss: 0.1516604721546173\n",
            "Generator loss: 4.667365550994873, Discriminator loss: 0.16377052664756775\n",
            "Generator loss: 4.729238510131836, Discriminator loss: 0.2032926231622696\n",
            "Generator loss: 4.790095806121826, Discriminator loss: 0.14717386662960052\n",
            "Generator loss: 4.733296871185303, Discriminator loss: 0.11946253478527069\n",
            "Generator loss: 4.654415130615234, Discriminator loss: 0.22817321121692657\n",
            "Generator loss: 4.679532051086426, Discriminator loss: 0.20812460780143738\n",
            "Generator loss: 4.641216278076172, Discriminator loss: 0.18871115148067474\n",
            "Generator loss: 4.526508331298828, Discriminator loss: 0.33693060278892517\n",
            "Generator loss: 4.537762641906738, Discriminator loss: 0.1325419396162033\n",
            "Generator loss: 4.638034343719482, Discriminator loss: 0.12024305015802383\n",
            "Generator loss: 4.617463111877441, Discriminator loss: 0.14480073750019073\n",
            "Generator loss: 4.578944206237793, Discriminator loss: 0.13358987867832184\n",
            "Generator loss: 4.515687465667725, Discriminator loss: 0.1413114368915558\n",
            "Generator loss: 4.423147201538086, Discriminator loss: 0.11483430862426758\n",
            "Generator loss: 4.381165981292725, Discriminator loss: 0.12612025439739227\n",
            "Generator loss: 4.3555426597595215, Discriminator loss: 0.11720522493124008\n",
            "Generator loss: 4.3882269859313965, Discriminator loss: 0.12881259620189667\n",
            "Generator loss: 4.448155403137207, Discriminator loss: 0.10098680108785629\n",
            "Generator loss: 4.463861465454102, Discriminator loss: 0.10821908712387085\n",
            "Generator loss: 4.3711137771606445, Discriminator loss: 0.1346706748008728\n",
            "Generator loss: 4.347777366638184, Discriminator loss: 0.13967470824718475\n",
            "Generator loss: 4.392852306365967, Discriminator loss: 0.1186414584517479\n",
            "Generator loss: 4.457474231719971, Discriminator loss: 0.11776503920555115\n",
            "Generator loss: 4.432636737823486, Discriminator loss: 0.17103877663612366\n",
            "Generator loss: 4.363743782043457, Discriminator loss: 0.11792983114719391\n",
            "Generator loss: 4.363813400268555, Discriminator loss: 0.08986097574234009\n",
            "Generator loss: 4.291849136352539, Discriminator loss: 0.0986318439245224\n",
            "Generator loss: 4.2250189781188965, Discriminator loss: 0.11195702850818634\n",
            "Generator loss: 4.151676177978516, Discriminator loss: 0.15130186080932617\n",
            "Generator loss: 4.145517826080322, Discriminator loss: 0.18464383482933044\n",
            "Generator loss: 4.218667984008789, Discriminator loss: 0.1354116052389145\n",
            "Generator loss: 4.285700798034668, Discriminator loss: 0.14307409524917603\n",
            "Generator loss: 4.344611644744873, Discriminator loss: 0.12335261702537537\n",
            "Generator loss: 4.339829444885254, Discriminator loss: 0.11808545887470245\n",
            "Generator loss: 4.343705654144287, Discriminator loss: 0.1627284288406372\n",
            "Generator loss: 4.371556282043457, Discriminator loss: 0.16291959583759308\n",
            "Generator loss: 4.420373916625977, Discriminator loss: 0.20950648188591003\n",
            "Generator loss: 4.427634239196777, Discriminator loss: 0.1234550029039383\n",
            "Generator loss: 4.333521842956543, Discriminator loss: 0.14717251062393188\n",
            "Generator loss: 4.317217826843262, Discriminator loss: 0.12764309346675873\n",
            "Generator loss: 4.368689060211182, Discriminator loss: 0.10616638511419296\n",
            "Generator loss: 4.442625045776367, Discriminator loss: 0.1310988813638687\n",
            "Generator loss: 4.558922290802002, Discriminator loss: 0.12435974180698395\n",
            "Generator loss: 4.562260150909424, Discriminator loss: 0.113491490483284\n",
            "Generator loss: 4.531702995300293, Discriminator loss: 0.08766292035579681\n",
            "Generator loss: 4.393889427185059, Discriminator loss: 0.1265854835510254\n",
            "Generator loss: 4.3216681480407715, Discriminator loss: 0.07998237013816833\n",
            "Generator loss: 4.237270832061768, Discriminator loss: 0.09010906517505646\n",
            "Generator loss: 4.2703142166137695, Discriminator loss: 0.078036829829216\n",
            "Generator loss: 4.271739959716797, Discriminator loss: 0.07652418315410614\n",
            "Generator loss: 4.288971900939941, Discriminator loss: 0.08095506578683853\n",
            "Generator loss: 4.264638423919678, Discriminator loss: 0.07088927179574966\n",
            "Generator loss: 4.283941268920898, Discriminator loss: 0.08744476735591888\n",
            "Generator loss: 4.244534969329834, Discriminator loss: 0.08454139530658722\n",
            "Generator loss: 4.281217575073242, Discriminator loss: 0.12764257192611694\n",
            "Generator loss: 4.357672691345215, Discriminator loss: 0.08940686285495758\n",
            "Generator loss: 4.424041748046875, Discriminator loss: 0.1176600307226181\n",
            "Generator loss: 4.514275550842285, Discriminator loss: 0.11422955989837646\n",
            "Generator loss: 4.638940811157227, Discriminator loss: 0.13330954313278198\n",
            "Generator loss: 4.659993648529053, Discriminator loss: 0.07277880609035492\n",
            "Generator loss: 4.5889129638671875, Discriminator loss: 0.07031750679016113\n",
            "Generator loss: 4.494746685028076, Discriminator loss: 0.06229543313384056\n",
            "Generator loss: 4.429852485656738, Discriminator loss: 0.10348300635814667\n",
            "Generator loss: 4.402060508728027, Discriminator loss: 0.0743609368801117\n",
            "Generator loss: 4.364495277404785, Discriminator loss: 0.1292743980884552\n",
            "Generator loss: 4.383505821228027, Discriminator loss: 0.08134927600622177\n",
            "Generator loss: 4.422040939331055, Discriminator loss: 0.07257465273141861\n",
            "Generator loss: 4.391646385192871, Discriminator loss: 0.08915336430072784\n",
            "Generator loss: 4.415019512176514, Discriminator loss: 0.07934440672397614\n",
            "Generator loss: 4.402927875518799, Discriminator loss: 0.0827009528875351\n",
            "Generator loss: 4.339332580566406, Discriminator loss: 0.07368707656860352\n",
            "Generator loss: 4.35848331451416, Discriminator loss: 0.09851684421300888\n",
            "Generator loss: 4.358465671539307, Discriminator loss: 0.05562372878193855\n",
            "Generator loss: 4.35305643081665, Discriminator loss: 0.07111313939094543\n",
            "Generator loss: 4.3209943771362305, Discriminator loss: 0.09797884523868561\n",
            "Generator loss: 4.295745849609375, Discriminator loss: 0.08886463940143585\n",
            "Generator loss: 4.291676044464111, Discriminator loss: 0.11669197678565979\n",
            "Generator loss: 4.279311180114746, Discriminator loss: 0.08187872171401978\n",
            "Generator loss: 4.320795059204102, Discriminator loss: 0.09800085425376892\n",
            "Generator loss: 4.321995735168457, Discriminator loss: 0.11205823719501495\n",
            "Generator loss: 4.384437561035156, Discriminator loss: 0.11515569686889648\n",
            "Generator loss: 4.523801803588867, Discriminator loss: 0.15263503789901733\n",
            "Generator loss: 4.614861011505127, Discriminator loss: 0.12779977917671204\n",
            "Generator loss: 4.581292629241943, Discriminator loss: 0.14111702144145966\n",
            "Generator loss: 4.642890453338623, Discriminator loss: 0.11126697808504105\n",
            "Generator loss: 4.582406997680664, Discriminator loss: 0.1579013615846634\n",
            "Generator loss: 4.542379856109619, Discriminator loss: 0.12243494391441345\n",
            "Generator loss: 4.532500267028809, Discriminator loss: 0.13751879334449768\n",
            "Generator loss: 4.558084487915039, Discriminator loss: 0.14208540320396423\n",
            "Generator loss: 4.604334354400635, Discriminator loss: 0.11010058969259262\n",
            "Generator loss: 4.580971717834473, Discriminator loss: 0.11172764003276825\n",
            "Generator loss: 4.583472728729248, Discriminator loss: 0.11406432837247849\n",
            "Generator loss: 4.591447830200195, Discriminator loss: 0.10523398220539093\n",
            "Generator loss: 4.575204372406006, Discriminator loss: 0.12090055644512177\n",
            "Generator loss: 4.538802623748779, Discriminator loss: 0.12373289465904236\n",
            "Generator loss: 4.600763320922852, Discriminator loss: 0.12570945918560028\n",
            "Generator loss: 4.695094108581543, Discriminator loss: 0.13351082801818848\n",
            "Generator loss: 4.729372024536133, Discriminator loss: 0.13384631276130676\n",
            "Generator loss: 4.76605224609375, Discriminator loss: 0.11688225716352463\n",
            "Generator loss: 4.726871013641357, Discriminator loss: 0.17019571363925934\n",
            "Generator loss: 4.659374237060547, Discriminator loss: 0.08981869369745255\n",
            "Generator loss: 4.551095008850098, Discriminator loss: 0.09472320228815079\n",
            "Generator loss: 4.524661540985107, Discriminator loss: 0.08715076744556427\n",
            "Generator loss: 4.559710502624512, Discriminator loss: 0.11593795567750931\n",
            "Generator loss: 4.48934268951416, Discriminator loss: 0.11588871479034424\n",
            "Generator loss: 4.514338493347168, Discriminator loss: 0.11280914396047592\n",
            "Generator loss: 4.682327747344971, Discriminator loss: 0.12906944751739502\n",
            "Generator loss: 4.801738739013672, Discriminator loss: 0.09802063554525375\n",
            "Generator loss: 4.696710586547852, Discriminator loss: 0.09280961006879807\n",
            "Generator loss: 4.59788703918457, Discriminator loss: 0.09147640317678452\n",
            "Generator loss: 4.444966793060303, Discriminator loss: 0.0817832499742508\n",
            "Generator loss: 4.294220924377441, Discriminator loss: 0.08779696375131607\n",
            "Generator loss: 4.3311614990234375, Discriminator loss: 0.1017288863658905\n",
            "Generator loss: 4.351075172424316, Discriminator loss: 0.11741603910923004\n",
            "Generator loss: 4.417234897613525, Discriminator loss: 0.07850706577301025\n",
            "Generator loss: 4.410473823547363, Discriminator loss: 0.06474915891885757\n",
            "Generator loss: 4.362438201904297, Discriminator loss: 0.08871755748987198\n",
            "Generator loss: 4.340784549713135, Discriminator loss: 0.08897757530212402\n",
            "Generator loss: 4.342203140258789, Discriminator loss: 0.0969650000333786\n",
            "Generator loss: 4.33620548248291, Discriminator loss: 0.09069368243217468\n",
            "Generator loss: 4.363171100616455, Discriminator loss: 0.08258557319641113\n",
            "Generator loss: 4.374584197998047, Discriminator loss: 0.13062675297260284\n",
            "Generator loss: 4.289154529571533, Discriminator loss: 0.1126597598195076\n",
            "Generator loss: 4.240623474121094, Discriminator loss: 0.11426503211259842\n",
            "Generator loss: 4.288086414337158, Discriminator loss: 0.10068623721599579\n",
            "Generator loss: 4.303594589233398, Discriminator loss: 0.17193567752838135\n",
            "Generator loss: 4.325000286102295, Discriminator loss: 0.11089931428432465\n",
            "Generator loss: 4.387015342712402, Discriminator loss: 0.1935676634311676\n",
            "Generator loss: 4.4674072265625, Discriminator loss: 0.11526922881603241\n",
            "Generator loss: 4.538941860198975, Discriminator loss: 0.16148652136325836\n",
            "Generator loss: 4.613359451293945, Discriminator loss: 0.17978984117507935\n",
            "Generator loss: 4.692638874053955, Discriminator loss: 0.1173953115940094\n",
            "Generator loss: 4.704791069030762, Discriminator loss: 0.11299660801887512\n",
            "Generator loss: 4.619887828826904, Discriminator loss: 0.14510099589824677\n",
            "Generator loss: 4.47772741317749, Discriminator loss: 0.1378958523273468\n",
            "Generator loss: 4.461470127105713, Discriminator loss: 0.13050420582294464\n",
            "Generator loss: 4.476996421813965, Discriminator loss: 0.10632675141096115\n",
            "Generator loss: 4.298244476318359, Discriminator loss: 0.2559461295604706\n",
            "Generator loss: 4.336418151855469, Discriminator loss: 0.13150346279144287\n",
            "Generator loss: 4.398597717285156, Discriminator loss: 0.12762653827667236\n",
            "Generator loss: 4.421008110046387, Discriminator loss: 0.08850681036710739\n",
            "Generator loss: 4.393060684204102, Discriminator loss: 0.1263255625963211\n",
            "Generator loss: 4.4051713943481445, Discriminator loss: 0.1829586923122406\n",
            "Generator loss: 4.575433731079102, Discriminator loss: 0.15880155563354492\n",
            "Generator loss: 4.662812232971191, Discriminator loss: 0.30067795515060425\n",
            "Generator loss: 4.72196102142334, Discriminator loss: 0.20677389204502106\n",
            "Generator loss: 4.671384811401367, Discriminator loss: 0.15213239192962646\n",
            "Generator loss: 4.665290355682373, Discriminator loss: 0.10448124259710312\n",
            "Generator loss: 4.49572229385376, Discriminator loss: 0.13978475332260132\n",
            "Generator loss: 4.4647321701049805, Discriminator loss: 0.12119898200035095\n",
            "Generator loss: 4.539463996887207, Discriminator loss: 0.14083178341388702\n",
            "Generator loss: 4.4681525230407715, Discriminator loss: 0.12905874848365784\n",
            "Generator loss: 4.46818733215332, Discriminator loss: 0.10690443962812424\n",
            "Generator loss: 4.32496976852417, Discriminator loss: 0.24526463449001312\n",
            "Generator loss: 4.324109077453613, Discriminator loss: 0.1143004447221756\n",
            "Generator loss: 4.344598293304443, Discriminator loss: 0.1536116898059845\n",
            "Generator loss: 4.407137870788574, Discriminator loss: 0.10412197560071945\n",
            "Generator loss: 4.402374267578125, Discriminator loss: 0.09006136655807495\n",
            "Generator loss: 4.429226875305176, Discriminator loss: 0.11214028298854828\n",
            "Generator loss: 4.4150567054748535, Discriminator loss: 0.16203859448432922\n",
            "Generator loss: 4.387325286865234, Discriminator loss: 0.13328075408935547\n",
            "Generator loss: 4.3840508460998535, Discriminator loss: 0.086896151304245\n",
            "Generator loss: 4.3736138343811035, Discriminator loss: 0.15085750818252563\n",
            "Generator loss: 4.39360237121582, Discriminator loss: 0.1938958764076233\n",
            "Generator loss: 4.4832658767700195, Discriminator loss: 0.14677104353904724\n",
            "Generator loss: 4.566267967224121, Discriminator loss: 0.1838427484035492\n",
            "Generator loss: 4.499912261962891, Discriminator loss: 0.14975634217262268\n",
            "Generator loss: 4.489530086517334, Discriminator loss: 0.17345938086509705\n",
            "Generator loss: 4.589042663574219, Discriminator loss: 0.18715113401412964\n",
            "Generator loss: 4.768667697906494, Discriminator loss: 0.14922094345092773\n",
            "Generator loss: 4.7613525390625, Discriminator loss: 0.15739867091178894\n",
            "Generator loss: 4.688496112823486, Discriminator loss: 0.21880179643630981\n",
            "Generator loss: 4.596179485321045, Discriminator loss: 0.1597534865140915\n",
            "Generator loss: 4.542234420776367, Discriminator loss: 0.13817447423934937\n",
            "Generator loss: 4.476333141326904, Discriminator loss: 0.10645733028650284\n",
            "Generator loss: 4.408641815185547, Discriminator loss: 0.12620440125465393\n",
            "Generator loss: 4.4228410720825195, Discriminator loss: 0.12028799206018448\n",
            "Generator loss: 4.533867835998535, Discriminator loss: 0.12485016137361526\n",
            "Generator loss: 4.602952003479004, Discriminator loss: 0.19180019199848175\n",
            "Generator loss: 4.624509334564209, Discriminator loss: 0.11839702725410461\n",
            "Generator loss: 4.696205139160156, Discriminator loss: 0.11727041751146317\n",
            "Generator loss: 4.676152229309082, Discriminator loss: 0.18862831592559814\n",
            "Generator loss: 4.729207992553711, Discriminator loss: 0.09337400645017624\n",
            "Generator loss: 4.619717597961426, Discriminator loss: 0.13931263983249664\n",
            "Generator loss: 4.592055320739746, Discriminator loss: 0.13644298911094666\n",
            "Generator loss: 4.569102764129639, Discriminator loss: 0.14568397402763367\n",
            "Generator loss: 4.556184768676758, Discriminator loss: 0.10100308060646057\n",
            "Generator loss: 4.517251491546631, Discriminator loss: 0.11016333848237991\n",
            "Generator loss: 4.324201583862305, Discriminator loss: 0.17730185389518738\n",
            "Generator loss: 4.235719203948975, Discriminator loss: 0.15176953375339508\n",
            "Generator loss: 4.291425704956055, Discriminator loss: 0.23671573400497437\n",
            "Generator loss: 4.405529022216797, Discriminator loss: 0.12162448465824127\n",
            "Generator loss: 4.452033519744873, Discriminator loss: 0.12580958008766174\n",
            "Generator loss: 4.370641708374023, Discriminator loss: 0.132444366812706\n",
            "Generator loss: 4.310469150543213, Discriminator loss: 0.1350192129611969\n",
            "Generator loss: 4.32145881652832, Discriminator loss: 0.1364343762397766\n",
            "Generator loss: 4.290828227996826, Discriminator loss: 0.1634451150894165\n",
            "Generator loss: 4.236196517944336, Discriminator loss: 0.204758882522583\n",
            "Generator loss: 4.294429302215576, Discriminator loss: 0.12566640973091125\n",
            "Generator loss: 4.327906608581543, Discriminator loss: 0.14079028367996216\n",
            "Generator loss: 4.326498985290527, Discriminator loss: 0.09056095033884048\n",
            "Generator loss: 4.376799583435059, Discriminator loss: 0.09227408468723297\n",
            "Generator loss: 4.406213760375977, Discriminator loss: 0.08366865664720535\n",
            "Generator loss: 4.433403491973877, Discriminator loss: 0.08284743130207062\n",
            "Generator loss: 4.3989739418029785, Discriminator loss: 0.1589246243238449\n",
            "Generator loss: 4.445540428161621, Discriminator loss: 0.1527508944272995\n",
            "Generator loss: 4.518757343292236, Discriminator loss: 0.11439625918865204\n",
            "Generator loss: 4.601917266845703, Discriminator loss: 0.09801455587148666\n",
            "Generator loss: 4.60658597946167, Discriminator loss: 0.10059313476085663\n",
            "Generator loss: 4.744380474090576, Discriminator loss: 0.14552104473114014\n",
            "Generator loss: 4.817379951477051, Discriminator loss: 0.09891922771930695\n",
            "Generator loss: 4.899286270141602, Discriminator loss: 0.09900334477424622\n",
            "Generator loss: 4.90577507019043, Discriminator loss: 0.09917066246271133\n",
            "Generator loss: 4.844277381896973, Discriminator loss: 0.12253417819738388\n",
            "Generator loss: 4.841701030731201, Discriminator loss: 0.1957925707101822\n",
            "Generator loss: 4.880983829498291, Discriminator loss: 0.16481542587280273\n",
            "Generator loss: 4.879230499267578, Discriminator loss: 0.12465354055166245\n",
            "Generator loss: 4.79208517074585, Discriminator loss: 0.08413059264421463\n",
            "Generator loss: 4.738642692565918, Discriminator loss: 0.20755809545516968\n",
            "Generator loss: 4.64269495010376, Discriminator loss: 0.1140262708067894\n",
            "Generator loss: 4.5545454025268555, Discriminator loss: 0.06733681261539459\n",
            "Generator loss: 4.423764228820801, Discriminator loss: 0.07788477838039398\n",
            "Generator loss: 4.37393856048584, Discriminator loss: 0.07321061193943024\n",
            "Generator loss: 4.309286594390869, Discriminator loss: 0.10553480684757233\n",
            "Generator loss: 4.321831226348877, Discriminator loss: 0.07854963839054108\n",
            "Generator loss: 4.2754621505737305, Discriminator loss: 0.08602577447891235\n",
            "Generator loss: 4.308403491973877, Discriminator loss: 0.05986570566892624\n",
            "Generator loss: 4.191195011138916, Discriminator loss: 0.06462513655424118\n",
            "Generator loss: 4.110377788543701, Discriminator loss: 0.08801718801259995\n",
            "Generator loss: 4.107398509979248, Discriminator loss: 0.09624125063419342\n",
            "Generator loss: 4.115038871765137, Discriminator loss: 0.0989016741514206\n",
            "Generator loss: 4.176185131072998, Discriminator loss: 0.09028544276952744\n",
            "Generator loss: 4.160722255706787, Discriminator loss: 0.10661818832159042\n",
            "Generator loss: 4.121826171875, Discriminator loss: 0.14437127113342285\n",
            "Generator loss: 4.222927570343018, Discriminator loss: 0.11378449946641922\n",
            "Generator loss: 4.334891319274902, Discriminator loss: 0.09562301635742188\n",
            "Generator loss: 4.416642189025879, Discriminator loss: 0.08203855901956558\n",
            "Generator loss: 4.352548122406006, Discriminator loss: 0.07787717878818512\n",
            "Generator loss: 4.348504066467285, Discriminator loss: 0.0724807158112526\n",
            "Generator loss: 4.294726371765137, Discriminator loss: 0.09987253695726395\n",
            "Generator loss: 4.46110725402832, Discriminator loss: 0.14413407444953918\n",
            "Generator loss: 4.5431928634643555, Discriminator loss: 0.11046364903450012\n",
            "Generator loss: 4.632038593292236, Discriminator loss: 0.17597727477550507\n",
            "Generator loss: 4.600528240203857, Discriminator loss: 0.07243605703115463\n",
            "Generator loss: 4.510223388671875, Discriminator loss: 0.0773748904466629\n",
            "Generator loss: 4.4772419929504395, Discriminator loss: 0.07163506746292114\n",
            "Generator loss: 4.444839000701904, Discriminator loss: 0.08685632050037384\n",
            "Generator loss: 4.486367225646973, Discriminator loss: 0.08232463896274567\n",
            "Generator loss: 4.519292831420898, Discriminator loss: 0.08467759937047958\n",
            "Generator loss: 4.557709693908691, Discriminator loss: 0.07081712782382965\n",
            "Generator loss: 4.564713478088379, Discriminator loss: 0.06497606635093689\n",
            "Generator loss: 4.512156009674072, Discriminator loss: 0.07277783751487732\n",
            "Generator loss: 4.422211647033691, Discriminator loss: 0.06461076438426971\n",
            "Generator loss: 4.411694526672363, Discriminator loss: 0.057747453451156616\n",
            "Generator loss: 4.409060001373291, Discriminator loss: 0.08530375361442566\n",
            "Generator loss: 4.402281761169434, Discriminator loss: 0.06338800489902496\n",
            "Generator loss: 4.424477577209473, Discriminator loss: 0.0618806891143322\n",
            "Generator loss: 4.366446495056152, Discriminator loss: 0.06481506675481796\n",
            "Generator loss: 4.3290605545043945, Discriminator loss: 0.05991823598742485\n",
            "Generator loss: 4.341126441955566, Discriminator loss: 0.07074921578168869\n",
            "Generator loss: 4.440622329711914, Discriminator loss: 0.09028692543506622\n",
            "Generator loss: 4.4961957931518555, Discriminator loss: 0.16333210468292236\n",
            "Generator loss: 4.4873785972595215, Discriminator loss: 0.1347019523382187\n",
            "Generator loss: 4.45212984085083, Discriminator loss: 0.0767097994685173\n",
            "Generator loss: 4.412558555603027, Discriminator loss: 0.09977854788303375\n",
            "Generator loss: 4.397707462310791, Discriminator loss: 0.07951386272907257\n",
            "Generator loss: 4.432583808898926, Discriminator loss: 0.08700288832187653\n",
            "Generator loss: 4.473827838897705, Discriminator loss: 0.07333888113498688\n",
            "Generator loss: 4.505833625793457, Discriminator loss: 0.0864606723189354\n",
            "Generator loss: 4.484285354614258, Discriminator loss: 0.09805189073085785\n",
            "Generator loss: 4.505249977111816, Discriminator loss: 0.07310299575328827\n",
            "Generator loss: 4.571738243103027, Discriminator loss: 0.07694225013256073\n",
            "Generator loss: 4.581006050109863, Discriminator loss: 0.08934370428323746\n",
            "Generator loss: 4.60345983505249, Discriminator loss: 0.09023813158273697\n",
            "Generator loss: 4.557402610778809, Discriminator loss: 0.0794135108590126\n",
            "Generator loss: 4.5717692375183105, Discriminator loss: 0.08484114706516266\n",
            "Generator loss: 4.46090030670166, Discriminator loss: 0.08234331756830215\n",
            "Generator loss: 4.466593265533447, Discriminator loss: 0.11351431906223297\n",
            "Generator loss: 4.456506729125977, Discriminator loss: 0.08511006832122803\n",
            "Generator loss: 4.430482864379883, Discriminator loss: 0.05734143778681755\n",
            "Generator loss: 4.3700642585754395, Discriminator loss: 0.11080671846866608\n",
            "Generator loss: 4.302380561828613, Discriminator loss: 0.05961406230926514\n",
            "Generator loss: 4.254925727844238, Discriminator loss: 0.0802169218659401\n",
            "Generator loss: 4.205437660217285, Discriminator loss: 0.09300386905670166\n",
            "Generator loss: 4.299678802490234, Discriminator loss: 0.07756981998682022\n",
            "Generator loss: 4.367335319519043, Discriminator loss: 0.06559326499700546\n",
            "Generator loss: 4.347270965576172, Discriminator loss: 0.1030726432800293\n",
            "Generator loss: 4.389669418334961, Discriminator loss: 0.07694879919290543\n",
            "Generator loss: 4.413306713104248, Discriminator loss: 0.06901770085096359\n",
            "Generator loss: 4.416191101074219, Discriminator loss: 0.08723991364240646\n",
            "Generator loss: 4.406376838684082, Discriminator loss: 0.09809667617082596\n",
            "Generator loss: 4.420932769775391, Discriminator loss: 0.0743693932890892\n",
            "Generator loss: 4.416475772857666, Discriminator loss: 0.10011765360832214\n",
            "Generator loss: 4.371883392333984, Discriminator loss: 0.07063613831996918\n",
            "Generator loss: 4.3701171875, Discriminator loss: 0.06707832217216492\n",
            "Generator loss: 4.396450519561768, Discriminator loss: 0.06547097861766815\n",
            "Generator loss: 4.404680252075195, Discriminator loss: 0.07995306700468063\n",
            "Generator loss: 4.429158687591553, Discriminator loss: 0.06827837228775024\n",
            "Generator loss: 4.4977192878723145, Discriminator loss: 0.07603318989276886\n",
            "Generator loss: 4.556577682495117, Discriminator loss: 0.08183182030916214\n",
            "Generator loss: 4.637100696563721, Discriminator loss: 0.0798654779791832\n",
            "Generator loss: 4.605905055999756, Discriminator loss: 0.07583382725715637\n",
            "Generator loss: 4.5329155921936035, Discriminator loss: 0.07141155004501343\n",
            "Generator loss: 4.516136646270752, Discriminator loss: 0.0818818137049675\n",
            "Generator loss: 4.562539100646973, Discriminator loss: 0.09047392010688782\n",
            "Generator loss: 4.5986714363098145, Discriminator loss: 0.09512709081172943\n",
            "Generator loss: 4.653598308563232, Discriminator loss: 0.10388631373643875\n",
            "Generator loss: 4.706687927246094, Discriminator loss: 0.12153518199920654\n",
            "Generator loss: 4.732260704040527, Discriminator loss: 0.09466955810785294\n",
            "Generator loss: 4.734001159667969, Discriminator loss: 0.08284503221511841\n",
            "Generator loss: 4.816038608551025, Discriminator loss: 0.1071213111281395\n",
            "Generator loss: 4.906284332275391, Discriminator loss: 0.09618081152439117\n",
            "Generator loss: 4.86823844909668, Discriminator loss: 0.13385479152202606\n",
            "Generator loss: 4.82877254486084, Discriminator loss: 0.10127761214971542\n",
            "Generator loss: 4.777202606201172, Discriminator loss: 0.12622970342636108\n",
            "Generator loss: 4.761503219604492, Discriminator loss: 0.09504050016403198\n",
            "Generator loss: 4.730795383453369, Discriminator loss: 0.11427825689315796\n",
            "Generator loss: 4.6534953117370605, Discriminator loss: 0.14707544445991516\n",
            "Generator loss: 4.649364948272705, Discriminator loss: 0.08276781439781189\n",
            "Generator loss: 4.714147567749023, Discriminator loss: 0.09756103157997131\n",
            "Generator loss: 4.5244035720825195, Discriminator loss: 0.17895996570587158\n",
            "Generator loss: 4.488748550415039, Discriminator loss: 0.08320911228656769\n",
            "Generator loss: 4.569714069366455, Discriminator loss: 0.0917276069521904\n",
            "Generator loss: 4.634082794189453, Discriminator loss: 0.0667194351553917\n",
            "Generator loss: 4.63081693649292, Discriminator loss: 0.08037319034337997\n",
            "Generator loss: 4.548725605010986, Discriminator loss: 0.07308833301067352\n",
            "Generator loss: 4.548317909240723, Discriminator loss: 0.0727807804942131\n",
            "Generator loss: 4.537071704864502, Discriminator loss: 0.09970861673355103\n",
            "Generator loss: 4.497698783874512, Discriminator loss: 0.08147887140512466\n",
            "Generator loss: 4.527523517608643, Discriminator loss: 0.07243568450212479\n",
            "Generator loss: 4.560336112976074, Discriminator loss: 0.07248930633068085\n",
            "Generator loss: 4.517816543579102, Discriminator loss: 0.0901063084602356\n",
            "Generator loss: 4.551666736602783, Discriminator loss: 0.10019382834434509\n",
            "Generator loss: 4.518238544464111, Discriminator loss: 0.12560732662677765\n",
            "Generator loss: 4.579542636871338, Discriminator loss: 0.07016722112894058\n",
            "Generator loss: 4.581200122833252, Discriminator loss: 0.13425607979297638\n",
            "Generator loss: 4.747823238372803, Discriminator loss: 0.15462863445281982\n",
            "Generator loss: 4.823757171630859, Discriminator loss: 0.18278862535953522\n",
            "Generator loss: 4.891312599182129, Discriminator loss: 0.09511733055114746\n",
            "Generator loss: 4.898681163787842, Discriminator loss: 0.13236719369888306\n",
            "Generator loss: 4.976100444793701, Discriminator loss: 0.12462097406387329\n",
            "Generator loss: 5.015779495239258, Discriminator loss: 0.09060985594987869\n",
            "Generator loss: 5.026413440704346, Discriminator loss: 0.11831027269363403\n",
            "Generator loss: 5.055817604064941, Discriminator loss: 0.11180147528648376\n",
            "Generator loss: 4.986336708068848, Discriminator loss: 0.0974591001868248\n",
            "Generator loss: 4.895214080810547, Discriminator loss: 0.0785222128033638\n",
            "Generator loss: 4.72626256942749, Discriminator loss: 0.09244698286056519\n",
            "Generator loss: 4.698812961578369, Discriminator loss: 0.11792578548192978\n",
            "Generator loss: 4.818525314331055, Discriminator loss: 0.11261701583862305\n",
            "Generator loss: 4.869293689727783, Discriminator loss: 0.11501047760248184\n",
            "Generator loss: 4.950534343719482, Discriminator loss: 0.14122642576694489\n",
            "Generator loss: 5.062590599060059, Discriminator loss: 0.12052172422409058\n",
            "Generator loss: 5.093873977661133, Discriminator loss: 0.12884308397769928\n",
            "Generator loss: 4.962136745452881, Discriminator loss: 0.09036685526371002\n",
            "Generator loss: 4.881745338439941, Discriminator loss: 0.09847501665353775\n",
            "Generator loss: 4.921056747436523, Discriminator loss: 0.1394507884979248\n",
            "Generator loss: 4.966697692871094, Discriminator loss: 0.10402634739875793\n",
            "Generator loss: 4.80043888092041, Discriminator loss: 0.13723403215408325\n",
            "Generator loss: 4.75438928604126, Discriminator loss: 0.0864681601524353\n",
            "Generator loss: 4.780787944793701, Discriminator loss: 0.08999788761138916\n",
            "Generator loss: 4.640742778778076, Discriminator loss: 0.14141421020030975\n",
            "Generator loss: 4.6609272956848145, Discriminator loss: 0.09144923835992813\n",
            "Generator loss: 4.606666564941406, Discriminator loss: 0.1080305427312851\n",
            "Generator loss: 4.542227268218994, Discriminator loss: 0.12350787222385406\n",
            "Generator loss: 4.514227867126465, Discriminator loss: 0.11797212809324265\n",
            "Generator loss: 4.537402153015137, Discriminator loss: 0.07266924530267715\n",
            "Generator loss: 4.592057228088379, Discriminator loss: 0.08333469927310944\n",
            "Generator loss: 4.6209821701049805, Discriminator loss: 0.07370894402265549\n",
            "Generator loss: 4.443474292755127, Discriminator loss: 0.11504155397415161\n",
            "Generator loss: 4.437565803527832, Discriminator loss: 0.08786875009536743\n",
            "Generator loss: 4.54237174987793, Discriminator loss: 0.07595765590667725\n",
            "Generator loss: 4.5920257568359375, Discriminator loss: 0.07638075947761536\n",
            "Generator loss: 4.410160541534424, Discriminator loss: 0.10977518558502197\n",
            "Generator loss: 4.4329118728637695, Discriminator loss: 0.09240484237670898\n",
            "Generator loss: 4.493494987487793, Discriminator loss: 0.0982651561498642\n",
            "Generator loss: 4.519279479980469, Discriminator loss: 0.11617329716682434\n",
            "Generator loss: 4.5301055908203125, Discriminator loss: 0.08275368064641953\n",
            "Generator loss: 4.495010852813721, Discriminator loss: 0.09339260309934616\n",
            "Generator loss: 4.433527946472168, Discriminator loss: 0.06692774593830109\n",
            "Generator loss: 4.476794242858887, Discriminator loss: 0.07364414632320404\n",
            "Generator loss: 4.455489158630371, Discriminator loss: 0.14553731679916382\n",
            "Generator loss: 4.447912216186523, Discriminator loss: 0.07587291300296783\n",
            "Generator loss: 4.437605857849121, Discriminator loss: 0.09209708869457245\n",
            "Generator loss: 4.445450782775879, Discriminator loss: 0.0819891095161438\n",
            "Generator loss: 4.484780311584473, Discriminator loss: 0.05794783681631088\n",
            "Generator loss: 4.547494888305664, Discriminator loss: 0.06274065375328064\n",
            "Generator loss: 4.507049560546875, Discriminator loss: 0.14078912138938904\n",
            "Generator loss: 4.499574184417725, Discriminator loss: 0.07281719148159027\n",
            "Generator loss: 4.549692630767822, Discriminator loss: 0.06732600182294846\n",
            "Generator loss: 4.5972747802734375, Discriminator loss: 0.06405320763587952\n",
            "Generator loss: 4.643101692199707, Discriminator loss: 0.06476433575153351\n",
            "Generator loss: 4.649007797241211, Discriminator loss: 0.07440817356109619\n",
            "Generator loss: 4.688469409942627, Discriminator loss: 0.0474054254591465\n",
            "Generator loss: 4.6667656898498535, Discriminator loss: 0.08966211974620819\n",
            "Generator loss: 4.600149631500244, Discriminator loss: 0.06805970519781113\n",
            "Generator loss: 4.6003546714782715, Discriminator loss: 0.0651831328868866\n",
            "Generator loss: 4.631148338317871, Discriminator loss: 0.041062816977500916\n",
            "Generator loss: 4.576386451721191, Discriminator loss: 0.06462696194648743\n",
            "Generator loss: 4.548900604248047, Discriminator loss: 0.04147252440452576\n",
            "Generator loss: 4.532366752624512, Discriminator loss: 0.05239718407392502\n",
            "Generator loss: 4.512691974639893, Discriminator loss: 0.08609044551849365\n",
            "Generator loss: 4.552052021026611, Discriminator loss: 0.04772431403398514\n",
            "Generator loss: 4.598235130310059, Discriminator loss: 0.08204834163188934\n",
            "Generator loss: 4.5963897705078125, Discriminator loss: 0.05180253088474274\n",
            "Generator loss: 4.629886150360107, Discriminator loss: 0.07991699874401093\n",
            "Generator loss: 4.6348161697387695, Discriminator loss: 0.05575883015990257\n",
            "Generator loss: 4.606931209564209, Discriminator loss: 0.04580939561128616\n",
            "Generator loss: 4.586208820343018, Discriminator loss: 0.05317261070013046\n",
            "Generator loss: 4.444502353668213, Discriminator loss: 0.1079455092549324\n",
            "Generator loss: 4.397874355316162, Discriminator loss: 0.06555703282356262\n",
            "Generator loss: 4.511693477630615, Discriminator loss: 0.053929463028907776\n",
            "Generator loss: 4.542774677276611, Discriminator loss: 0.05525580793619156\n",
            "Generator loss: 4.5574846267700195, Discriminator loss: 0.06334170699119568\n",
            "Generator loss: 4.559825420379639, Discriminator loss: 0.048287488520145416\n",
            "Generator loss: 4.464910507202148, Discriminator loss: 0.05382803454995155\n",
            "Generator loss: 4.475481986999512, Discriminator loss: 0.04643522575497627\n",
            "Generator loss: 4.425747871398926, Discriminator loss: 0.07183898985385895\n",
            "Generator loss: 4.529919147491455, Discriminator loss: 0.06527981162071228\n",
            "Generator loss: 4.587350845336914, Discriminator loss: 0.07139599323272705\n",
            "Generator loss: 4.649364471435547, Discriminator loss: 0.05651722848415375\n",
            "Generator loss: 4.640708923339844, Discriminator loss: 0.06853878498077393\n",
            "Generator loss: 4.599828720092773, Discriminator loss: 0.04765726998448372\n",
            "Generator loss: 4.589034557342529, Discriminator loss: 0.05749237537384033\n",
            "Generator loss: 4.544146537780762, Discriminator loss: 0.05847892910242081\n",
            "Generator loss: 4.584542274475098, Discriminator loss: 0.07725256681442261\n",
            "Generator loss: 4.6540021896362305, Discriminator loss: 0.08132819086313248\n",
            "Generator loss: 4.714472770690918, Discriminator loss: 0.07617857307195663\n",
            "Generator loss: 4.723073959350586, Discriminator loss: 0.06196163594722748\n",
            "Generator loss: 4.682703495025635, Discriminator loss: 0.09065760672092438\n",
            "Generator loss: 4.609277725219727, Discriminator loss: 0.05547843873500824\n",
            "Generator loss: 4.620773792266846, Discriminator loss: 0.06726030260324478\n",
            "Generator loss: 4.576896667480469, Discriminator loss: 0.05894780531525612\n",
            "Generator loss: 4.527324676513672, Discriminator loss: 0.052016012370586395\n",
            "Generator loss: 4.554600238800049, Discriminator loss: 0.04929167032241821\n",
            "Generator loss: 4.540908336639404, Discriminator loss: 0.07107330858707428\n",
            "Generator loss: 4.514580726623535, Discriminator loss: 0.07621122151613235\n",
            "Generator loss: 4.517125129699707, Discriminator loss: 0.06057360768318176\n",
            "Generator loss: 4.542417526245117, Discriminator loss: 0.05335266888141632\n",
            "Generator loss: 4.367673873901367, Discriminator loss: 0.13227851688861847\n",
            "Generator loss: 4.373383522033691, Discriminator loss: 0.05541439354419708\n",
            "Generator loss: 4.407682418823242, Discriminator loss: 0.07074500620365143\n",
            "Generator loss: 4.485371112823486, Discriminator loss: 0.05236421898007393\n",
            "Generator loss: 4.498878479003906, Discriminator loss: 0.05200760439038277\n",
            "Generator loss: 4.458908557891846, Discriminator loss: 0.06868867576122284\n",
            "Generator loss: 4.422496318817139, Discriminator loss: 0.05988041311502457\n",
            "Generator loss: 4.434670925140381, Discriminator loss: 0.06590145081281662\n",
            "Generator loss: 4.513160705566406, Discriminator loss: 0.062469031661748886\n",
            "Generator loss: 4.527850151062012, Discriminator loss: 0.09162081778049469\n",
            "Generator loss: 4.554457187652588, Discriminator loss: 0.06263989210128784\n",
            "Generator loss: 4.6435322761535645, Discriminator loss: 0.06207998842000961\n",
            "Generator loss: 4.553741931915283, Discriminator loss: 0.10222676396369934\n",
            "Generator loss: 4.49763298034668, Discriminator loss: 0.078036367893219\n",
            "Generator loss: 4.537233352661133, Discriminator loss: 0.05861601606011391\n",
            "Generator loss: 4.605023384094238, Discriminator loss: 0.05924272537231445\n",
            "Generator loss: 4.497231960296631, Discriminator loss: 0.127055823802948\n",
            "Generator loss: 4.457009315490723, Discriminator loss: 0.09965845942497253\n",
            "Generator loss: 4.511870384216309, Discriminator loss: 0.06888581067323685\n",
            "Generator loss: 4.627202987670898, Discriminator loss: 0.044211920350790024\n",
            "Generator loss: 4.526663303375244, Discriminator loss: 0.08201295137405396\n",
            "Generator loss: 4.38612174987793, Discriminator loss: 0.10561003535985947\n",
            "Generator loss: 4.350191116333008, Discriminator loss: 0.11509421467781067\n",
            "Generator loss: 4.460616111755371, Discriminator loss: 0.10283499956130981\n",
            "Generator loss: 4.546966552734375, Discriminator loss: 0.15243172645568848\n",
            "Generator loss: 4.488783836364746, Discriminator loss: 0.14437229931354523\n",
            "Generator loss: 4.550868034362793, Discriminator loss: 0.0660124123096466\n",
            "Generator loss: 4.525323390960693, Discriminator loss: 0.1344090849161148\n",
            "Generator loss: 4.5253143310546875, Discriminator loss: 0.0761132687330246\n",
            "Generator loss: 4.480915069580078, Discriminator loss: 0.18971684575080872\n",
            "Generator loss: 4.628286838531494, Discriminator loss: 0.07734338939189911\n",
            "Generator loss: 4.722507476806641, Discriminator loss: 0.10275471210479736\n",
            "Generator loss: 4.733622074127197, Discriminator loss: 0.15597845613956451\n",
            "Generator loss: 4.770018577575684, Discriminator loss: 0.08740466833114624\n",
            "Generator loss: 4.926259517669678, Discriminator loss: 0.11375019699335098\n",
            "Generator loss: 5.099143028259277, Discriminator loss: 0.14999929070472717\n",
            "Generator loss: 5.331130504608154, Discriminator loss: 0.1367816925048828\n",
            "Generator loss: 5.348386764526367, Discriminator loss: 0.08737174421548843\n",
            "Generator loss: 5.121510982513428, Discriminator loss: 0.09555225074291229\n",
            "Generator loss: 4.859130859375, Discriminator loss: 0.057889096438884735\n",
            "Generator loss: 4.692818641662598, Discriminator loss: 0.1332261562347412\n",
            "Generator loss: 4.710916519165039, Discriminator loss: 0.06989992409944534\n",
            "Generator loss: 4.764317512512207, Discriminator loss: 0.08701637387275696\n",
            "Generator loss: 4.820967674255371, Discriminator loss: 0.07420188933610916\n",
            "Generator loss: 4.7736921310424805, Discriminator loss: 0.06401549279689789\n",
            "Generator loss: 4.76590633392334, Discriminator loss: 0.07221557199954987\n",
            "Generator loss: 4.794907093048096, Discriminator loss: 0.0640074610710144\n",
            "Generator loss: 4.8506059646606445, Discriminator loss: 0.07136650383472443\n",
            "Generator loss: 4.855691432952881, Discriminator loss: 0.11645621061325073\n",
            "Generator loss: 4.893507957458496, Discriminator loss: 0.11091795563697815\n",
            "Generator loss: 4.957457065582275, Discriminator loss: 0.05811060965061188\n",
            "Generator loss: 4.768512725830078, Discriminator loss: 0.07694454491138458\n",
            "Generator loss: 4.712615489959717, Discriminator loss: 0.06007545813918114\n",
            "Generator loss: 4.604536056518555, Discriminator loss: 0.08364804089069366\n",
            "Generator loss: 4.616504192352295, Discriminator loss: 0.06472542881965637\n",
            "Generator loss: 4.667062759399414, Discriminator loss: 0.0829717367887497\n",
            "Generator loss: 4.722072124481201, Discriminator loss: 0.10591830313205719\n",
            "Generator loss: 4.6866559982299805, Discriminator loss: 0.03904419392347336\n",
            "Generator loss: 4.645711898803711, Discriminator loss: 0.05381304398179054\n",
            "Generator loss: 4.703513145446777, Discriminator loss: 0.062284160405397415\n",
            "Generator loss: 4.597672462463379, Discriminator loss: 0.0563262514770031\n",
            "Generator loss: 4.566922187805176, Discriminator loss: 0.06006443500518799\n",
            "Generator loss: 4.574226379394531, Discriminator loss: 0.059659793972969055\n",
            "Generator loss: 4.580406665802002, Discriminator loss: 0.05194415897130966\n",
            "Generator loss: 4.573079586029053, Discriminator loss: 0.06918013840913773\n",
            "Generator loss: 4.595902919769287, Discriminator loss: 0.06346265971660614\n",
            "Generator loss: 4.6423540115356445, Discriminator loss: 0.09421688318252563\n",
            "Generator loss: 4.642692565917969, Discriminator loss: 0.08317050337791443\n",
            "Generator loss: 4.637927055358887, Discriminator loss: 0.09770959615707397\n",
            "Generator loss: 4.705128192901611, Discriminator loss: 0.041121065616607666\n",
            "Generator loss: 4.671567916870117, Discriminator loss: 0.05203579366207123\n",
            "Generator loss: 4.657919883728027, Discriminator loss: 0.05926501378417015\n",
            "Generator loss: 4.688236236572266, Discriminator loss: 0.05673513188958168\n",
            "Generator loss: 4.6991167068481445, Discriminator loss: 0.055164456367492676\n",
            "Generator loss: 4.692338466644287, Discriminator loss: 0.045758750289678574\n",
            "Generator loss: 4.682008743286133, Discriminator loss: 0.0485847108066082\n",
            "Generator loss: 4.645654201507568, Discriminator loss: 0.06992530077695847\n",
            "Generator loss: 4.6626996994018555, Discriminator loss: 0.0655466616153717\n",
            "Generator loss: 4.71747350692749, Discriminator loss: 0.05257895961403847\n",
            "Generator loss: 4.693180561065674, Discriminator loss: 0.0983390212059021\n",
            "Generator loss: 4.654478549957275, Discriminator loss: 0.11943802237510681\n",
            "Generator loss: 4.7572431564331055, Discriminator loss: 0.07399190962314606\n",
            "Generator loss: 4.854860305786133, Discriminator loss: 0.08236414194107056\n",
            "Generator loss: 4.869173526763916, Discriminator loss: 0.05022118240594864\n",
            "Generator loss: 4.774381637573242, Discriminator loss: 0.07412727177143097\n",
            "Generator loss: 4.678420066833496, Discriminator loss: 0.07405032962560654\n",
            "Generator loss: 4.62732458114624, Discriminator loss: 0.05095706507563591\n",
            "Generator loss: 4.586109638214111, Discriminator loss: 0.06093002110719681\n",
            "Generator loss: 4.57713508605957, Discriminator loss: 0.055096887052059174\n",
            "Generator loss: 4.605645179748535, Discriminator loss: 0.05348493531346321\n",
            "Generator loss: 4.643581390380859, Discriminator loss: 0.06354789435863495\n",
            "Generator loss: 4.665663719177246, Discriminator loss: 0.059897929430007935\n",
            "Generator loss: 4.6862101554870605, Discriminator loss: 0.06881057471036911\n",
            "Generator loss: 4.67105770111084, Discriminator loss: 0.056867294013500214\n",
            "Generator loss: 4.710900783538818, Discriminator loss: 0.044581733644008636\n",
            "Generator loss: 4.604465961456299, Discriminator loss: 0.1065298467874527\n",
            "Generator loss: 4.6032609939575195, Discriminator loss: 0.043887410312891006\n",
            "Generator loss: 4.617759704589844, Discriminator loss: 0.04902881383895874\n",
            "Generator loss: 4.542296409606934, Discriminator loss: 0.04525333270430565\n",
            "Generator loss: 4.53782320022583, Discriminator loss: 0.06720997393131256\n",
            "Generator loss: 4.562763214111328, Discriminator loss: 0.07431285083293915\n",
            "Generator loss: 4.537580490112305, Discriminator loss: 0.04148088023066521\n",
            "Generator loss: 4.51826810836792, Discriminator loss: 0.13587506115436554\n",
            "Generator loss: 4.500023365020752, Discriminator loss: 0.08656167984008789\n",
            "Generator loss: 4.47329044342041, Discriminator loss: 0.055277206003665924\n",
            "Generator loss: 4.615321636199951, Discriminator loss: 0.17917591333389282\n",
            "Generator loss: 4.663206577301025, Discriminator loss: 0.06896574795246124\n",
            "Generator loss: 4.608231067657471, Discriminator loss: 0.05625586211681366\n",
            "Generator loss: 4.560421943664551, Discriminator loss: 0.05963017791509628\n",
            "Generator loss: 4.522897720336914, Discriminator loss: 0.08285526931285858\n",
            "Generator loss: 4.536233901977539, Discriminator loss: 0.05794277414679527\n",
            "Generator loss: 4.617760181427002, Discriminator loss: 0.059451423585414886\n",
            "Generator loss: 4.676083087921143, Discriminator loss: 0.05038486048579216\n",
            "Generator loss: 4.614388465881348, Discriminator loss: 0.05357998609542847\n",
            "Generator loss: 4.610427379608154, Discriminator loss: 0.059338610619306564\n",
            "Generator loss: 4.610997200012207, Discriminator loss: 0.09934835135936737\n",
            "Generator loss: 4.6036696434021, Discriminator loss: 0.06498396396636963\n",
            "Generator loss: 4.5918684005737305, Discriminator loss: 0.0962064266204834\n",
            "Generator loss: 4.57489013671875, Discriminator loss: 0.08463314175605774\n",
            "Generator loss: 4.638364315032959, Discriminator loss: 0.07629527151584625\n",
            "Generator loss: 4.68585205078125, Discriminator loss: 0.10696026682853699\n",
            "Generator loss: 4.760349750518799, Discriminator loss: 0.09105951339006424\n",
            "Generator loss: 4.826830863952637, Discriminator loss: 0.06527234613895416\n",
            "Generator loss: 4.734992504119873, Discriminator loss: 0.0766504779458046\n",
            "Generator loss: 4.701357841491699, Discriminator loss: 0.07748281955718994\n",
            "Generator loss: 4.5961456298828125, Discriminator loss: 0.12168470025062561\n",
            "Generator loss: 4.394566535949707, Discriminator loss: 0.17236511409282684\n",
            "Generator loss: 4.47244119644165, Discriminator loss: 0.14533309638500214\n",
            "Generator loss: 4.8023200035095215, Discriminator loss: 0.07085102051496506\n",
            "Generator loss: 4.67619514465332, Discriminator loss: 0.2556244730949402\n",
            "Generator loss: 4.566413402557373, Discriminator loss: 0.07460308074951172\n",
            "Generator loss: 4.61897087097168, Discriminator loss: 0.06368538737297058\n",
            "Generator loss: 4.513791084289551, Discriminator loss: 0.08915992081165314\n",
            "Generator loss: 4.221319198608398, Discriminator loss: 0.164701446890831\n",
            "Generator loss: 4.288565158843994, Discriminator loss: 0.11286063492298126\n",
            "Generator loss: 4.52178430557251, Discriminator loss: 0.10931656509637833\n",
            "Generator loss: 4.510583877563477, Discriminator loss: 0.12374778836965561\n",
            "Generator loss: 4.516926288604736, Discriminator loss: 0.06690028309822083\n",
            "Generator loss: 4.6204962730407715, Discriminator loss: 0.05482494831085205\n",
            "Generator loss: 4.570670127868652, Discriminator loss: 0.07602804899215698\n",
            "Generator loss: 4.5666937828063965, Discriminator loss: 0.048945095390081406\n",
            "Generator loss: 4.397531986236572, Discriminator loss: 0.1320459544658661\n",
            "Generator loss: 4.43236780166626, Discriminator loss: 0.08343973755836487\n",
            "Generator loss: 4.567863464355469, Discriminator loss: 0.0513022243976593\n",
            "Generator loss: 4.572834014892578, Discriminator loss: 0.07391448318958282\n",
            "Generator loss: 4.106293678283691, Discriminator loss: 0.21197250485420227\n",
            "Generator loss: 4.111572265625, Discriminator loss: 0.08717269450426102\n",
            "Generator loss: 4.240683555603027, Discriminator loss: 0.16229726374149323\n",
            "Generator loss: 4.457432746887207, Discriminator loss: 0.09867516905069351\n",
            "Generator loss: 4.44711971282959, Discriminator loss: 0.12862379848957062\n",
            "Generator loss: 4.473042964935303, Discriminator loss: 0.08532044291496277\n",
            "Generator loss: 4.601587295532227, Discriminator loss: 0.04808148369193077\n",
            "Generator loss: 4.60477876663208, Discriminator loss: 0.06993381679058075\n",
            "Generator loss: 4.317312717437744, Discriminator loss: 0.11967521905899048\n",
            "Generator loss: 4.3034539222717285, Discriminator loss: 0.10053087770938873\n",
            "Generator loss: 4.362502098083496, Discriminator loss: 0.06566549837589264\n",
            "Generator loss: 4.418905258178711, Discriminator loss: 0.07600025832653046\n",
            "Generator loss: 4.591254711151123, Discriminator loss: 0.04288357123732567\n",
            "Generator loss: 4.741135597229004, Discriminator loss: 0.03749086335301399\n",
            "Generator loss: 4.5340423583984375, Discriminator loss: 0.07006314396858215\n",
            "Generator loss: 4.57757568359375, Discriminator loss: 0.02976010926067829\n",
            "Generator loss: 4.490487575531006, Discriminator loss: 0.038229018449783325\n",
            "Generator loss: 4.425774574279785, Discriminator loss: 0.061443597078323364\n",
            "Generator loss: 4.120927810668945, Discriminator loss: 0.18071232736110687\n",
            "Generator loss: 4.0407843589782715, Discriminator loss: 0.1069493442773819\n",
            "Generator loss: 4.268917083740234, Discriminator loss: 0.061641350388526917\n",
            "Generator loss: 4.386585235595703, Discriminator loss: 0.0751645416021347\n",
            "Generator loss: 4.5430426597595215, Discriminator loss: 0.07960893958806992\n",
            "Generator loss: 4.49058723449707, Discriminator loss: 0.06266308575868607\n",
            "Generator loss: 4.295102119445801, Discriminator loss: 0.14805501699447632\n",
            "Generator loss: 4.142095565795898, Discriminator loss: 0.0736713632941246\n",
            "Generator loss: 4.241912841796875, Discriminator loss: 0.06434174627065659\n",
            "Generator loss: 4.433194160461426, Discriminator loss: 0.04285580664873123\n",
            "Generator loss: 4.500408172607422, Discriminator loss: 0.039388131350278854\n",
            "Generator loss: 4.566667556762695, Discriminator loss: 0.030040398240089417\n",
            "Generator loss: 4.481024265289307, Discriminator loss: 0.07270936667919159\n",
            "Generator loss: 4.315319538116455, Discriminator loss: 0.07988469302654266\n",
            "Generator loss: 4.1028733253479, Discriminator loss: 0.08732602000236511\n",
            "Generator loss: 4.211703300476074, Discriminator loss: 0.04457786679267883\n",
            "Generator loss: 4.359865665435791, Discriminator loss: 0.05316760018467903\n",
            "Generator loss: 4.412570953369141, Discriminator loss: 0.04858332499861717\n",
            "Generator loss: 4.4583420753479, Discriminator loss: 0.038239769637584686\n",
            "Generator loss: 4.374999046325684, Discriminator loss: 0.07362004369497299\n",
            "Generator loss: 4.019316673278809, Discriminator loss: 0.17958033084869385\n",
            "Generator loss: 4.037520885467529, Discriminator loss: 0.1007550060749054\n",
            "Generator loss: 4.149786472320557, Discriminator loss: 0.10901392251253128\n",
            "Generator loss: 4.396698951721191, Discriminator loss: 0.07756615430116653\n",
            "Generator loss: 4.3102192878723145, Discriminator loss: 0.2246045470237732\n",
            "Generator loss: 4.415026664733887, Discriminator loss: 0.10042320191860199\n",
            "Generator loss: 4.611898422241211, Discriminator loss: 0.06557878106832504\n",
            "Generator loss: 4.68155574798584, Discriminator loss: 0.07075051218271255\n",
            "Generator loss: 4.604822635650635, Discriminator loss: 0.06804688274860382\n",
            "Generator loss: 4.369275093078613, Discriminator loss: 0.08035405725240707\n",
            "Generator loss: 4.31934928894043, Discriminator loss: 0.07791198045015335\n",
            "Generator loss: 4.051233768463135, Discriminator loss: 0.2879974842071533\n",
            "Generator loss: 4.332637310028076, Discriminator loss: 0.13205961883068085\n",
            "Generator loss: 4.677030086517334, Discriminator loss: 0.07657898962497711\n",
            "Generator loss: 4.775622844696045, Discriminator loss: 0.0733666867017746\n",
            "Generator loss: 4.640353202819824, Discriminator loss: 0.07388882339000702\n",
            "Generator loss: 4.447651386260986, Discriminator loss: 0.24448275566101074\n",
            "Generator loss: 4.5477800369262695, Discriminator loss: 0.15379193425178528\n",
            "Generator loss: 4.986346244812012, Discriminator loss: 0.09661353379487991\n",
            "Generator loss: 5.1931915283203125, Discriminator loss: 0.0899726152420044\n",
            "Generator loss: 4.835563659667969, Discriminator loss: 0.1283877193927765\n",
            "Generator loss: 4.712270736694336, Discriminator loss: 0.16084399819374084\n",
            "Generator loss: 4.584909915924072, Discriminator loss: 0.23132362961769104\n",
            "Generator loss: 4.931766986846924, Discriminator loss: 0.3531743288040161\n",
            "Generator loss: 3.9316015243530273, Discriminator loss: 0.531133234500885\n",
            "Generator loss: 5.322715759277344, Discriminator loss: 0.3191804885864258\n",
            "Generator loss: 5.6116204261779785, Discriminator loss: 0.09739761054515839\n",
            "Generator loss: 5.249461650848389, Discriminator loss: 0.0952392965555191\n",
            "Generator loss: 4.002470970153809, Discriminator loss: 0.22934460639953613\n",
            "Generator loss: 4.794685363769531, Discriminator loss: 0.3561204671859741\n",
            "Generator loss: 5.1364426612854, Discriminator loss: 0.2168499082326889\n",
            "Generator loss: 4.823785781860352, Discriminator loss: 0.10602588951587677\n",
            "Generator loss: 3.958155632019043, Discriminator loss: 0.2558952271938324\n",
            "Generator loss: 4.728731155395508, Discriminator loss: 0.22886045277118683\n",
            "Generator loss: 4.226077079772949, Discriminator loss: 0.32414674758911133\n",
            "Generator loss: 4.7352142333984375, Discriminator loss: 0.1256544440984726\n",
            "Generator loss: 4.837782382965088, Discriminator loss: 0.13547298312187195\n",
            "Generator loss: 4.334151268005371, Discriminator loss: 0.1678163707256317\n",
            "Generator loss: 4.4780097007751465, Discriminator loss: 0.10968862473964691\n",
            "Generator loss: 4.67556619644165, Discriminator loss: 0.11761819571256638\n",
            "Generator loss: 4.4959540367126465, Discriminator loss: 0.1501857340335846\n",
            "Generator loss: 4.213184356689453, Discriminator loss: 0.18077653646469116\n",
            "Generator loss: 4.479676246643066, Discriminator loss: 0.16565074026584625\n",
            "Generator loss: 4.909064292907715, Discriminator loss: 0.09485577046871185\n",
            "Generator loss: 4.1906585693359375, Discriminator loss: 0.19285164773464203\n",
            "Generator loss: 4.116815090179443, Discriminator loss: 0.19789618253707886\n",
            "Generator loss: 4.192277908325195, Discriminator loss: 0.21373537182807922\n",
            "Generator loss: 4.111715316772461, Discriminator loss: 0.18452097475528717\n",
            "Generator loss: 4.391379356384277, Discriminator loss: 0.12509240210056305\n",
            "Generator loss: 3.7755751609802246, Discriminator loss: 0.21946394443511963\n",
            "Generator loss: 4.560287952423096, Discriminator loss: 0.1653687059879303\n",
            "Generator loss: 3.5345633029937744, Discriminator loss: 0.2662632167339325\n",
            "Generator loss: 3.513993501663208, Discriminator loss: 0.37604814767837524\n",
            "Generator loss: 4.6407999992370605, Discriminator loss: 0.1740952432155609\n",
            "Generator loss: 4.737903118133545, Discriminator loss: 0.10918829590082169\n",
            "Generator loss: 4.136404037475586, Discriminator loss: 0.10446944832801819\n",
            "Generator loss: 3.4844284057617188, Discriminator loss: 0.15788407623767853\n",
            "Generator loss: 4.280373573303223, Discriminator loss: 0.12581859529018402\n",
            "Generator loss: 3.897608757019043, Discriminator loss: 0.22056525945663452\n",
            "Generator loss: 3.8993191719055176, Discriminator loss: 0.12748084962368011\n",
            "Generator loss: 4.107952117919922, Discriminator loss: 0.12633031606674194\n",
            "Generator loss: 4.471339225769043, Discriminator loss: 0.1109810322523117\n",
            "Generator loss: 4.343592166900635, Discriminator loss: 0.11176382750272751\n",
            "Generator loss: 2.682170867919922, Discriminator loss: 0.33892542123794556\n",
            "Generator loss: 4.171565532684326, Discriminator loss: 0.316169410943985\n",
            "Generator loss: 4.307539939880371, Discriminator loss: 0.20805081725120544\n",
            "Generator loss: 2.9958221912384033, Discriminator loss: 0.5258238911628723\n",
            "Generator loss: 3.6341500282287598, Discriminator loss: 0.19053733348846436\n",
            "Generator loss: 4.089748382568359, Discriminator loss: 0.1451168656349182\n",
            "Generator loss: 3.9741110801696777, Discriminator loss: 0.17538709938526154\n",
            "Generator loss: 4.045097351074219, Discriminator loss: 0.12158890813589096\n",
            "Generator loss: 3.349667549133301, Discriminator loss: 0.2778785228729248\n",
            "Generator loss: 3.9113941192626953, Discriminator loss: 0.1461237370967865\n",
            "Generator loss: 4.261222839355469, Discriminator loss: 0.14161938428878784\n",
            "Generator loss: 3.8845510482788086, Discriminator loss: 0.18779043853282928\n",
            "Generator loss: 3.5708885192871094, Discriminator loss: 0.22815996408462524\n",
            "Generator loss: 3.4035496711730957, Discriminator loss: 0.34222540259361267\n",
            "Generator loss: 3.670713424682617, Discriminator loss: 0.24167679250240326\n",
            "Generator loss: 4.1099772453308105, Discriminator loss: 0.14461012184619904\n",
            "Generator loss: 4.402663230895996, Discriminator loss: 0.10250408947467804\n",
            "Generator loss: 4.029829025268555, Discriminator loss: 0.13162004947662354\n",
            "Generator loss: 3.6006298065185547, Discriminator loss: 0.23551394045352936\n",
            "Generator loss: 3.874946117401123, Discriminator loss: 0.18989187479019165\n",
            "Generator loss: 3.7704315185546875, Discriminator loss: 0.20044414699077606\n",
            "Generator loss: 2.6548333168029785, Discriminator loss: 0.5544552803039551\n",
            "Generator loss: 2.965728759765625, Discriminator loss: 0.4404281973838806\n",
            "Generator loss: 3.647315740585327, Discriminator loss: 0.36421269178390503\n",
            "Generator loss: 3.4973857402801514, Discriminator loss: 0.249657541513443\n",
            "Generator loss: 3.6837074756622314, Discriminator loss: 0.24010327458381653\n",
            "Generator loss: 3.903048276901245, Discriminator loss: 0.18237456679344177\n",
            "Generator loss: 4.280580520629883, Discriminator loss: 0.07749880850315094\n",
            "Generator loss: 3.95658802986145, Discriminator loss: 0.1794900745153427\n",
            "Generator loss: 3.5581629276275635, Discriminator loss: 0.13566546142101288\n",
            "Generator loss: 3.806180715560913, Discriminator loss: 0.11966124922037125\n",
            "Generator loss: 3.729721784591675, Discriminator loss: 0.19015493988990784\n",
            "Generator loss: 3.830439805984497, Discriminator loss: 0.16815420985221863\n",
            "Generator loss: 3.2349627017974854, Discriminator loss: 0.33690667152404785\n",
            "Generator loss: 3.371006965637207, Discriminator loss: 0.2602531313896179\n",
            "Generator loss: 4.106563568115234, Discriminator loss: 0.1620892584323883\n",
            "Generator loss: 4.102727890014648, Discriminator loss: 0.2155279517173767\n",
            "Generator loss: 3.111161708831787, Discriminator loss: 0.3347223103046417\n",
            "Generator loss: 4.681615829467773, Discriminator loss: 0.17942723631858826\n",
            "Generator loss: 4.165704727172852, Discriminator loss: 0.24334688484668732\n",
            "Generator loss: 3.905697822570801, Discriminator loss: 0.09859179705381393\n",
            "Generator loss: 3.9847941398620605, Discriminator loss: 0.1051441878080368\n",
            "Generator loss: 4.13870906829834, Discriminator loss: 0.08571752905845642\n",
            "Generator loss: 3.7974586486816406, Discriminator loss: 0.14673414826393127\n",
            "Generator loss: 3.866888999938965, Discriminator loss: 0.09896154701709747\n",
            "Generator loss: 4.004299163818359, Discriminator loss: 0.12172570824623108\n",
            "Generator loss: 3.45404314994812, Discriminator loss: 0.2601664066314697\n",
            "Generator loss: 3.6326968669891357, Discriminator loss: 0.15407276153564453\n",
            "Generator loss: 4.464908599853516, Discriminator loss: 0.07307617366313934\n",
            "Generator loss: 4.229787826538086, Discriminator loss: 0.11366340517997742\n",
            "Generator loss: 3.472754716873169, Discriminator loss: 0.21672967076301575\n",
            "Generator loss: 3.9310622215270996, Discriminator loss: 0.13488927483558655\n",
            "Generator loss: 4.6026740074157715, Discriminator loss: 0.07391884922981262\n",
            "Generator loss: 4.643861293792725, Discriminator loss: 0.07528205215930939\n",
            "Generator loss: 3.8426969051361084, Discriminator loss: 0.1547594666481018\n",
            "Generator loss: 3.8514580726623535, Discriminator loss: 0.09901759028434753\n",
            "Generator loss: 4.259710788726807, Discriminator loss: 0.08499331772327423\n",
            "Generator loss: 3.1689624786376953, Discriminator loss: 0.24268528819084167\n",
            "Generator loss: 4.111584663391113, Discriminator loss: 0.12510840594768524\n",
            "Generator loss: 4.466568946838379, Discriminator loss: 0.12109212577342987\n",
            "Generator loss: 3.795502185821533, Discriminator loss: 0.20669320225715637\n",
            "Generator loss: 3.981283187866211, Discriminator loss: 0.07066481560468674\n",
            "Generator loss: 4.3237080574035645, Discriminator loss: 0.06924953311681747\n",
            "Generator loss: 4.119721412658691, Discriminator loss: 0.1018601506948471\n",
            "Generator loss: 3.7676849365234375, Discriminator loss: 0.14238975942134857\n",
            "Generator loss: 2.6987013816833496, Discriminator loss: 0.3066139817237854\n",
            "Generator loss: 4.442275524139404, Discriminator loss: 0.2332795262336731\n",
            "Generator loss: 4.171000003814697, Discriminator loss: 0.19679920375347137\n",
            "Generator loss: 4.273094177246094, Discriminator loss: 0.07351639866828918\n",
            "Generator loss: 3.769955635070801, Discriminator loss: 0.179855078458786\n",
            "Generator loss: 4.007379531860352, Discriminator loss: 0.10569163411855698\n",
            "Generator loss: 4.310746192932129, Discriminator loss: 0.0934453159570694\n",
            "Generator loss: 3.6738576889038086, Discriminator loss: 0.17850396037101746\n",
            "Generator loss: 3.313751459121704, Discriminator loss: 0.19112250208854675\n",
            "Generator loss: 4.1096601486206055, Discriminator loss: 0.1251562237739563\n",
            "Generator loss: 4.442602157592773, Discriminator loss: 0.08203325420618057\n",
            "Generator loss: 4.613945007324219, Discriminator loss: 0.04788501560688019\n",
            "Generator loss: 3.935865879058838, Discriminator loss: 0.15026752650737762\n",
            "Generator loss: 3.828941822052002, Discriminator loss: 0.07244861125946045\n",
            "Generator loss: 3.397287368774414, Discriminator loss: 0.1565498560667038\n",
            "Generator loss: 3.8120522499084473, Discriminator loss: 0.13737814128398895\n",
            "Generator loss: 4.003417491912842, Discriminator loss: 0.12940187752246857\n",
            "Generator loss: 4.146919250488281, Discriminator loss: 0.09312377870082855\n",
            "Generator loss: 3.478104829788208, Discriminator loss: 0.18544921278953552\n",
            "Generator loss: 3.2159931659698486, Discriminator loss: 0.2845863103866577\n",
            "Generator loss: 3.9499573707580566, Discriminator loss: 0.22489424049854279\n",
            "Generator loss: 3.9696342945098877, Discriminator loss: 0.21158955991268158\n",
            "Generator loss: 3.8225626945495605, Discriminator loss: 0.18174485862255096\n",
            "Generator loss: 3.928248643875122, Discriminator loss: 0.15563637018203735\n",
            "Generator loss: 4.275278568267822, Discriminator loss: 0.09936179965734482\n",
            "Generator loss: 3.6078197956085205, Discriminator loss: 0.17302674055099487\n",
            "Generator loss: 2.8408801555633545, Discriminator loss: 0.286773145198822\n",
            "Generator loss: 4.488334655761719, Discriminator loss: 0.2530529499053955\n",
            "Generator loss: 4.739884853363037, Discriminator loss: 0.13426053524017334\n",
            "Generator loss: 3.9251766204833984, Discriminator loss: 0.199965700507164\n",
            "Generator loss: 3.962893009185791, Discriminator loss: 0.1029348373413086\n",
            "Generator loss: 4.046319484710693, Discriminator loss: 0.1504945605993271\n",
            "Generator loss: 3.6713197231292725, Discriminator loss: 0.16767480969429016\n",
            "Generator loss: 3.682849884033203, Discriminator loss: 0.16285476088523865\n",
            "Generator loss: 3.5381133556365967, Discriminator loss: 0.22309744358062744\n",
            "Generator loss: 3.8169775009155273, Discriminator loss: 0.1755676567554474\n",
            "Generator loss: 3.915729284286499, Discriminator loss: 0.18456889688968658\n",
            "Generator loss: 4.4601640701293945, Discriminator loss: 0.0998617634177208\n",
            "Generator loss: 4.607853889465332, Discriminator loss: 0.1004629135131836\n",
            "Generator loss: 4.195412635803223, Discriminator loss: 0.09210139513015747\n",
            "Generator loss: 3.578777313232422, Discriminator loss: 0.15100440382957458\n",
            "Generator loss: 4.589722156524658, Discriminator loss: 0.14767996966838837\n",
            "Generator loss: 4.50076961517334, Discriminator loss: 0.13613033294677734\n",
            "Generator loss: 4.261693000793457, Discriminator loss: 0.11118710041046143\n",
            "Generator loss: 4.147871017456055, Discriminator loss: 0.15477797389030457\n",
            "Generator loss: 3.7264091968536377, Discriminator loss: 0.24992451071739197\n",
            "Generator loss: 4.26105260848999, Discriminator loss: 0.16577845811843872\n",
            "Generator loss: 4.826891899108887, Discriminator loss: 0.06632459163665771\n",
            "Generator loss: 4.142742156982422, Discriminator loss: 0.19028650224208832\n",
            "Generator loss: 4.074700355529785, Discriminator loss: 0.09427198022603989\n",
            "Generator loss: 4.3747758865356445, Discriminator loss: 0.0855264812707901\n",
            "Generator loss: 4.507721900939941, Discriminator loss: 0.0941314548254013\n",
            "Generator loss: 4.625181198120117, Discriminator loss: 0.045834727585315704\n",
            "Generator loss: 3.9986770153045654, Discriminator loss: 0.24364054203033447\n",
            "Generator loss: 3.5712814331054688, Discriminator loss: 0.18139684200286865\n",
            "Generator loss: 4.195379257202148, Discriminator loss: 0.1429046094417572\n",
            "Generator loss: 4.65986967086792, Discriminator loss: 0.09260822832584381\n",
            "Generator loss: 4.498340606689453, Discriminator loss: 0.12227165699005127\n",
            "Generator loss: 4.378537654876709, Discriminator loss: 0.06279172003269196\n",
            "Generator loss: 4.383257865905762, Discriminator loss: 0.06640619039535522\n",
            "Generator loss: 4.354256629943848, Discriminator loss: 0.09803164750337601\n",
            "Generator loss: 4.3984694480896, Discriminator loss: 0.07494522631168365\n",
            "Generator loss: 4.343362808227539, Discriminator loss: 0.11562480032444\n",
            "Generator loss: 4.308131694793701, Discriminator loss: 0.0842556357383728\n",
            "Generator loss: 4.398292064666748, Discriminator loss: 0.04802139103412628\n",
            "Generator loss: 4.023712158203125, Discriminator loss: 0.22898872196674347\n",
            "Generator loss: 4.079605579376221, Discriminator loss: 0.07638054341077805\n",
            "Generator loss: 4.043505668640137, Discriminator loss: 0.17649121582508087\n",
            "Generator loss: 4.3852009773254395, Discriminator loss: 0.05300296097993851\n",
            "Generator loss: 4.589147567749023, Discriminator loss: 0.07329212874174118\n",
            "Generator loss: 4.250206470489502, Discriminator loss: 0.21119949221611023\n",
            "Generator loss: 4.1186723709106445, Discriminator loss: 0.12597878277301788\n",
            "Generator loss: 4.2772908210754395, Discriminator loss: 0.10410220175981522\n",
            "Generator loss: 4.535889148712158, Discriminator loss: 0.09325933456420898\n",
            "Generator loss: 4.367937088012695, Discriminator loss: 0.1630260944366455\n",
            "Generator loss: 4.5166425704956055, Discriminator loss: 0.07758492976427078\n",
            "Generator loss: 4.738386631011963, Discriminator loss: 0.06873819231987\n",
            "Generator loss: 4.85272741317749, Discriminator loss: 0.06068367883563042\n",
            "Generator loss: 4.781557083129883, Discriminator loss: 0.06472625583410263\n",
            "Generator loss: 4.853848934173584, Discriminator loss: 0.07168453931808472\n",
            "Generator loss: 4.9781012535095215, Discriminator loss: 0.13325811922550201\n",
            "Generator loss: 5.194619655609131, Discriminator loss: 0.22333382070064545\n",
            "Generator loss: 5.378722190856934, Discriminator loss: 0.17983493208885193\n",
            "Generator loss: 5.504249095916748, Discriminator loss: 0.12569648027420044\n",
            "Generator loss: 5.423151969909668, Discriminator loss: 0.2967420220375061\n",
            "Generator loss: 5.580068588256836, Discriminator loss: 0.16166836023330688\n",
            "Generator loss: 5.444910526275635, Discriminator loss: 0.11302436143159866\n",
            "Generator loss: 5.0286030769348145, Discriminator loss: 0.13060633838176727\n",
            "Generator loss: 4.385696887969971, Discriminator loss: 0.26017099618911743\n",
            "Generator loss: 4.91182279586792, Discriminator loss: 0.1739339828491211\n",
            "Generator loss: 4.780590057373047, Discriminator loss: 0.19288426637649536\n",
            "Generator loss: 4.778650283813477, Discriminator loss: 0.0982189029455185\n",
            "Generator loss: 4.460254669189453, Discriminator loss: 0.2146274298429489\n",
            "Generator loss: 4.754500865936279, Discriminator loss: 0.11153900623321533\n",
            "Generator loss: 4.661552429199219, Discriminator loss: 0.11451823264360428\n",
            "Generator loss: 4.571301460266113, Discriminator loss: 0.10019633173942566\n",
            "Generator loss: 4.628616809844971, Discriminator loss: 0.09112054109573364\n",
            "Generator loss: 4.399522304534912, Discriminator loss: 0.1807275414466858\n",
            "Generator loss: 4.293390274047852, Discriminator loss: 0.15450072288513184\n",
            "Generator loss: 4.391943454742432, Discriminator loss: 0.1456337571144104\n",
            "Generator loss: 4.609710693359375, Discriminator loss: 0.09629299491643906\n",
            "Generator loss: 4.410303592681885, Discriminator loss: 0.1758684366941452\n",
            "Generator loss: 4.524416923522949, Discriminator loss: 0.12708741426467896\n",
            "Generator loss: 4.721938133239746, Discriminator loss: 0.0825413167476654\n",
            "Generator loss: 4.6640119552612305, Discriminator loss: 0.10486361384391785\n",
            "Generator loss: 4.633607864379883, Discriminator loss: 0.055347904562950134\n",
            "Generator loss: 4.090595245361328, Discriminator loss: 0.17405802011489868\n",
            "Generator loss: 4.176161766052246, Discriminator loss: 0.08340820670127869\n",
            "Generator loss: 4.533447742462158, Discriminator loss: 0.08071981370449066\n",
            "Generator loss: 4.756379127502441, Discriminator loss: 0.04893358051776886\n",
            "Generator loss: 4.642513751983643, Discriminator loss: 0.06974661350250244\n",
            "Generator loss: 4.353126049041748, Discriminator loss: 0.07966037094593048\n",
            "Generator loss: 4.199942588806152, Discriminator loss: 0.07858654856681824\n",
            "Generator loss: 4.379482746124268, Discriminator loss: 0.06705892086029053\n",
            "Generator loss: 4.233295917510986, Discriminator loss: 0.10561342537403107\n",
            "Generator loss: 4.506146430969238, Discriminator loss: 0.04907068982720375\n",
            "Generator loss: 4.581877708435059, Discriminator loss: 0.06541915237903595\n",
            "Generator loss: 4.4026031494140625, Discriminator loss: 0.11642061918973923\n",
            "Generator loss: 4.274598598480225, Discriminator loss: 0.08114755898714066\n",
            "Generator loss: 4.3908281326293945, Discriminator loss: 0.06250130385160446\n",
            "Generator loss: 4.562086582183838, Discriminator loss: 0.060491207987070084\n",
            "Generator loss: 4.566276550292969, Discriminator loss: 0.07765078544616699\n",
            "Generator loss: 4.201157093048096, Discriminator loss: 0.17700833082199097\n",
            "Generator loss: 4.423776626586914, Discriminator loss: 0.10253726691007614\n",
            "Generator loss: 4.516914367675781, Discriminator loss: 0.14957484602928162\n",
            "Generator loss: 4.562776565551758, Discriminator loss: 0.08956024050712585\n",
            "Generator loss: 4.628856658935547, Discriminator loss: 0.06714314967393875\n",
            "Generator loss: 4.397974967956543, Discriminator loss: 0.11260131001472473\n",
            "Generator loss: 4.026856422424316, Discriminator loss: 0.20774918794631958\n",
            "Generator loss: 4.097139835357666, Discriminator loss: 0.14665356278419495\n",
            "Generator loss: 4.290305137634277, Discriminator loss: 0.1252644807100296\n",
            "Generator loss: 4.453871726989746, Discriminator loss: 0.1007736399769783\n",
            "Generator loss: 4.487735748291016, Discriminator loss: 0.06665434688329697\n",
            "Generator loss: 4.374878883361816, Discriminator loss: 0.08252222090959549\n",
            "Generator loss: 4.295727252960205, Discriminator loss: 0.07458440959453583\n",
            "Generator loss: 4.284608840942383, Discriminator loss: 0.07379736751317978\n",
            "Generator loss: 4.334061622619629, Discriminator loss: 0.06476268917322159\n",
            "Generator loss: 4.330726623535156, Discriminator loss: 0.07164257764816284\n",
            "Generator loss: 4.409371852874756, Discriminator loss: 0.048338353633880615\n",
            "Generator loss: 4.220898151397705, Discriminator loss: 0.11581593006849289\n",
            "Generator loss: 4.232295513153076, Discriminator loss: 0.06473231315612793\n",
            "Generator loss: 4.2792463302612305, Discriminator loss: 0.08966964483261108\n",
            "Generator loss: 4.33910608291626, Discriminator loss: 0.07666085660457611\n",
            "Generator loss: 4.278895378112793, Discriminator loss: 0.10187199711799622\n",
            "Generator loss: 4.205617904663086, Discriminator loss: 0.13176539540290833\n",
            "Generator loss: 4.300935745239258, Discriminator loss: 0.06194629147648811\n",
            "Generator loss: 4.436999320983887, Discriminator loss: 0.04517512395977974\n",
            "Generator loss: 4.393317699432373, Discriminator loss: 0.08253763616085052\n",
            "Generator loss: 4.288470268249512, Discriminator loss: 0.0757201761007309\n",
            "Generator loss: 4.304487228393555, Discriminator loss: 0.05377833917737007\n",
            "Generator loss: 4.383726119995117, Discriminator loss: 0.05606719106435776\n",
            "Generator loss: 4.4012956619262695, Discriminator loss: 0.056926876306533813\n",
            "Generator loss: 4.282309532165527, Discriminator loss: 0.12909194827079773\n",
            "Generator loss: 3.9531521797180176, Discriminator loss: 0.2788112759590149\n",
            "Generator loss: 4.091186046600342, Discriminator loss: 0.09272931516170502\n",
            "Generator loss: 4.322068691253662, Discriminator loss: 0.09286783635616302\n",
            "Generator loss: 4.5132551193237305, Discriminator loss: 0.09973692893981934\n",
            "Generator loss: 4.505956649780273, Discriminator loss: 0.08134506642818451\n",
            "Generator loss: 4.477889537811279, Discriminator loss: 0.0746445506811142\n",
            "Generator loss: 4.327374458312988, Discriminator loss: 0.10974877327680588\n",
            "Generator loss: 4.337061405181885, Discriminator loss: 0.08684559166431427\n",
            "Generator loss: 4.421998023986816, Discriminator loss: 0.11932200193405151\n",
            "Generator loss: 4.377681255340576, Discriminator loss: 0.1637623906135559\n",
            "Generator loss: 4.096686840057373, Discriminator loss: 0.2525936961174011\n",
            "Generator loss: 4.4513258934021, Discriminator loss: 0.05465032160282135\n",
            "Generator loss: 4.740819454193115, Discriminator loss: 0.051498785614967346\n",
            "Generator loss: 4.554813385009766, Discriminator loss: 0.10653966665267944\n",
            "Generator loss: 4.543173789978027, Discriminator loss: 0.044855616986751556\n",
            "Generator loss: 3.930790662765503, Discriminator loss: 0.2610552906990051\n",
            "Generator loss: 4.4432902336120605, Discriminator loss: 0.12802602350711823\n",
            "Generator loss: 4.475153923034668, Discriminator loss: 0.19093799591064453\n",
            "Generator loss: 4.251030921936035, Discriminator loss: 0.19748042523860931\n",
            "Generator loss: 4.362451076507568, Discriminator loss: 0.10198857635259628\n",
            "Generator loss: 4.285121917724609, Discriminator loss: 0.16732455790042877\n",
            "Generator loss: 4.462398529052734, Discriminator loss: 0.14322933554649353\n",
            "Generator loss: 4.538210391998291, Discriminator loss: 0.22229117155075073\n",
            "Generator loss: 4.282752513885498, Discriminator loss: 0.14517752826213837\n",
            "Generator loss: 3.989544630050659, Discriminator loss: 0.2752974033355713\n",
            "Generator loss: 4.357720851898193, Discriminator loss: 0.33100780844688416\n",
            "Generator loss: 4.994668960571289, Discriminator loss: 0.08513030409812927\n",
            "Generator loss: 5.167963027954102, Discriminator loss: 0.06671123951673508\n",
            "Generator loss: 4.93046760559082, Discriminator loss: 0.04349140077829361\n",
            "Generator loss: 4.4223432540893555, Discriminator loss: 0.12596864998340607\n",
            "Generator loss: 4.038242816925049, Discriminator loss: 0.21088802814483643\n",
            "Generator loss: 4.532045841217041, Discriminator loss: 0.15572413802146912\n",
            "Generator loss: 4.906731605529785, Discriminator loss: 0.09751935303211212\n",
            "Generator loss: 4.94071102142334, Discriminator loss: 0.06131601706147194\n",
            "Generator loss: 4.709683418273926, Discriminator loss: 0.09502090513706207\n",
            "Generator loss: 4.534509658813477, Discriminator loss: 0.06240195780992508\n",
            "Generator loss: 4.600450038909912, Discriminator loss: 0.10647250711917877\n",
            "Generator loss: 4.674344539642334, Discriminator loss: 0.07787718623876572\n",
            "Generator loss: 4.73252010345459, Discriminator loss: 0.09752252697944641\n",
            "Generator loss: 4.756053924560547, Discriminator loss: 0.10830030590295792\n",
            "Generator loss: 4.917974948883057, Discriminator loss: 0.12241955101490021\n",
            "Generator loss: 5.091100215911865, Discriminator loss: 0.10564441978931427\n",
            "Generator loss: 4.982841491699219, Discriminator loss: 0.12781992554664612\n",
            "Generator loss: 4.7249579429626465, Discriminator loss: 0.2640606164932251\n",
            "Generator loss: 4.534334182739258, Discriminator loss: 0.45627009868621826\n",
            "Generator loss: 5.0286173820495605, Discriminator loss: 0.1124819666147232\n",
            "Generator loss: 5.2215118408203125, Discriminator loss: 0.06838236004114151\n",
            "Generator loss: 4.901844024658203, Discriminator loss: 0.12983176112174988\n",
            "Generator loss: 4.319186687469482, Discriminator loss: 0.1881045252084732\n",
            "Generator loss: 5.044588565826416, Discriminator loss: 0.14632466435432434\n",
            "Generator loss: 5.07387113571167, Discriminator loss: 0.11154284328222275\n",
            "Generator loss: 4.835290431976318, Discriminator loss: 0.16129177808761597\n",
            "Generator loss: 4.950570106506348, Discriminator loss: 0.10784624516963959\n",
            "Generator loss: 4.6087646484375, Discriminator loss: 0.20353172719478607\n",
            "Generator loss: 4.538923263549805, Discriminator loss: 0.27753716707229614\n",
            "Generator loss: 4.494471549987793, Discriminator loss: 0.23460903763771057\n",
            "Generator loss: 4.8595757484436035, Discriminator loss: 0.14724870026111603\n",
            "Generator loss: 4.635360240936279, Discriminator loss: 0.12418581545352936\n",
            "Generator loss: 4.493772983551025, Discriminator loss: 0.12002168595790863\n",
            "Generator loss: 4.118558406829834, Discriminator loss: 0.25322389602661133\n",
            "Generator loss: 3.95273494720459, Discriminator loss: 0.23136629164218903\n",
            "Generator loss: 4.4736456871032715, Discriminator loss: 0.2518499493598938\n",
            "Generator loss: 4.884356498718262, Discriminator loss: 0.12024368345737457\n",
            "Generator loss: 4.5036139488220215, Discriminator loss: 0.15587298572063446\n",
            "Generator loss: 4.046807765960693, Discriminator loss: 0.15617018938064575\n",
            "Generator loss: 3.6217169761657715, Discriminator loss: 0.396881639957428\n",
            "Generator loss: 4.572303771972656, Discriminator loss: 0.11617816239595413\n",
            "Generator loss: 4.898601531982422, Discriminator loss: 0.09287549555301666\n",
            "Generator loss: 4.695529937744141, Discriminator loss: 0.0980672687292099\n",
            "Generator loss: 4.434876441955566, Discriminator loss: 0.07393130660057068\n",
            "Generator loss: 4.107995510101318, Discriminator loss: 0.13427144289016724\n",
            "Generator loss: 4.056524753570557, Discriminator loss: 0.1613970398902893\n",
            "Generator loss: 4.568848609924316, Discriminator loss: 0.15470682084560394\n",
            "Generator loss: 4.9543232917785645, Discriminator loss: 0.0753730908036232\n",
            "Generator loss: 4.5316081047058105, Discriminator loss: 0.15638300776481628\n",
            "Generator loss: 4.036550521850586, Discriminator loss: 0.2533848285675049\n",
            "Generator loss: 5.122442245483398, Discriminator loss: 0.12463579326868057\n",
            "Generator loss: 5.075347423553467, Discriminator loss: 0.13333234190940857\n",
            "Generator loss: 4.476438522338867, Discriminator loss: 0.1888711154460907\n",
            "Generator loss: 3.9120004177093506, Discriminator loss: 0.2362043708562851\n",
            "Generator loss: 5.814155578613281, Discriminator loss: 0.3055832087993622\n",
            "Generator loss: 5.783895015716553, Discriminator loss: 0.11038899421691895\n",
            "Generator loss: 4.262914180755615, Discriminator loss: 0.24110907316207886\n",
            "Generator loss: 3.9607040882110596, Discriminator loss: 0.21917733550071716\n",
            "Generator loss: 4.281058311462402, Discriminator loss: 0.2725731134414673\n",
            "Generator loss: 4.572925567626953, Discriminator loss: 0.15561354160308838\n",
            "Generator loss: 4.522587299346924, Discriminator loss: 0.123125821352005\n",
            "Generator loss: 3.653407096862793, Discriminator loss: 0.21634893119335175\n",
            "Generator loss: 4.035431861877441, Discriminator loss: 0.17956897616386414\n",
            "Generator loss: 4.1162004470825195, Discriminator loss: 0.14807525277137756\n",
            "Generator loss: 4.553388595581055, Discriminator loss: 0.06573626399040222\n",
            "Generator loss: 4.648629188537598, Discriminator loss: 0.06897357851266861\n",
            "Generator loss: 4.479299068450928, Discriminator loss: 0.06998633593320847\n",
            "Generator loss: 4.138243198394775, Discriminator loss: 0.07678239047527313\n",
            "Generator loss: 4.123159885406494, Discriminator loss: 0.06135477125644684\n",
            "Generator loss: 3.9351024627685547, Discriminator loss: 0.1080588847398758\n",
            "Generator loss: 4.125247001647949, Discriminator loss: 0.08150534331798553\n",
            "Generator loss: 4.268125534057617, Discriminator loss: 0.07863280177116394\n",
            "Generator loss: 4.4791340827941895, Discriminator loss: 0.050548046827316284\n",
            "Generator loss: 4.545734405517578, Discriminator loss: 0.045332685112953186\n",
            "Generator loss: 4.320688247680664, Discriminator loss: 0.07906854152679443\n",
            "Generator loss: 4.157833576202393, Discriminator loss: 0.0993245542049408\n",
            "Generator loss: 3.3963449001312256, Discriminator loss: 0.24726749956607819\n",
            "Generator loss: 4.299704551696777, Discriminator loss: 0.16653785109519958\n",
            "Generator loss: 4.928948879241943, Discriminator loss: 0.05592254921793938\n",
            "Generator loss: 4.451853275299072, Discriminator loss: 0.13617001473903656\n",
            "Generator loss: 4.177119255065918, Discriminator loss: 0.0807005986571312\n",
            "Generator loss: 4.253940105438232, Discriminator loss: 0.06383240222930908\n",
            "Generator loss: 4.2432026863098145, Discriminator loss: 0.09638150036334991\n",
            "Generator loss: 4.312503814697266, Discriminator loss: 0.06742554157972336\n",
            "Generator loss: 4.558206558227539, Discriminator loss: 0.043764784932136536\n",
            "Generator loss: 4.639515399932861, Discriminator loss: 0.03270529583096504\n",
            "Generator loss: 4.686052322387695, Discriminator loss: 0.03081313520669937\n",
            "Generator loss: 4.55765438079834, Discriminator loss: 0.04694303870201111\n",
            "Generator loss: 4.426513195037842, Discriminator loss: 0.0525171235203743\n",
            "Generator loss: 4.402050495147705, Discriminator loss: 0.051658183336257935\n",
            "Generator loss: 4.261809349060059, Discriminator loss: 0.09372150897979736\n",
            "Generator loss: 4.308923244476318, Discriminator loss: 0.03927934914827347\n",
            "Generator loss: 4.318378448486328, Discriminator loss: 0.05780467391014099\n",
            "Generator loss: 4.4274420738220215, Discriminator loss: 0.043283455073833466\n",
            "Generator loss: 4.336968421936035, Discriminator loss: 0.06317608058452606\n",
            "Generator loss: 4.120974063873291, Discriminator loss: 0.11160678416490555\n",
            "Generator loss: 4.119549751281738, Discriminator loss: 0.08646056056022644\n",
            "Generator loss: 4.284740447998047, Discriminator loss: 0.06123488396406174\n",
            "Generator loss: 4.456709861755371, Discriminator loss: 0.06089765951037407\n",
            "Generator loss: 4.48792839050293, Discriminator loss: 0.06401978433132172\n",
            "Generator loss: 4.466765403747559, Discriminator loss: 0.06744161248207092\n",
            "Generator loss: 4.53691291809082, Discriminator loss: 0.06484385579824448\n",
            "Generator loss: 4.681362152099609, Discriminator loss: 0.06022772565484047\n",
            "Generator loss: 4.608449459075928, Discriminator loss: 0.09665356576442719\n",
            "Generator loss: 4.694703102111816, Discriminator loss: 0.04989166930317879\n",
            "Generator loss: 4.733303546905518, Discriminator loss: 0.08317934721708298\n",
            "Generator loss: 4.84954833984375, Discriminator loss: 0.04331088811159134\n",
            "Generator loss: 4.783515453338623, Discriminator loss: 0.07089854776859283\n",
            "Generator loss: 4.521764755249023, Discriminator loss: 0.11318288743495941\n",
            "Generator loss: 4.473566055297852, Discriminator loss: 0.07086260616779327\n",
            "Generator loss: 4.4685444831848145, Discriminator loss: 0.1062067300081253\n",
            "Generator loss: 4.285675048828125, Discriminator loss: 0.19851307570934296\n",
            "Generator loss: 4.631344318389893, Discriminator loss: 0.11002783477306366\n",
            "Generator loss: 5.160919189453125, Discriminator loss: 0.09114887565374374\n",
            "Generator loss: 4.829030990600586, Discriminator loss: 0.26669013500213623\n",
            "Generator loss: 4.588263034820557, Discriminator loss: 0.06883373856544495\n",
            "Generator loss: 4.726309776306152, Discriminator loss: 0.06797845661640167\n",
            "Generator loss: 5.034838676452637, Discriminator loss: 0.058029502630233765\n",
            "Generator loss: 5.020200252532959, Discriminator loss: 0.056522246450185776\n",
            "Generator loss: 4.9463958740234375, Discriminator loss: 0.07756601274013519\n",
            "Generator loss: 4.823894023895264, Discriminator loss: 0.08880534768104553\n",
            "Generator loss: 4.8450140953063965, Discriminator loss: 0.06254757940769196\n",
            "Generator loss: 4.86658239364624, Discriminator loss: 0.08509568870067596\n",
            "Generator loss: 4.515620708465576, Discriminator loss: 0.21059231460094452\n",
            "Generator loss: 4.756075382232666, Discriminator loss: 0.09463420510292053\n",
            "Generator loss: 5.103480815887451, Discriminator loss: 0.07838182151317596\n",
            "Generator loss: 5.22076940536499, Discriminator loss: 0.07915206253528595\n",
            "Generator loss: 5.136147499084473, Discriminator loss: 0.05231547728180885\n",
            "Generator loss: 5.032382011413574, Discriminator loss: 0.07080909609794617\n",
            "Generator loss: 4.527665615081787, Discriminator loss: 0.25092458724975586\n",
            "Generator loss: 4.847678184509277, Discriminator loss: 0.08350865542888641\n",
            "Generator loss: 5.194792747497559, Discriminator loss: 0.041981298476457596\n",
            "Generator loss: 5.264312744140625, Discriminator loss: 0.048953741788864136\n",
            "Generator loss: 5.034763813018799, Discriminator loss: 0.042542263865470886\n",
            "Generator loss: 4.8271484375, Discriminator loss: 0.034230876713991165\n",
            "Generator loss: 4.7941131591796875, Discriminator loss: 0.043427418917417526\n",
            "Generator loss: 4.860630035400391, Discriminator loss: 0.05270213633775711\n",
            "Generator loss: 4.790806293487549, Discriminator loss: 0.11591578274965286\n",
            "Generator loss: 4.975340843200684, Discriminator loss: 0.04960499703884125\n",
            "Generator loss: 5.125608444213867, Discriminator loss: 0.059863604605197906\n",
            "Generator loss: 5.1942138671875, Discriminator loss: 0.07027025520801544\n",
            "Generator loss: 5.340389728546143, Discriminator loss: 0.07072083652019501\n",
            "Generator loss: 5.439001560211182, Discriminator loss: 0.058157265186309814\n",
            "Generator loss: 5.406476974487305, Discriminator loss: 0.050654977560043335\n",
            "Generator loss: 5.368490695953369, Discriminator loss: 0.0521024614572525\n",
            "Generator loss: 5.425690650939941, Discriminator loss: 0.053460173308849335\n",
            "Generator loss: 5.354288578033447, Discriminator loss: 0.03986513987183571\n",
            "Generator loss: 5.3001275062561035, Discriminator loss: 0.03571971505880356\n",
            "Generator loss: 5.215777397155762, Discriminator loss: 0.04144344478845596\n",
            "Generator loss: 5.212889194488525, Discriminator loss: 0.06963463127613068\n",
            "Generator loss: 5.162170886993408, Discriminator loss: 0.044666096568107605\n",
            "Generator loss: 5.106202602386475, Discriminator loss: 0.06273452937602997\n",
            "Generator loss: 5.031744003295898, Discriminator loss: 0.039849258959293365\n",
            "Generator loss: 5.060606956481934, Discriminator loss: 0.04348667711019516\n",
            "Generator loss: 5.060644149780273, Discriminator loss: 0.038487810641527176\n",
            "Generator loss: 5.0529913902282715, Discriminator loss: 0.04735593870282173\n",
            "Generator loss: 5.00726318359375, Discriminator loss: 0.05046885460615158\n",
            "Generator loss: 5.091561794281006, Discriminator loss: 0.05809876322746277\n",
            "Generator loss: 5.100742816925049, Discriminator loss: 0.06081961840391159\n",
            "Generator loss: 5.164943695068359, Discriminator loss: 0.0662493035197258\n",
            "Generator loss: 5.276944637298584, Discriminator loss: 0.05810799449682236\n",
            "Generator loss: 5.262479782104492, Discriminator loss: 0.06563423573970795\n",
            "Generator loss: 5.527767181396484, Discriminator loss: 0.06278052181005478\n",
            "Generator loss: 4.875073432922363, Discriminator loss: 0.10744019597768784\n",
            "Generator loss: 3.737337827682495, Discriminator loss: 0.1801338940858841\n",
            "Generator loss: 5.60236930847168, Discriminator loss: 0.15857194364070892\n",
            "Generator loss: 5.143455505371094, Discriminator loss: 0.14692239463329315\n",
            "Generator loss: 3.8944151401519775, Discriminator loss: 0.14395762979984283\n",
            "Generator loss: 5.8275532722473145, Discriminator loss: 0.11703885346651077\n",
            "Generator loss: 5.965084075927734, Discriminator loss: 0.040627457201480865\n",
            "Generator loss: 4.8507280349731445, Discriminator loss: 0.10232783854007721\n",
            "Generator loss: 4.046139717102051, Discriminator loss: 0.08342340588569641\n",
            "Generator loss: 4.1847710609436035, Discriminator loss: 0.12143747508525848\n",
            "Generator loss: 3.8905625343322754, Discriminator loss: 0.15569449961185455\n",
            "Generator loss: 4.359522819519043, Discriminator loss: 0.12864798307418823\n",
            "Generator loss: 4.520103454589844, Discriminator loss: 0.11333682388067245\n",
            "Generator loss: 3.8950839042663574, Discriminator loss: 0.1862414926290512\n",
            "Generator loss: 4.798287391662598, Discriminator loss: 0.07863344997167587\n",
            "Generator loss: 4.0464887619018555, Discriminator loss: 0.1156650260090828\n",
            "Generator loss: 4.138211250305176, Discriminator loss: 0.10429593175649643\n",
            "Generator loss: 5.007424831390381, Discriminator loss: 0.08139713108539581\n",
            "Generator loss: 3.836529493331909, Discriminator loss: 0.18503440916538239\n",
            "Generator loss: 3.1232547760009766, Discriminator loss: 0.18814688920974731\n",
            "Generator loss: 5.152922630310059, Discriminator loss: 0.1428195983171463\n",
            "Generator loss: 4.717135906219482, Discriminator loss: 0.1638931930065155\n",
            "Generator loss: 4.025113105773926, Discriminator loss: 0.07728306949138641\n",
            "Generator loss: 4.2619218826293945, Discriminator loss: 0.07256617397069931\n",
            "Generator loss: 4.562800884246826, Discriminator loss: 0.08043870329856873\n",
            "Generator loss: 3.297945737838745, Discriminator loss: 0.1803593635559082\n",
            "Generator loss: 4.485610485076904, Discriminator loss: 0.11174356192350388\n",
            "Generator loss: 5.21604061126709, Discriminator loss: 0.033528584986925125\n",
            "Generator loss: 4.807353496551514, Discriminator loss: 0.06600634008646011\n",
            "Generator loss: 3.861236572265625, Discriminator loss: 0.1336561143398285\n",
            "Generator loss: 3.8171803951263428, Discriminator loss: 0.08928185701370239\n",
            "Generator loss: 3.969043731689453, Discriminator loss: 0.12874983251094818\n",
            "Generator loss: 4.343053817749023, Discriminator loss: 0.07377311587333679\n",
            "Generator loss: 4.559255123138428, Discriminator loss: 0.052793994545936584\n",
            "Generator loss: 4.492661952972412, Discriminator loss: 0.05110272392630577\n",
            "Generator loss: 4.247313976287842, Discriminator loss: 0.07449931651353836\n",
            "Generator loss: 3.318667411804199, Discriminator loss: 0.1599225103855133\n",
            "Generator loss: 4.206498146057129, Discriminator loss: 0.09082992374897003\n",
            "Generator loss: 4.823732852935791, Discriminator loss: 0.053715698421001434\n",
            "Generator loss: 4.410620212554932, Discriminator loss: 0.12314921617507935\n",
            "Generator loss: 3.6399528980255127, Discriminator loss: 0.10699553042650223\n",
            "Generator loss: 3.3912601470947266, Discriminator loss: 0.1073734238743782\n",
            "Generator loss: 4.225666046142578, Discriminator loss: 0.10756339132785797\n",
            "Generator loss: 4.320735931396484, Discriminator loss: 0.11589796096086502\n",
            "Generator loss: 4.034144878387451, Discriminator loss: 0.07745326310396194\n",
            "Generator loss: 3.754507303237915, Discriminator loss: 0.0910620465874672\n",
            "Generator loss: 4.255824565887451, Discriminator loss: 0.05568240210413933\n",
            "Generator loss: 4.14024019241333, Discriminator loss: 0.0896221175789833\n",
            "Generator loss: 4.032365798950195, Discriminator loss: 0.07490380108356476\n",
            "Generator loss: 4.1349263191223145, Discriminator loss: 0.06001412123441696\n",
            "Generator loss: 3.6134116649627686, Discriminator loss: 0.1174330785870552\n",
            "Generator loss: 4.015677452087402, Discriminator loss: 0.06292645633220673\n",
            "Generator loss: 4.262353897094727, Discriminator loss: 0.06536852568387985\n",
            "Generator loss: 4.1964569091796875, Discriminator loss: 0.06828716397285461\n",
            "Generator loss: 3.746157169342041, Discriminator loss: 0.1087447851896286\n",
            "Generator loss: 4.257203578948975, Discriminator loss: 0.04770356044173241\n",
            "Generator loss: 4.672024726867676, Discriminator loss: 0.04217640310525894\n",
            "Generator loss: 4.0655059814453125, Discriminator loss: 0.11566232144832611\n",
            "Generator loss: 4.041658878326416, Discriminator loss: 0.05291371047496796\n",
            "Generator loss: 4.476665019989014, Discriminator loss: 0.04238580912351608\n",
            "Generator loss: 4.746425628662109, Discriminator loss: 0.03399420529603958\n",
            "Generator loss: 4.3941144943237305, Discriminator loss: 0.08213572204113007\n",
            "Generator loss: 4.204330921173096, Discriminator loss: 0.06279578059911728\n",
            "Generator loss: 4.011386871337891, Discriminator loss: 0.09022499620914459\n",
            "Generator loss: 3.324981212615967, Discriminator loss: 0.14020977914333344\n",
            "Generator loss: 4.312385082244873, Discriminator loss: 0.0886262059211731\n",
            "Generator loss: 4.878018379211426, Discriminator loss: 0.058984287083148956\n",
            "Generator loss: 4.4252777099609375, Discriminator loss: 0.11180094629526138\n",
            "Generator loss: 3.966862440109253, Discriminator loss: 0.1007009893655777\n",
            "Generator loss: 4.287812232971191, Discriminator loss: 0.0507453978061676\n",
            "Generator loss: 4.766840934753418, Discriminator loss: 0.03774496912956238\n",
            "Generator loss: 5.054985523223877, Discriminator loss: 0.03194096311926842\n",
            "Generator loss: 4.849564075469971, Discriminator loss: 0.05244673788547516\n",
            "Generator loss: 4.586226940155029, Discriminator loss: 0.03412865847349167\n",
            "Generator loss: 4.402730464935303, Discriminator loss: 0.05301135405898094\n",
            "Generator loss: 4.165128707885742, Discriminator loss: 0.0898175910115242\n",
            "Generator loss: 3.8614742755889893, Discriminator loss: 0.10767345130443573\n",
            "Generator loss: 3.9878995418548584, Discriminator loss: 0.1402001976966858\n",
            "Generator loss: 4.697906494140625, Discriminator loss: 0.06359625607728958\n",
            "Generator loss: 4.55617094039917, Discriminator loss: 0.09677021205425262\n",
            "Generator loss: 4.43948221206665, Discriminator loss: 0.06450073421001434\n",
            "Generator loss: 4.150685787200928, Discriminator loss: 0.09348171949386597\n",
            "Generator loss: 4.280975341796875, Discriminator loss: 0.07637884467840195\n",
            "Generator loss: 4.230862617492676, Discriminator loss: 0.10558648407459259\n",
            "Generator loss: 4.706474781036377, Discriminator loss: 0.05750187858939171\n",
            "Generator loss: 4.709314823150635, Discriminator loss: 0.0894480049610138\n",
            "Generator loss: 4.067060947418213, Discriminator loss: 0.1461910605430603\n",
            "Generator loss: 4.856477737426758, Discriminator loss: 0.07404766976833344\n",
            "Generator loss: 4.969859600067139, Discriminator loss: 0.07777289301156998\n",
            "Generator loss: 5.01809024810791, Discriminator loss: 0.04929788038134575\n",
            "Generator loss: 4.914991855621338, Discriminator loss: 0.06445840001106262\n",
            "Generator loss: 5.160094738006592, Discriminator loss: 0.07345271855592728\n",
            "Generator loss: 5.2657623291015625, Discriminator loss: 0.10791312903165817\n",
            "Generator loss: 5.415127277374268, Discriminator loss: 0.1158100813627243\n",
            "Generator loss: 5.447544097900391, Discriminator loss: 0.11159732937812805\n",
            "Generator loss: 5.40641975402832, Discriminator loss: 0.0770903080701828\n",
            "Generator loss: 5.336878776550293, Discriminator loss: 0.07493004202842712\n",
            "Generator loss: 5.338833808898926, Discriminator loss: 0.06320411711931229\n",
            "Generator loss: 5.268360137939453, Discriminator loss: 0.04551456496119499\n",
            "Generator loss: 4.954987525939941, Discriminator loss: 0.08500787615776062\n",
            "Generator loss: 4.8499979972839355, Discriminator loss: 0.11314380168914795\n",
            "Generator loss: 5.166519641876221, Discriminator loss: 0.07139173895120621\n",
            "Generator loss: 5.244252681732178, Discriminator loss: 0.06943634152412415\n",
            "Generator loss: 5.095736026763916, Discriminator loss: 0.04467327892780304\n",
            "Generator loss: 4.978671073913574, Discriminator loss: 0.046659916639328\n",
            "Generator loss: 4.733650207519531, Discriminator loss: 0.16007861495018005\n",
            "Generator loss: 4.800393104553223, Discriminator loss: 0.056557781994342804\n",
            "Generator loss: 4.51831579208374, Discriminator loss: 0.17886710166931152\n",
            "Generator loss: 4.543797969818115, Discriminator loss: 0.08818458020687103\n",
            "Generator loss: 4.7643961906433105, Discriminator loss: 0.05945756286382675\n",
            "Generator loss: 4.839889049530029, Discriminator loss: 0.0597701370716095\n",
            "Generator loss: 4.875892639160156, Discriminator loss: 0.04543854296207428\n",
            "Generator loss: 4.80833625793457, Discriminator loss: 0.059282831847667694\n",
            "Generator loss: 4.69937801361084, Discriminator loss: 0.05784047394990921\n",
            "Generator loss: 4.621271133422852, Discriminator loss: 0.06686746329069138\n",
            "Generator loss: 4.543094635009766, Discriminator loss: 0.06267387419939041\n",
            "Generator loss: 4.531374931335449, Discriminator loss: 0.10769112408161163\n",
            "Generator loss: 4.553043365478516, Discriminator loss: 0.0907481461763382\n",
            "Generator loss: 4.672410011291504, Discriminator loss: 0.0724746435880661\n",
            "Generator loss: 4.607719421386719, Discriminator loss: 0.14188939332962036\n",
            "Generator loss: 4.839435577392578, Discriminator loss: 0.06032438203692436\n",
            "Generator loss: 4.844507217407227, Discriminator loss: 0.06237036734819412\n",
            "Generator loss: 4.931891918182373, Discriminator loss: 0.07893224060535431\n",
            "Generator loss: 5.12682580947876, Discriminator loss: 0.05290796607732773\n",
            "Generator loss: 5.196351051330566, Discriminator loss: 0.048824243247509\n",
            "Generator loss: 4.516003608703613, Discriminator loss: 0.19110338389873505\n",
            "Generator loss: 4.731314659118652, Discriminator loss: 0.1213354542851448\n",
            "Generator loss: 4.344101905822754, Discriminator loss: 0.21046306192874908\n",
            "Generator loss: 4.786870956420898, Discriminator loss: 0.0805962011218071\n",
            "Generator loss: 5.172183036804199, Discriminator loss: 0.04191287234425545\n",
            "Generator loss: 5.250163555145264, Discriminator loss: 0.028199901804327965\n",
            "Generator loss: 4.786172866821289, Discriminator loss: 0.06290280818939209\n",
            "Generator loss: 4.7453718185424805, Discriminator loss: 0.056197553873062134\n",
            "Generator loss: 5.028550624847412, Discriminator loss: 0.066459059715271\n",
            "Generator loss: 4.702002048492432, Discriminator loss: 0.17134883999824524\n",
            "Generator loss: 4.387073516845703, Discriminator loss: 0.20707346498966217\n",
            "Generator loss: 5.144355773925781, Discriminator loss: 0.12023025006055832\n",
            "Generator loss: 5.070662021636963, Discriminator loss: 0.2423621416091919\n",
            "Generator loss: 5.171558856964111, Discriminator loss: 0.09190385043621063\n",
            "Generator loss: 5.309447765350342, Discriminator loss: 0.18887090682983398\n",
            "Generator loss: 5.471864700317383, Discriminator loss: 0.1617436707019806\n",
            "Generator loss: 6.0635480880737305, Discriminator loss: 0.16332906484603882\n",
            "Generator loss: 5.7941083908081055, Discriminator loss: 0.18294267356395721\n",
            "Generator loss: 5.8645429611206055, Discriminator loss: 0.1590118408203125\n",
            "Generator loss: 5.828217506408691, Discriminator loss: 0.2959321439266205\n",
            "Generator loss: 6.000096797943115, Discriminator loss: 0.15214811265468597\n",
            "Generator loss: 5.509212970733643, Discriminator loss: 0.23151487112045288\n",
            "Generator loss: 5.399244785308838, Discriminator loss: 0.1574174463748932\n",
            "Generator loss: 5.750981330871582, Discriminator loss: 0.09932803362607956\n",
            "Generator loss: 5.429077625274658, Discriminator loss: 0.11956239491701126\n",
            "Generator loss: 4.993034362792969, Discriminator loss: 0.10751353204250336\n",
            "Generator loss: 5.130870342254639, Discriminator loss: 0.08321870863437653\n",
            "Generator loss: 4.997712135314941, Discriminator loss: 0.12064318358898163\n",
            "Generator loss: 5.036218643188477, Discriminator loss: 0.10570760816335678\n",
            "Generator loss: 4.778372287750244, Discriminator loss: 0.07567678391933441\n",
            "Generator loss: 4.502094268798828, Discriminator loss: 0.1096453070640564\n",
            "Generator loss: 4.661800384521484, Discriminator loss: 0.08443319797515869\n",
            "Generator loss: 4.658759593963623, Discriminator loss: 0.08501176536083221\n",
            "Generator loss: 4.762477874755859, Discriminator loss: 0.054604172706604004\n",
            "Generator loss: 4.841906547546387, Discriminator loss: 0.0735338032245636\n",
            "Generator loss: 4.964375972747803, Discriminator loss: 0.04034137725830078\n",
            "Generator loss: 4.961589813232422, Discriminator loss: 0.042160168290138245\n",
            "Generator loss: 4.782751083374023, Discriminator loss: 0.06770384311676025\n",
            "Generator loss: 4.418682098388672, Discriminator loss: 0.13085293769836426\n",
            "Generator loss: 4.317585468292236, Discriminator loss: 0.16281351447105408\n",
            "Generator loss: 4.804652690887451, Discriminator loss: 0.06598354876041412\n",
            "Generator loss: 4.973480701446533, Discriminator loss: 0.08164995163679123\n",
            "Generator loss: 4.77733039855957, Discriminator loss: 0.10602320730686188\n",
            "Generator loss: 4.607097148895264, Discriminator loss: 0.1044858917593956\n",
            "Generator loss: 4.4640045166015625, Discriminator loss: 0.08778759837150574\n",
            "Generator loss: 4.700455188751221, Discriminator loss: 0.08406272530555725\n",
            "Generator loss: 4.820629119873047, Discriminator loss: 0.07099948078393936\n",
            "Generator loss: 4.972960472106934, Discriminator loss: 0.060062676668167114\n",
            "Generator loss: 4.882828712463379, Discriminator loss: 0.059908147901296616\n",
            "Generator loss: 4.761874198913574, Discriminator loss: 0.07356371730566025\n",
            "Generator loss: 4.751571178436279, Discriminator loss: 0.0593344010412693\n",
            "Generator loss: 4.695225715637207, Discriminator loss: 0.09410904347896576\n",
            "Generator loss: 4.854653358459473, Discriminator loss: 0.07000139355659485\n",
            "Generator loss: 4.951786041259766, Discriminator loss: 0.0658237412571907\n",
            "Generator loss: 4.621001720428467, Discriminator loss: 0.2736302316188812\n",
            "Generator loss: 5.011651039123535, Discriminator loss: 0.07440997660160065\n",
            "Generator loss: 5.404048442840576, Discriminator loss: 0.058888085186481476\n",
            "Generator loss: 5.507436275482178, Discriminator loss: 0.05179731920361519\n",
            "Generator loss: 5.391088008880615, Discriminator loss: 0.08040817081928253\n",
            "Generator loss: 5.46196174621582, Discriminator loss: 0.133279487490654\n",
            "Generator loss: 5.388538360595703, Discriminator loss: 0.23672135174274445\n",
            "Generator loss: 5.298509120941162, Discriminator loss: 0.3414555788040161\n",
            "Generator loss: 5.543339729309082, Discriminator loss: 0.0975433737039566\n",
            "Generator loss: 5.611412525177002, Discriminator loss: 0.05789487436413765\n",
            "Generator loss: 5.443150520324707, Discriminator loss: 0.04420720785856247\n",
            "Generator loss: 5.207693099975586, Discriminator loss: 0.07541815936565399\n",
            "Generator loss: 4.959166049957275, Discriminator loss: 0.23080119490623474\n",
            "Generator loss: 5.0891804695129395, Discriminator loss: 0.060723740607500076\n",
            "Generator loss: 5.306553363800049, Discriminator loss: 0.0480831079185009\n",
            "Generator loss: 5.190379619598389, Discriminator loss: 0.09422628581523895\n",
            "Generator loss: 5.148983478546143, Discriminator loss: 0.04715636000037193\n",
            "Generator loss: 5.270152568817139, Discriminator loss: 0.05112916976213455\n",
            "Generator loss: 5.321600914001465, Discriminator loss: 0.046188630163669586\n",
            "Generator loss: 5.276933670043945, Discriminator loss: 0.05856795236468315\n",
            "Generator loss: 5.28204345703125, Discriminator loss: 0.07188305258750916\n",
            "Generator loss: 5.352267265319824, Discriminator loss: 0.06203502044081688\n",
            "Generator loss: 5.316469192504883, Discriminator loss: 0.25086331367492676\n",
            "Generator loss: 5.444595813751221, Discriminator loss: 0.059894971549510956\n",
            "Generator loss: 5.5114006996154785, Discriminator loss: 0.04787783324718475\n",
            "Generator loss: 5.403130054473877, Discriminator loss: 0.05054924637079239\n",
            "Generator loss: 5.321266174316406, Discriminator loss: 0.06602020561695099\n",
            "Generator loss: 5.350396156311035, Discriminator loss: 0.058300454169511795\n",
            "Generator loss: 5.357651233673096, Discriminator loss: 0.05878986418247223\n",
            "Generator loss: 5.434257984161377, Discriminator loss: 0.04875922203063965\n",
            "Generator loss: 5.306520462036133, Discriminator loss: 0.05938372015953064\n",
            "Generator loss: 5.110610485076904, Discriminator loss: 0.21902909874916077\n",
            "Generator loss: 5.346860885620117, Discriminator loss: 0.06051091477274895\n",
            "Generator loss: 5.431184768676758, Discriminator loss: 0.18155796825885773\n",
            "Generator loss: 5.408246040344238, Discriminator loss: 0.054599788039922714\n",
            "Generator loss: 5.431018352508545, Discriminator loss: 0.16108986735343933\n",
            "Generator loss: 6.082490921020508, Discriminator loss: 0.10000421106815338\n",
            "Generator loss: 6.411135673522949, Discriminator loss: 0.07949396967887878\n",
            "Generator loss: 6.160114765167236, Discriminator loss: 0.08606959134340286\n",
            "Generator loss: 5.99144983291626, Discriminator loss: 0.05603018030524254\n",
            "Generator loss: 6.181521415710449, Discriminator loss: 0.10668673366308212\n",
            "Generator loss: 6.068600654602051, Discriminator loss: 0.05307193100452423\n",
            "Generator loss: 5.851862907409668, Discriminator loss: 0.057204753160476685\n",
            "Generator loss: 5.856805801391602, Discriminator loss: 0.06450901180505753\n",
            "Generator loss: 5.825205326080322, Discriminator loss: 0.06437313556671143\n",
            "Generator loss: 5.922266006469727, Discriminator loss: 0.09510567784309387\n",
            "Generator loss: 5.943190574645996, Discriminator loss: 0.04863215982913971\n",
            "Generator loss: 5.6842756271362305, Discriminator loss: 0.043019965291023254\n",
            "Generator loss: 5.312088966369629, Discriminator loss: 0.07624489068984985\n",
            "Generator loss: 5.536303520202637, Discriminator loss: 0.07719825208187103\n",
            "Generator loss: 5.793346405029297, Discriminator loss: 0.06977446377277374\n",
            "Generator loss: 5.777050495147705, Discriminator loss: 0.049586862325668335\n",
            "Generator loss: 5.554261684417725, Discriminator loss: 0.07009708881378174\n",
            "Generator loss: 5.443804740905762, Discriminator loss: 0.06393454223871231\n",
            "Generator loss: 5.533298492431641, Discriminator loss: 0.05760890245437622\n",
            "Generator loss: 5.447385787963867, Discriminator loss: 0.0683092325925827\n",
            "Generator loss: 5.295846462249756, Discriminator loss: 0.07489937543869019\n",
            "Generator loss: 5.397729873657227, Discriminator loss: 0.0632687583565712\n",
            "Generator loss: 5.542784214019775, Discriminator loss: 0.04031112790107727\n",
            "Generator loss: 5.347502708435059, Discriminator loss: 0.0773046463727951\n",
            "Generator loss: 5.271914482116699, Discriminator loss: 0.06263406574726105\n",
            "Generator loss: 5.492748737335205, Discriminator loss: 0.05385613068938255\n",
            "Generator loss: 5.373392581939697, Discriminator loss: 0.11249646544456482\n",
            "Generator loss: 5.258601188659668, Discriminator loss: 0.09287296235561371\n",
            "Generator loss: 5.402799129486084, Discriminator loss: 0.09157133102416992\n",
            "Generator loss: 5.558657646179199, Discriminator loss: 0.07631941139698029\n",
            "Generator loss: 5.503575325012207, Discriminator loss: 0.07067632675170898\n",
            "Generator loss: 5.4435601234436035, Discriminator loss: 0.06708046048879623\n",
            "Generator loss: 5.578789234161377, Discriminator loss: 0.08943122625350952\n",
            "Generator loss: 5.765639781951904, Discriminator loss: 0.09615296870470047\n",
            "Generator loss: 5.9971771240234375, Discriminator loss: 0.07866254448890686\n",
            "Generator loss: 6.339428901672363, Discriminator loss: 0.1287287473678589\n",
            "Generator loss: 6.594823837280273, Discriminator loss: 0.15397267043590546\n",
            "Generator loss: 6.282774925231934, Discriminator loss: 0.12239359319210052\n",
            "Generator loss: 6.132429599761963, Discriminator loss: 0.10170602798461914\n",
            "Generator loss: 6.204166412353516, Discriminator loss: 0.0869145467877388\n",
            "Generator loss: 6.086026191711426, Discriminator loss: 0.0565800815820694\n",
            "Generator loss: 5.914328575134277, Discriminator loss: 0.05096904933452606\n",
            "Generator loss: 5.837179183959961, Discriminator loss: 0.07579708099365234\n",
            "Generator loss: 5.764769077301025, Discriminator loss: 0.051556769758462906\n",
            "Generator loss: 5.574799537658691, Discriminator loss: 0.058943912386894226\n",
            "Generator loss: 5.469909191131592, Discriminator loss: 0.05614238977432251\n",
            "Generator loss: 5.220768928527832, Discriminator loss: 0.08267112076282501\n",
            "Generator loss: 5.332509994506836, Discriminator loss: 0.05708400160074234\n",
            "Generator loss: 5.381791114807129, Discriminator loss: 0.05905890464782715\n",
            "Generator loss: 5.373468399047852, Discriminator loss: 0.038929276168346405\n",
            "Generator loss: 5.347078800201416, Discriminator loss: 0.044731512665748596\n",
            "Generator loss: 5.3021626472473145, Discriminator loss: 0.07740601152181625\n",
            "Generator loss: 5.151535987854004, Discriminator loss: 0.08214659243822098\n",
            "Generator loss: 5.233108997344971, Discriminator loss: 0.061468832194805145\n",
            "Generator loss: 5.347841739654541, Discriminator loss: 0.05295577272772789\n",
            "Generator loss: 5.2822675704956055, Discriminator loss: 0.06406749039888382\n",
            "Generator loss: 5.20733118057251, Discriminator loss: 0.06943704932928085\n",
            "Generator loss: 4.896490573883057, Discriminator loss: 0.16844125092029572\n",
            "Generator loss: 5.365900039672852, Discriminator loss: 0.09507283568382263\n",
            "Generator loss: 5.305454730987549, Discriminator loss: 0.07218744605779648\n",
            "Generator loss: 5.149058818817139, Discriminator loss: 0.038797441869974136\n",
            "Generator loss: 4.818475723266602, Discriminator loss: 0.07272069156169891\n",
            "Generator loss: 4.796108245849609, Discriminator loss: 0.0662492960691452\n",
            "Generator loss: 4.812609672546387, Discriminator loss: 0.0785415768623352\n",
            "Generator loss: 4.71449613571167, Discriminator loss: 0.08661409467458725\n",
            "Generator loss: 4.649938106536865, Discriminator loss: 0.08869507908821106\n",
            "Generator loss: 4.623089790344238, Discriminator loss: 0.0844620019197464\n",
            "Generator loss: 4.808135986328125, Discriminator loss: 0.03506636619567871\n",
            "Generator loss: 4.995992660522461, Discriminator loss: 0.027976173907518387\n",
            "Generator loss: 5.035645961761475, Discriminator loss: 0.03217777609825134\n",
            "Generator loss: 4.997978687286377, Discriminator loss: 0.025297224521636963\n",
            "Generator loss: 4.9144182205200195, Discriminator loss: 0.042331159114837646\n",
            "Generator loss: 4.891446590423584, Discriminator loss: 0.0489652045071125\n",
            "Generator loss: 5.007017612457275, Discriminator loss: 0.038394685834646225\n",
            "Generator loss: 5.189653396606445, Discriminator loss: 0.03899391368031502\n",
            "Generator loss: 5.226599216461182, Discriminator loss: 0.058157168328762054\n",
            "Generator loss: 5.292807579040527, Discriminator loss: 0.07705633342266083\n",
            "Generator loss: 5.430964469909668, Discriminator loss: 0.033103734254837036\n",
            "Generator loss: 5.211620330810547, Discriminator loss: 0.07617116719484329\n",
            "Generator loss: 5.0978803634643555, Discriminator loss: 0.05765010043978691\n",
            "Generator loss: 5.004187107086182, Discriminator loss: 0.10225961357355118\n",
            "Generator loss: 5.436129570007324, Discriminator loss: 0.0444854311645031\n",
            "Generator loss: 5.61250114440918, Discriminator loss: 0.03356122970581055\n",
            "Generator loss: 5.506213188171387, Discriminator loss: 0.0311712808907032\n",
            "Generator loss: 5.337516784667969, Discriminator loss: 0.08303606510162354\n",
            "Generator loss: 5.690553665161133, Discriminator loss: 0.06798551231622696\n",
            "Generator loss: 6.369044303894043, Discriminator loss: 0.06883355975151062\n",
            "Generator loss: 6.629629135131836, Discriminator loss: 0.05372815951704979\n",
            "Generator loss: 6.510169982910156, Discriminator loss: 0.04286111518740654\n",
            "Generator loss: 6.279135704040527, Discriminator loss: 0.055564507842063904\n",
            "Generator loss: 6.065865516662598, Discriminator loss: 0.07331932336091995\n",
            "Generator loss: 5.838580131530762, Discriminator loss: 0.031420331448316574\n",
            "Generator loss: 5.588179588317871, Discriminator loss: 0.047924645245075226\n",
            "Generator loss: 5.541269302368164, Discriminator loss: 0.03915522247552872\n",
            "Generator loss: 5.695228576660156, Discriminator loss: 0.0532660074532032\n",
            "Generator loss: 5.790143966674805, Discriminator loss: 0.04101406782865524\n",
            "Generator loss: 5.76857852935791, Discriminator loss: 0.030048158019781113\n",
            "Generator loss: 5.530501365661621, Discriminator loss: 0.07931973040103912\n",
            "Generator loss: 5.414557456970215, Discriminator loss: 0.03487744927406311\n",
            "Generator loss: 5.534001350402832, Discriminator loss: 0.04031316563487053\n",
            "Generator loss: 5.771643161773682, Discriminator loss: 0.044382888823747635\n",
            "Generator loss: 5.910195350646973, Discriminator loss: 0.031244870275259018\n",
            "Generator loss: 5.868446350097656, Discriminator loss: 0.03195558115839958\n",
            "Generator loss: 5.691633224487305, Discriminator loss: 0.04860271140933037\n",
            "Generator loss: 5.641538619995117, Discriminator loss: 0.0366644412279129\n",
            "Generator loss: 5.707329750061035, Discriminator loss: 0.033642761409282684\n",
            "Generator loss: 5.695929050445557, Discriminator loss: 0.0354340523481369\n",
            "Generator loss: 5.649031639099121, Discriminator loss: 0.03225445747375488\n",
            "Generator loss: 5.420591831207275, Discriminator loss: 0.05198288708925247\n",
            "Generator loss: 5.334133148193359, Discriminator loss: 0.029982857406139374\n",
            "Generator loss: 5.299788951873779, Discriminator loss: 0.04325982183218002\n",
            "Generator loss: 5.326411724090576, Discriminator loss: 0.04066457971930504\n",
            "Generator loss: 5.462632179260254, Discriminator loss: 0.030140332877635956\n",
            "Generator loss: 5.394790172576904, Discriminator loss: 0.04829529672861099\n",
            "Generator loss: 5.270565986633301, Discriminator loss: 0.092238649725914\n",
            "Generator loss: 5.467134952545166, Discriminator loss: 0.0328272320330143\n",
            "Generator loss: 5.586618900299072, Discriminator loss: 0.026663977652788162\n",
            "Generator loss: 5.558480262756348, Discriminator loss: 0.02479364722967148\n",
            "Generator loss: 5.498987197875977, Discriminator loss: 0.024385148659348488\n",
            "Generator loss: 5.461196422576904, Discriminator loss: 0.02142941951751709\n",
            "Generator loss: 5.500061988830566, Discriminator loss: 0.026179995387792587\n",
            "Generator loss: 5.4886651039123535, Discriminator loss: 0.0598616823554039\n",
            "Generator loss: 5.5460052490234375, Discriminator loss: 0.03861479461193085\n",
            "Generator loss: 5.592160224914551, Discriminator loss: 0.0351618193089962\n",
            "Generator loss: 5.669889450073242, Discriminator loss: 0.026867331936955452\n",
            "Generator loss: 5.565776348114014, Discriminator loss: 0.055264912545681\n",
            "Generator loss: 5.708436965942383, Discriminator loss: 0.060269154608249664\n",
            "Generator loss: 5.883410453796387, Discriminator loss: 0.04690787196159363\n",
            "Generator loss: 5.9720611572265625, Discriminator loss: 0.043561700731515884\n",
            "Generator loss: 5.992414951324463, Discriminator loss: 0.04971332848072052\n",
            "Generator loss: 5.993629455566406, Discriminator loss: 0.06210668012499809\n",
            "Generator loss: 5.986917495727539, Discriminator loss: 0.051788851618766785\n",
            "Generator loss: 5.874246120452881, Discriminator loss: 0.028661545366048813\n",
            "Generator loss: 5.584564208984375, Discriminator loss: 0.03514184430241585\n",
            "Generator loss: 5.454970359802246, Discriminator loss: 0.03115423396229744\n",
            "Generator loss: 5.5331339836120605, Discriminator loss: 0.03062833659350872\n",
            "Generator loss: 5.6213836669921875, Discriminator loss: 0.020992834120988846\n",
            "Generator loss: 5.566506385803223, Discriminator loss: 0.038793742656707764\n",
            "Generator loss: 5.411298751831055, Discriminator loss: 0.03360003978013992\n",
            "Generator loss: 5.48277473449707, Discriminator loss: 0.025083700194954872\n",
            "Generator loss: 5.588710308074951, Discriminator loss: 0.026263680309057236\n",
            "Generator loss: 5.502728462219238, Discriminator loss: 0.07690997421741486\n",
            "Generator loss: 5.630483627319336, Discriminator loss: 0.028090298175811768\n",
            "Generator loss: 5.6478071212768555, Discriminator loss: 0.021939272060990334\n",
            "Generator loss: 5.476048946380615, Discriminator loss: 0.0365084633231163\n",
            "Generator loss: 5.427212715148926, Discriminator loss: 0.02423524111509323\n",
            "Generator loss: 5.495075702667236, Discriminator loss: 0.025452792644500732\n",
            "Generator loss: 5.577311992645264, Discriminator loss: 0.03457212820649147\n",
            "Generator loss: 5.726269721984863, Discriminator loss: 0.03504709526896477\n",
            "Generator loss: 5.875796318054199, Discriminator loss: 0.04699675738811493\n",
            "Generator loss: 5.904267311096191, Discriminator loss: 0.03286764770746231\n",
            "Generator loss: 5.719117164611816, Discriminator loss: 0.030257448554039\n",
            "Generator loss: 5.499759674072266, Discriminator loss: 0.02930692955851555\n",
            "Generator loss: 5.466386795043945, Discriminator loss: 0.043867677450180054\n",
            "Generator loss: 5.586521148681641, Discriminator loss: 0.039825163781642914\n",
            "Generator loss: 5.673788547515869, Discriminator loss: 0.040810756385326385\n",
            "Generator loss: 5.662017822265625, Discriminator loss: 0.028046276420354843\n",
            "Generator loss: 5.566112041473389, Discriminator loss: 0.0322781540453434\n",
            "Generator loss: 5.526577949523926, Discriminator loss: 0.03516905754804611\n",
            "Generator loss: 5.57119083404541, Discriminator loss: 0.04198799654841423\n",
            "Generator loss: 5.578024387359619, Discriminator loss: 0.03771613910794258\n",
            "Generator loss: 5.678215503692627, Discriminator loss: 0.0270291268825531\n",
            "Generator loss: 5.67977237701416, Discriminator loss: 0.02630070596933365\n",
            "Generator loss: 5.694537162780762, Discriminator loss: 0.020683439448475838\n",
            "Generator loss: 5.5929365158081055, Discriminator loss: 0.020871296525001526\n",
            "Generator loss: 5.555576324462891, Discriminator loss: 0.023673709481954575\n",
            "Generator loss: 5.572879791259766, Discriminator loss: 0.021242868155241013\n",
            "Generator loss: 5.571045875549316, Discriminator loss: 0.022783752530813217\n",
            "Generator loss: 5.5423126220703125, Discriminator loss: 0.03156372904777527\n",
            "Generator loss: 5.510047435760498, Discriminator loss: 0.022797852754592896\n",
            "Generator loss: 5.610694408416748, Discriminator loss: 0.027510063722729683\n",
            "Generator loss: 5.750810146331787, Discriminator loss: 0.03563765063881874\n",
            "Generator loss: 5.80828857421875, Discriminator loss: 0.031091593205928802\n",
            "Generator loss: 5.540942668914795, Discriminator loss: 0.06841415166854858\n",
            "Generator loss: 5.625436782836914, Discriminator loss: 0.037247925996780396\n",
            "Generator loss: 5.706179618835449, Discriminator loss: 0.033306390047073364\n",
            "Generator loss: 5.716163635253906, Discriminator loss: 0.02985081449151039\n",
            "Generator loss: 5.686474323272705, Discriminator loss: 0.03168893977999687\n",
            "Generator loss: 5.539775848388672, Discriminator loss: 0.04237407445907593\n",
            "Generator loss: 5.462599754333496, Discriminator loss: 0.044863637536764145\n",
            "Generator loss: 5.552978038787842, Discriminator loss: 0.0431491956114769\n",
            "Generator loss: 5.6654276847839355, Discriminator loss: 0.030050426721572876\n",
            "Generator loss: 5.619726657867432, Discriminator loss: 0.02910788170993328\n",
            "Generator loss: 5.450901031494141, Discriminator loss: 0.02775183692574501\n",
            "Generator loss: 5.220028400421143, Discriminator loss: 0.041909657418727875\n",
            "Generator loss: 4.9207353591918945, Discriminator loss: 0.07314678281545639\n",
            "Generator loss: 5.228315353393555, Discriminator loss: 0.048499368131160736\n",
            "Generator loss: 5.519160270690918, Discriminator loss: 0.03231999650597572\n",
            "Generator loss: 5.561318874359131, Discriminator loss: 0.02875274047255516\n",
            "Generator loss: 5.167555332183838, Discriminator loss: 0.07259993255138397\n",
            "Generator loss: 5.1226806640625, Discriminator loss: 0.038828715682029724\n",
            "Generator loss: 5.364545822143555, Discriminator loss: 0.042144160717725754\n",
            "Generator loss: 5.520835876464844, Discriminator loss: 0.026542216539382935\n",
            "Generator loss: 5.517513275146484, Discriminator loss: 0.021721065044403076\n",
            "Generator loss: 5.451981544494629, Discriminator loss: 0.023462742567062378\n",
            "Generator loss: 5.185568332672119, Discriminator loss: 0.037417203187942505\n",
            "Generator loss: 5.150458335876465, Discriminator loss: 0.028358131647109985\n",
            "Generator loss: 5.230874061584473, Discriminator loss: 0.03022526204586029\n",
            "Generator loss: 5.3022308349609375, Discriminator loss: 0.024032115936279297\n",
            "Generator loss: 5.1080403327941895, Discriminator loss: 0.08665748685598373\n",
            "Generator loss: 4.905751705169678, Discriminator loss: 0.08636164665222168\n",
            "Generator loss: 5.203946590423584, Discriminator loss: 0.03910841420292854\n",
            "Generator loss: 5.50121545791626, Discriminator loss: 0.027497559785842896\n",
            "Generator loss: 5.468476295471191, Discriminator loss: 0.024978455156087875\n",
            "Generator loss: 5.320870876312256, Discriminator loss: 0.02458794042468071\n",
            "Generator loss: 5.173365116119385, Discriminator loss: 0.034854479134082794\n",
            "Generator loss: 5.150522232055664, Discriminator loss: 0.0665518268942833\n",
            "Generator loss: 5.279646873474121, Discriminator loss: 0.04673086106777191\n",
            "Generator loss: 5.331291198730469, Discriminator loss: 0.04750511050224304\n",
            "Generator loss: 5.373645782470703, Discriminator loss: 0.03396269306540489\n",
            "Generator loss: 5.47528076171875, Discriminator loss: 0.0396401509642601\n",
            "Generator loss: 5.559375762939453, Discriminator loss: 0.03955230861902237\n",
            "Generator loss: 5.566356658935547, Discriminator loss: 0.05394617095589638\n",
            "Generator loss: 5.643426895141602, Discriminator loss: 0.04588032513856888\n",
            "Generator loss: 5.598710060119629, Discriminator loss: 0.07053050398826599\n",
            "Generator loss: 5.637139797210693, Discriminator loss: 0.04368976503610611\n",
            "Generator loss: 5.7823872566223145, Discriminator loss: 0.05199044197797775\n",
            "Generator loss: 5.907189846038818, Discriminator loss: 0.054833125323057175\n",
            "Generator loss: 6.018075942993164, Discriminator loss: 0.04305798187851906\n",
            "Generator loss: 5.861491680145264, Discriminator loss: 0.06815861165523529\n",
            "Generator loss: 5.822535037994385, Discriminator loss: 0.04751426726579666\n",
            "Generator loss: 5.838479995727539, Discriminator loss: 0.04381778836250305\n",
            "Generator loss: 5.818657398223877, Discriminator loss: 0.026070160791277885\n",
            "Generator loss: 5.677587985992432, Discriminator loss: 0.036935146898031235\n",
            "Generator loss: 5.560707092285156, Discriminator loss: 0.05071404576301575\n",
            "Generator loss: 5.543023586273193, Discriminator loss: 0.057605087757110596\n",
            "Generator loss: 5.7153849601745605, Discriminator loss: 0.03331849351525307\n",
            "Generator loss: 5.770158767700195, Discriminator loss: 0.026328986510634422\n",
            "Generator loss: 4.926090240478516, Discriminator loss: 0.11163792014122009\n",
            "Generator loss: 5.384831428527832, Discriminator loss: 0.07419710606336594\n",
            "Generator loss: 5.867337226867676, Discriminator loss: 0.037025086581707\n",
            "Generator loss: 5.870704650878906, Discriminator loss: 0.022632036358118057\n",
            "Generator loss: 5.691910266876221, Discriminator loss: 0.020949354395270348\n",
            "Generator loss: 4.984020233154297, Discriminator loss: 0.07096154987812042\n",
            "Generator loss: 5.091085910797119, Discriminator loss: 0.03382466360926628\n",
            "Generator loss: 5.2659101486206055, Discriminator loss: 0.06778012216091156\n",
            "Generator loss: 5.578727722167969, Discriminator loss: 0.027932075783610344\n",
            "Generator loss: 5.663149356842041, Discriminator loss: 0.023053057491779327\n",
            "Generator loss: 5.350605010986328, Discriminator loss: 0.053382422775030136\n",
            "Generator loss: 5.327821254730225, Discriminator loss: 0.03277868032455444\n",
            "Generator loss: 5.485804080963135, Discriminator loss: 0.03171839192509651\n",
            "Generator loss: 5.635148525238037, Discriminator loss: 0.03281141072511673\n",
            "Generator loss: 5.6359052658081055, Discriminator loss: 0.030784377828240395\n",
            "Generator loss: 5.613014221191406, Discriminator loss: 0.037108585238456726\n",
            "Generator loss: 5.585123538970947, Discriminator loss: 0.04465833306312561\n",
            "Generator loss: 5.697433948516846, Discriminator loss: 0.042903441935777664\n",
            "Generator loss: 5.862974643707275, Discriminator loss: 0.060024481266736984\n",
            "Generator loss: 5.917791366577148, Discriminator loss: 0.04434765875339508\n",
            "Generator loss: 5.815540313720703, Discriminator loss: 0.05089805647730827\n",
            "Generator loss: 5.906247615814209, Discriminator loss: 0.04924396425485611\n",
            "Generator loss: 6.233636379241943, Discriminator loss: 0.05319036543369293\n",
            "Generator loss: 6.186425685882568, Discriminator loss: 0.08992068469524384\n",
            "Generator loss: 6.047392845153809, Discriminator loss: 0.0410628467798233\n",
            "Generator loss: 5.873269081115723, Discriminator loss: 0.031077485531568527\n",
            "Generator loss: 5.726095199584961, Discriminator loss: 0.03357188403606415\n",
            "Generator loss: 5.683233737945557, Discriminator loss: 0.07250822335481644\n",
            "Generator loss: 5.760224342346191, Discriminator loss: 0.027338698506355286\n",
            "Generator loss: 5.742183685302734, Discriminator loss: 0.0283405352383852\n",
            "Generator loss: 5.698802947998047, Discriminator loss: 0.022409457713365555\n",
            "Generator loss: 5.572307586669922, Discriminator loss: 0.02678189054131508\n",
            "Generator loss: 5.529996871948242, Discriminator loss: 0.024367090314626694\n",
            "Generator loss: 5.389293670654297, Discriminator loss: 0.032833270728588104\n",
            "Generator loss: 5.338318347930908, Discriminator loss: 0.03326309844851494\n",
            "Generator loss: 5.43304967880249, Discriminator loss: 0.034162737429142\n",
            "Generator loss: 5.512139797210693, Discriminator loss: 0.022027604281902313\n",
            "Generator loss: 5.417534828186035, Discriminator loss: 0.023483533412218094\n",
            "Generator loss: 5.390253067016602, Discriminator loss: 0.02293318137526512\n",
            "Generator loss: 5.298905849456787, Discriminator loss: 0.03258123993873596\n",
            "Generator loss: 5.257490158081055, Discriminator loss: 0.026636086404323578\n",
            "Generator loss: 5.185394287109375, Discriminator loss: 0.04567388445138931\n",
            "Generator loss: 5.1950225830078125, Discriminator loss: 0.02386525832116604\n",
            "Generator loss: 5.202288627624512, Discriminator loss: 0.038013093173503876\n",
            "Generator loss: 5.259846210479736, Discriminator loss: 0.0217421967536211\n",
            "Generator loss: 5.2147321701049805, Discriminator loss: 0.024762919172644615\n",
            "Generator loss: 5.2323102951049805, Discriminator loss: 0.03722494840621948\n",
            "Generator loss: 5.189831256866455, Discriminator loss: 0.04275684058666229\n",
            "Generator loss: 5.241986274719238, Discriminator loss: 0.03280084580183029\n",
            "Generator loss: 5.456015586853027, Discriminator loss: 0.024124855175614357\n",
            "Generator loss: 5.4800872802734375, Discriminator loss: 0.03228618577122688\n",
            "Generator loss: 5.444648265838623, Discriminator loss: 0.03934171050786972\n",
            "Generator loss: 5.340681552886963, Discriminator loss: 0.02409481257200241\n",
            "Generator loss: 5.261584281921387, Discriminator loss: 0.031676165759563446\n",
            "Generator loss: 5.203180313110352, Discriminator loss: 0.032685548067092896\n",
            "Generator loss: 5.321368217468262, Discriminator loss: 0.025248758494853973\n",
            "Generator loss: 5.366724967956543, Discriminator loss: 0.023070890456438065\n",
            "Generator loss: 5.012905120849609, Discriminator loss: 0.08312082290649414\n",
            "Generator loss: 5.191859245300293, Discriminator loss: 0.04094385728240013\n",
            "Generator loss: 5.350489139556885, Discriminator loss: 0.07922239601612091\n",
            "Generator loss: 5.464061260223389, Discriminator loss: 0.02598928101360798\n",
            "Generator loss: 5.445047855377197, Discriminator loss: 0.026082713156938553\n",
            "Generator loss: 5.390059471130371, Discriminator loss: 0.021766548976302147\n",
            "Generator loss: 5.383891582489014, Discriminator loss: 0.02541782334446907\n",
            "Generator loss: 5.336268901824951, Discriminator loss: 0.0273897647857666\n",
            "Generator loss: 5.389493465423584, Discriminator loss: 0.021753454580903053\n",
            "Generator loss: 5.263199329376221, Discriminator loss: 0.0356629379093647\n",
            "Generator loss: 5.224853992462158, Discriminator loss: 0.03327830880880356\n",
            "Generator loss: 5.144030570983887, Discriminator loss: 0.05945437401533127\n",
            "Generator loss: 5.256130695343018, Discriminator loss: 0.033227864652872086\n",
            "Generator loss: 5.175078392028809, Discriminator loss: 0.05188982933759689\n",
            "Generator loss: 5.299304008483887, Discriminator loss: 0.03849249705672264\n",
            "Generator loss: 5.506606101989746, Discriminator loss: 0.027561001479625702\n",
            "Generator loss: 5.462011814117432, Discriminator loss: 0.04036882519721985\n",
            "Generator loss: 5.25049352645874, Discriminator loss: 0.04369021952152252\n",
            "Generator loss: 5.404646873474121, Discriminator loss: 0.039296481758356094\n",
            "Generator loss: 5.623270511627197, Discriminator loss: 0.037981562316417694\n",
            "Generator loss: 4.897918224334717, Discriminator loss: 0.19370721280574799\n",
            "Generator loss: 5.787944793701172, Discriminator loss: 0.07574987411499023\n",
            "Generator loss: 6.029874801635742, Discriminator loss: 0.02915053814649582\n",
            "Generator loss: 5.470778465270996, Discriminator loss: 0.07789675146341324\n",
            "Generator loss: 5.329209327697754, Discriminator loss: 0.058231040835380554\n",
            "Generator loss: 5.702210426330566, Discriminator loss: 0.04240938276052475\n",
            "Generator loss: 5.718415260314941, Discriminator loss: 0.05872156471014023\n",
            "Generator loss: 5.708876132965088, Discriminator loss: 0.04490819573402405\n",
            "Generator loss: 5.921532154083252, Discriminator loss: 0.037799496203660965\n",
            "Generator loss: 5.861213684082031, Discriminator loss: 0.049735650420188904\n",
            "Generator loss: 5.653319835662842, Discriminator loss: 0.05020315200090408\n",
            "Generator loss: 5.653959274291992, Discriminator loss: 0.030016928911209106\n",
            "Generator loss: 5.746301651000977, Discriminator loss: 0.03496116027235985\n",
            "Generator loss: 5.752017974853516, Discriminator loss: 0.03019193932414055\n",
            "Generator loss: 5.687860488891602, Discriminator loss: 0.022011876106262207\n",
            "Generator loss: 5.4860076904296875, Discriminator loss: 0.05126779153943062\n",
            "Generator loss: 5.58834171295166, Discriminator loss: 0.028033651411533356\n",
            "Generator loss: 5.504838466644287, Discriminator loss: 0.04492064565420151\n",
            "Generator loss: 5.615903854370117, Discriminator loss: 0.021310625597834587\n",
            "Generator loss: 5.552344799041748, Discriminator loss: 0.026221923530101776\n",
            "Generator loss: 5.5094122886657715, Discriminator loss: 0.03886964172124863\n",
            "Generator loss: 5.577113151550293, Discriminator loss: 0.03992686793208122\n",
            "Generator loss: 5.704668045043945, Discriminator loss: 0.034376271069049835\n",
            "Generator loss: 5.820728302001953, Discriminator loss: 0.025997009128332138\n",
            "Generator loss: 5.769406318664551, Discriminator loss: 0.026128269731998444\n",
            "Generator loss: 5.674828052520752, Discriminator loss: 0.029218632727861404\n",
            "Generator loss: 5.61236572265625, Discriminator loss: 0.025566626340150833\n",
            "Generator loss: 5.571702003479004, Discriminator loss: 0.028320202603936195\n",
            "Generator loss: 5.312319755554199, Discriminator loss: 0.04778541624546051\n",
            "Generator loss: 4.345232963562012, Discriminator loss: 0.232080340385437\n",
            "Generator loss: 5.481378078460693, Discriminator loss: 0.08800189197063446\n",
            "Generator loss: 6.191067695617676, Discriminator loss: 0.025806674733757973\n",
            "Generator loss: 5.647212028503418, Discriminator loss: 0.07697542756795883\n",
            "Generator loss: 5.292088508605957, Discriminator loss: 0.026804843917489052\n",
            "Generator loss: 5.41049861907959, Discriminator loss: 0.025424446910619736\n",
            "Generator loss: 5.373730659484863, Discriminator loss: 0.03748123347759247\n",
            "Generator loss: 5.395378112792969, Discriminator loss: 0.06556560099124908\n",
            "Generator loss: 5.210457801818848, Discriminator loss: 0.050855137407779694\n",
            "Generator loss: 5.410831451416016, Discriminator loss: 0.026596151292324066\n",
            "Generator loss: 5.592566967010498, Discriminator loss: 0.02472485601902008\n",
            "Generator loss: 5.521796703338623, Discriminator loss: 0.02769780345261097\n",
            "Generator loss: 5.452511787414551, Discriminator loss: 0.023198679089546204\n",
            "Generator loss: 5.15346622467041, Discriminator loss: 0.04374488443136215\n",
            "Generator loss: 4.776090621948242, Discriminator loss: 0.0742245465517044\n",
            "Generator loss: 5.100112438201904, Discriminator loss: 0.03931448608636856\n",
            "Generator loss: 5.3654327392578125, Discriminator loss: 0.031351037323474884\n",
            "Generator loss: 5.224739074707031, Discriminator loss: 0.04275000840425491\n",
            "Generator loss: 5.069098472595215, Discriminator loss: 0.04611630737781525\n",
            "Generator loss: 4.975886821746826, Discriminator loss: 0.048426054418087006\n",
            "Generator loss: 4.7942328453063965, Discriminator loss: 0.0860167145729065\n",
            "Generator loss: 5.16679573059082, Discriminator loss: 0.05130573734641075\n",
            "Generator loss: 4.5772385597229, Discriminator loss: 0.13109233975410461\n",
            "Generator loss: 4.882184028625488, Discriminator loss: 0.09301258623600006\n",
            "Generator loss: 5.039048671722412, Discriminator loss: 0.08289740979671478\n",
            "Generator loss: 5.187007904052734, Discriminator loss: 0.05554300919175148\n",
            "Generator loss: 5.112252235412598, Discriminator loss: 0.056938283145427704\n",
            "Generator loss: 5.21855354309082, Discriminator loss: 0.03526436537504196\n",
            "Generator loss: 5.218478202819824, Discriminator loss: 0.04242905601859093\n",
            "Generator loss: 5.242387771606445, Discriminator loss: 0.04207934811711311\n",
            "Generator loss: 5.189553260803223, Discriminator loss: 0.04325273633003235\n",
            "Generator loss: 5.1075897216796875, Discriminator loss: 0.041519589722156525\n",
            "Generator loss: 5.089842796325684, Discriminator loss: 0.05707752704620361\n",
            "Generator loss: 4.8921217918396, Discriminator loss: 0.0798625499010086\n",
            "Generator loss: 4.919064521789551, Discriminator loss: 0.0761321410536766\n",
            "Generator loss: 5.116430759429932, Discriminator loss: 0.047036830335855484\n",
            "Generator loss: 5.223855018615723, Discriminator loss: 0.04976974427700043\n",
            "Generator loss: 5.037941932678223, Discriminator loss: 0.05196240544319153\n",
            "Generator loss: 4.880148887634277, Discriminator loss: 0.06370638310909271\n",
            "Generator loss: 5.04369592666626, Discriminator loss: 0.03950578719377518\n",
            "Generator loss: 5.038518905639648, Discriminator loss: 0.07359568029642105\n",
            "Generator loss: 5.243456840515137, Discriminator loss: 0.04514245688915253\n",
            "Generator loss: 4.9317121505737305, Discriminator loss: 0.1441689431667328\n",
            "Generator loss: 5.4074811935424805, Discriminator loss: 0.05476813018321991\n",
            "Generator loss: 5.399717807769775, Discriminator loss: 0.0781446248292923\n",
            "Generator loss: 5.38792085647583, Discriminator loss: 0.05654276907444\n",
            "Generator loss: 5.2550506591796875, Discriminator loss: 0.07848221063613892\n",
            "Generator loss: 5.437016487121582, Discriminator loss: 0.04777923971414566\n",
            "Generator loss: 5.373436450958252, Discriminator loss: 0.053244564682245255\n",
            "Generator loss: 4.972720623016357, Discriminator loss: 0.0816565603017807\n",
            "Generator loss: 5.193118095397949, Discriminator loss: 0.12649013102054596\n",
            "Generator loss: 5.548435211181641, Discriminator loss: 0.08536969125270844\n",
            "Generator loss: 5.817566871643066, Discriminator loss: 0.047787342220544815\n",
            "Generator loss: 5.782384395599365, Discriminator loss: 0.037165284156799316\n",
            "Generator loss: 4.717329978942871, Discriminator loss: 0.11095119267702103\n",
            "Generator loss: 4.918963432312012, Discriminator loss: 0.09030428528785706\n",
            "Generator loss: 5.431205749511719, Discriminator loss: 0.07548011839389801\n",
            "Generator loss: 5.419040203094482, Discriminator loss: 0.05597996339201927\n",
            "Generator loss: 5.026353359222412, Discriminator loss: 0.10157313197851181\n",
            "Generator loss: 4.8516526222229, Discriminator loss: 0.07736828923225403\n",
            "Generator loss: 4.633800029754639, Discriminator loss: 0.12016148865222931\n",
            "Generator loss: 5.0482378005981445, Discriminator loss: 0.07129891216754913\n",
            "Generator loss: 5.269097328186035, Discriminator loss: 0.07060843706130981\n",
            "Generator loss: 5.404824256896973, Discriminator loss: 0.055061809718608856\n",
            "Generator loss: 5.191933631896973, Discriminator loss: 0.04631637781858444\n",
            "Generator loss: 5.169130325317383, Discriminator loss: 0.05581296235322952\n",
            "Generator loss: 5.198990821838379, Discriminator loss: 0.04352167993783951\n",
            "Generator loss: 4.823576927185059, Discriminator loss: 0.10404197126626968\n",
            "Generator loss: 5.126387596130371, Discriminator loss: 0.04889223352074623\n",
            "Generator loss: 5.343867301940918, Discriminator loss: 0.04847493767738342\n",
            "Generator loss: 5.554358959197998, Discriminator loss: 0.04135792702436447\n",
            "Generator loss: 5.501275539398193, Discriminator loss: 0.028764840215444565\n",
            "Generator loss: 5.2247185707092285, Discriminator loss: 0.050335824489593506\n",
            "Generator loss: 4.769018173217773, Discriminator loss: 0.1870507150888443\n",
            "Generator loss: 5.2191057205200195, Discriminator loss: 0.05790891498327255\n",
            "Generator loss: 5.738571643829346, Discriminator loss: 0.02812732756137848\n",
            "Generator loss: 5.488123416900635, Discriminator loss: 0.050689563155174255\n",
            "Generator loss: 4.9018235206604, Discriminator loss: 0.06905712187290192\n",
            "Generator loss: 5.031381607055664, Discriminator loss: 0.03248263895511627\n",
            "Generator loss: 5.258939266204834, Discriminator loss: 0.040223244577646255\n",
            "Generator loss: 5.321313858032227, Discriminator loss: 0.06258784234523773\n",
            "Generator loss: 5.307511329650879, Discriminator loss: 0.05595904961228371\n",
            "Generator loss: 5.301437854766846, Discriminator loss: 0.03452908247709274\n",
            "Generator loss: 4.708739757537842, Discriminator loss: 0.11873030662536621\n",
            "Generator loss: 5.013504981994629, Discriminator loss: 0.047176871448755264\n",
            "Generator loss: 5.456974506378174, Discriminator loss: 0.037043482065200806\n",
            "Generator loss: 5.577455043792725, Discriminator loss: 0.029436014592647552\n",
            "Generator loss: 5.49631929397583, Discriminator loss: 0.03127875179052353\n",
            "Generator loss: 5.381874084472656, Discriminator loss: 0.0277820136398077\n",
            "Generator loss: 5.220590114593506, Discriminator loss: 0.03593787923455238\n",
            "Generator loss: 5.1314239501953125, Discriminator loss: 0.04296562075614929\n",
            "Generator loss: 5.384753227233887, Discriminator loss: 0.044298168271780014\n",
            "Generator loss: 4.096318244934082, Discriminator loss: 0.271100252866745\n",
            "Generator loss: 6.167903423309326, Discriminator loss: 0.140752911567688\n",
            "Generator loss: 5.280606746673584, Discriminator loss: 0.18864595890045166\n",
            "Generator loss: 4.801761150360107, Discriminator loss: 0.06728901714086533\n",
            "Generator loss: 5.750549793243408, Discriminator loss: 0.04581020027399063\n",
            "Generator loss: 5.962529182434082, Discriminator loss: 0.0447952039539814\n",
            "Generator loss: 5.167895317077637, Discriminator loss: 0.08642621338367462\n",
            "Generator loss: 5.114744663238525, Discriminator loss: 0.0416501984000206\n",
            "Generator loss: 5.450305461883545, Discriminator loss: 0.04452294856309891\n",
            "Generator loss: 4.761743545532227, Discriminator loss: 0.09365246444940567\n",
            "Generator loss: 4.974979877471924, Discriminator loss: 0.06197158992290497\n",
            "Generator loss: 5.100928783416748, Discriminator loss: 0.06309043616056442\n",
            "Generator loss: 4.091129302978516, Discriminator loss: 0.1579059362411499\n",
            "Generator loss: 5.777969837188721, Discriminator loss: 0.08190988004207611\n",
            "Generator loss: 5.979842662811279, Discriminator loss: 0.04493265599012375\n",
            "Generator loss: 5.219305515289307, Discriminator loss: 0.06806899607181549\n",
            "Generator loss: 4.726337432861328, Discriminator loss: 0.04486798122525215\n",
            "Generator loss: 5.0608415603637695, Discriminator loss: 0.04419546574354172\n",
            "Generator loss: 5.345047950744629, Discriminator loss: 0.03608923777937889\n",
            "Generator loss: 5.555917263031006, Discriminator loss: 0.019538521766662598\n",
            "Generator loss: 4.977987289428711, Discriminator loss: 0.048731520771980286\n",
            "Generator loss: 4.403090000152588, Discriminator loss: 0.06486353278160095\n",
            "Generator loss: 4.635629177093506, Discriminator loss: 0.08429715037345886\n",
            "Generator loss: 5.423389911651611, Discriminator loss: 0.031563930213451385\n",
            "Generator loss: 4.576703071594238, Discriminator loss: 0.15123486518859863\n",
            "Generator loss: 4.665577411651611, Discriminator loss: 0.05147864669561386\n",
            "Generator loss: 3.7757885456085205, Discriminator loss: 0.20665541291236877\n",
            "Generator loss: 5.624169826507568, Discriminator loss: 0.09606562554836273\n",
            "Generator loss: 5.888640880584717, Discriminator loss: 0.03836192190647125\n",
            "Generator loss: 4.078020095825195, Discriminator loss: 0.19927674531936646\n",
            "Generator loss: 4.015267372131348, Discriminator loss: 0.09903483837842941\n",
            "Generator loss: 5.7391743659973145, Discriminator loss: 0.07066741585731506\n",
            "Generator loss: 5.256181240081787, Discriminator loss: 0.09899399429559708\n",
            "Generator loss: 4.685966968536377, Discriminator loss: 0.048015154898166656\n",
            "Generator loss: 5.0670881271362305, Discriminator loss: 0.04038868471980095\n",
            "Generator loss: 4.8463335037231445, Discriminator loss: 0.07294230163097382\n",
            "Generator loss: 3.2753496170043945, Discriminator loss: 0.249175027012825\n",
            "Generator loss: 5.895267486572266, Discriminator loss: 0.15661928057670593\n",
            "Generator loss: 6.567449569702148, Discriminator loss: 0.02844831719994545\n",
            "Generator loss: 5.100470542907715, Discriminator loss: 0.22785204648971558\n",
            "Generator loss: 4.260035991668701, Discriminator loss: 0.05663985013961792\n",
            "Generator loss: 5.007621765136719, Discriminator loss: 0.059404414147138596\n",
            "Generator loss: 5.010316371917725, Discriminator loss: 0.07178603112697601\n",
            "Generator loss: 5.376736164093018, Discriminator loss: 0.04982813820242882\n",
            "Generator loss: 4.271640777587891, Discriminator loss: 0.1928536742925644\n",
            "Generator loss: 7.285735130310059, Discriminator loss: 0.175472691655159\n",
            "Generator loss: 4.5582594871521, Discriminator loss: 0.3771534562110901\n",
            "Generator loss: 4.194047927856445, Discriminator loss: 0.19324475526809692\n",
            "Generator loss: 5.600035667419434, Discriminator loss: 0.1921224296092987\n",
            "Generator loss: 5.8114333152771, Discriminator loss: 0.09657201915979385\n",
            "Generator loss: 4.752713203430176, Discriminator loss: 0.10204467177391052\n",
            "Generator loss: 4.709878921508789, Discriminator loss: 0.12163439393043518\n",
            "Generator loss: 5.469686985015869, Discriminator loss: 0.17154249548912048\n",
            "Generator loss: 5.475969314575195, Discriminator loss: 0.06108175963163376\n",
            "Generator loss: 4.975882530212402, Discriminator loss: 0.07894996553659439\n",
            "Generator loss: 4.257957458496094, Discriminator loss: 0.1620461493730545\n",
            "Generator loss: 5.559041976928711, Discriminator loss: 0.22646132111549377\n",
            "Generator loss: 5.530118942260742, Discriminator loss: 0.13507510721683502\n",
            "Generator loss: 4.973522186279297, Discriminator loss: 0.09689433872699738\n",
            "Generator loss: 4.628932476043701, Discriminator loss: 0.11279220879077911\n",
            "Generator loss: 4.713581085205078, Discriminator loss: 0.11650122702121735\n",
            "Generator loss: 5.036758899688721, Discriminator loss: 0.06614702939987183\n",
            "Generator loss: 4.545401096343994, Discriminator loss: 0.1886707991361618\n",
            "Generator loss: 4.621329307556152, Discriminator loss: 0.08091944456100464\n",
            "Generator loss: 4.953304290771484, Discriminator loss: 0.05655072629451752\n",
            "Generator loss: 5.205264091491699, Discriminator loss: 0.031088676303625107\n",
            "Generator loss: 5.131842136383057, Discriminator loss: 0.038896746933460236\n",
            "Generator loss: 4.1736955642700195, Discriminator loss: 0.1681824028491974\n",
            "Generator loss: 4.5312957763671875, Discriminator loss: 0.05567335709929466\n",
            "Generator loss: 4.929892539978027, Discriminator loss: 0.05972465127706528\n",
            "Generator loss: 4.750975131988525, Discriminator loss: 0.0835530012845993\n",
            "Generator loss: 4.6479926109313965, Discriminator loss: 0.05064964294433594\n",
            "Generator loss: 4.85689640045166, Discriminator loss: 0.03862351179122925\n",
            "Generator loss: 4.626262664794922, Discriminator loss: 0.06903096288442612\n",
            "Generator loss: 4.4771318435668945, Discriminator loss: 0.06098350137472153\n",
            "Generator loss: 4.746373176574707, Discriminator loss: 0.04668522626161575\n",
            "Generator loss: 4.58021879196167, Discriminator loss: 0.10602564364671707\n",
            "Generator loss: 4.813580513000488, Discriminator loss: 0.04113948345184326\n",
            "Generator loss: 4.879645347595215, Discriminator loss: 0.056207381188869476\n",
            "Generator loss: 4.648077964782715, Discriminator loss: 0.09810870885848999\n",
            "Generator loss: 4.629236221313477, Discriminator loss: 0.07821129262447357\n",
            "Generator loss: 5.099242210388184, Discriminator loss: 0.05319296568632126\n",
            "Generator loss: 5.202969074249268, Discriminator loss: 0.054243072867393494\n",
            "Generator loss: 4.812648296356201, Discriminator loss: 0.06538840383291245\n",
            "Generator loss: 3.6296231746673584, Discriminator loss: 0.1489740014076233\n",
            "Generator loss: 4.944433212280273, Discriminator loss: 0.08732583373785019\n",
            "Generator loss: 5.513100624084473, Discriminator loss: 0.05575493723154068\n",
            "Generator loss: 5.473994255065918, Discriminator loss: 0.05919802188873291\n",
            "Generator loss: 5.4379563331604, Discriminator loss: 0.018661096692085266\n",
            "Generator loss: 5.076175689697266, Discriminator loss: 0.04327826946973801\n",
            "Generator loss: 5.016788005828857, Discriminator loss: 0.023148998618125916\n",
            "Generator loss: 4.888503551483154, Discriminator loss: 0.037471555173397064\n",
            "Generator loss: 4.579333782196045, Discriminator loss: 0.05179329589009285\n",
            "Generator loss: 3.6909613609313965, Discriminator loss: 0.2285768985748291\n",
            "Generator loss: 4.292328357696533, Discriminator loss: 0.10065211355686188\n",
            "Generator loss: 4.692997932434082, Discriminator loss: 0.08101753890514374\n",
            "Generator loss: 5.114581108093262, Discriminator loss: 0.03847227990627289\n",
            "Generator loss: 5.243664264678955, Discriminator loss: 0.032792121171951294\n",
            "Generator loss: 4.540781021118164, Discriminator loss: 0.05818425118923187\n",
            "Generator loss: 4.595729827880859, Discriminator loss: 0.033917203545570374\n",
            "Generator loss: 4.332765579223633, Discriminator loss: 0.14591610431671143\n",
            "Generator loss: 4.360019683837891, Discriminator loss: 0.061103060841560364\n",
            "Generator loss: 4.7472028732299805, Discriminator loss: 0.04254043102264404\n",
            "Generator loss: 5.052610874176025, Discriminator loss: 0.03603840991854668\n",
            "Generator loss: 4.770224571228027, Discriminator loss: 0.04106459766626358\n",
            "Generator loss: 4.846418857574463, Discriminator loss: 0.027447916567325592\n",
            "Generator loss: 4.92772102355957, Discriminator loss: 0.03422660380601883\n",
            "Generator loss: 4.940798759460449, Discriminator loss: 0.031135667115449905\n",
            "Generator loss: 4.788118362426758, Discriminator loss: 0.05130898952484131\n",
            "Generator loss: 2.629868507385254, Discriminator loss: 0.2419484257698059\n",
            "Generator loss: 5.612795352935791, Discriminator loss: 0.13473889231681824\n",
            "Generator loss: 6.363821029663086, Discriminator loss: 0.04975944012403488\n",
            "Generator loss: 5.931947231292725, Discriminator loss: 0.039462894201278687\n",
            "Generator loss: 5.267351150512695, Discriminator loss: 0.03298274800181389\n",
            "Generator loss: 4.509401321411133, Discriminator loss: 0.03500054404139519\n",
            "Generator loss: 4.197140216827393, Discriminator loss: 0.06458324939012527\n",
            "Generator loss: 4.265583038330078, Discriminator loss: 0.08034805208444595\n",
            "Generator loss: 4.517125606536865, Discriminator loss: 0.055326834321022034\n",
            "Generator loss: 4.986663341522217, Discriminator loss: 0.026660025119781494\n",
            "Generator loss: 4.681794166564941, Discriminator loss: 0.09200744330883026\n",
            "Generator loss: 4.428743839263916, Discriminator loss: 0.04953908175230026\n",
            "Generator loss: 4.257942199707031, Discriminator loss: 0.06777171790599823\n",
            "Generator loss: 4.321422576904297, Discriminator loss: 0.07045022398233414\n",
            "Generator loss: 4.400478363037109, Discriminator loss: 0.05840739607810974\n",
            "Generator loss: 4.763509273529053, Discriminator loss: 0.03740586340427399\n",
            "Generator loss: 4.360658645629883, Discriminator loss: 0.08663273602724075\n",
            "Generator loss: 4.503237724304199, Discriminator loss: 0.04439232870936394\n",
            "Generator loss: 4.7462005615234375, Discriminator loss: 0.03892196714878082\n",
            "Generator loss: 4.499741554260254, Discriminator loss: 0.06737236678600311\n",
            "Generator loss: 4.6692214012146, Discriminator loss: 0.03160654008388519\n",
            "Generator loss: 4.5784783363342285, Discriminator loss: 0.05054071545600891\n",
            "Generator loss: 4.743865013122559, Discriminator loss: 0.029489601030945778\n",
            "Generator loss: 4.401678562164307, Discriminator loss: 0.08845461159944534\n",
            "Generator loss: 4.7074785232543945, Discriminator loss: 0.03172212839126587\n",
            "Generator loss: 4.594980239868164, Discriminator loss: 0.10085280239582062\n",
            "Generator loss: 4.051208019256592, Discriminator loss: 0.08616704493761063\n",
            "Generator loss: 4.20664644241333, Discriminator loss: 0.08508744090795517\n",
            "Generator loss: 5.204258441925049, Discriminator loss: 0.03821474313735962\n",
            "Generator loss: 5.554457187652588, Discriminator loss: 0.0261993408203125\n",
            "Generator loss: 5.439034938812256, Discriminator loss: 0.022536750882864\n",
            "Generator loss: 5.018100738525391, Discriminator loss: 0.02921370603144169\n",
            "Generator loss: 4.370583534240723, Discriminator loss: 0.06868307292461395\n",
            "Generator loss: 4.364025115966797, Discriminator loss: 0.09063218533992767\n",
            "Generator loss: 4.829824924468994, Discriminator loss: 0.1165844202041626\n",
            "Generator loss: 4.716407299041748, Discriminator loss: 0.09681608527898788\n",
            "Generator loss: 5.1212921142578125, Discriminator loss: 0.03700763359665871\n",
            "Generator loss: 5.177148342132568, Discriminator loss: 0.05592166259884834\n",
            "Generator loss: 5.014157295227051, Discriminator loss: 0.04266830533742905\n",
            "Generator loss: 5.217738151550293, Discriminator loss: 0.029778627678751945\n",
            "Generator loss: 5.228421688079834, Discriminator loss: 0.0558546781539917\n",
            "Generator loss: 5.213362216949463, Discriminator loss: 0.046083200722932816\n",
            "Generator loss: 5.452132701873779, Discriminator loss: 0.04143432900309563\n",
            "Generator loss: 5.21091365814209, Discriminator loss: 0.07285213470458984\n",
            "Generator loss: 4.857193946838379, Discriminator loss: 0.15018154680728912\n",
            "Generator loss: 5.881307601928711, Discriminator loss: 0.1063445657491684\n",
            "Generator loss: 5.836711883544922, Discriminator loss: 0.09402363002300262\n",
            "Generator loss: 5.320423126220703, Discriminator loss: 0.1417454481124878\n",
            "Generator loss: 6.696133613586426, Discriminator loss: 0.10274779796600342\n",
            "Generator loss: 6.6302080154418945, Discriminator loss: 0.03678324073553085\n",
            "Generator loss: 5.875497817993164, Discriminator loss: 0.04952847957611084\n",
            "Generator loss: 6.475021839141846, Discriminator loss: 0.11809881031513214\n",
            "Generator loss: 6.6668500900268555, Discriminator loss: 0.10120953619480133\n",
            "Generator loss: 5.9186601638793945, Discriminator loss: 0.3001794219017029\n",
            "Generator loss: 6.101622581481934, Discriminator loss: 0.1935749650001526\n",
            "Generator loss: 7.076788425445557, Discriminator loss: 0.10155981779098511\n",
            "Generator loss: 6.7751264572143555, Discriminator loss: 0.03811222314834595\n",
            "Generator loss: 5.117123126983643, Discriminator loss: 0.22865843772888184\n",
            "Generator loss: 7.594449996948242, Discriminator loss: 0.18935763835906982\n",
            "Generator loss: 5.630895614624023, Discriminator loss: 0.44913026690483093\n",
            "Generator loss: 4.963704586029053, Discriminator loss: 0.18678291141986847\n",
            "Generator loss: 7.224547386169434, Discriminator loss: 0.18865220248699188\n",
            "Generator loss: 6.753317356109619, Discriminator loss: 0.1403008997440338\n",
            "Generator loss: 4.8916521072387695, Discriminator loss: 0.12446601688861847\n",
            "Generator loss: 4.956168174743652, Discriminator loss: 0.1437932699918747\n",
            "Generator loss: 5.933291912078857, Discriminator loss: 0.11765891313552856\n",
            "Generator loss: 5.396115303039551, Discriminator loss: 0.1701439917087555\n",
            "Generator loss: 5.663756847381592, Discriminator loss: 0.07888137549161911\n",
            "Generator loss: 5.419971942901611, Discriminator loss: 0.13400518894195557\n",
            "Generator loss: 5.815606117248535, Discriminator loss: 0.12264759838581085\n",
            "Generator loss: 4.582155227661133, Discriminator loss: 0.2653353214263916\n",
            "Generator loss: 6.406204700469971, Discriminator loss: 0.21101877093315125\n",
            "Generator loss: 6.159986972808838, Discriminator loss: 0.09342999756336212\n",
            "Generator loss: 5.115566253662109, Discriminator loss: 0.08882009983062744\n",
            "Generator loss: 4.6637187004089355, Discriminator loss: 0.2738804817199707\n",
            "Generator loss: 7.197038650512695, Discriminator loss: 0.176160529255867\n",
            "Generator loss: 6.755764961242676, Discriminator loss: 0.16955895721912384\n",
            "Generator loss: 5.170899868011475, Discriminator loss: 0.06986001133918762\n",
            "Generator loss: 4.8553056716918945, Discriminator loss: 0.11118120700120926\n",
            "Generator loss: 5.007930278778076, Discriminator loss: 0.23619835078716278\n",
            "Generator loss: 5.277070045471191, Discriminator loss: 0.16309736669063568\n",
            "Generator loss: 5.305983543395996, Discriminator loss: 0.09609505534172058\n",
            "Generator loss: 4.718358039855957, Discriminator loss: 0.10462899506092072\n",
            "Generator loss: 5.711175918579102, Discriminator loss: 0.09272652864456177\n",
            "Generator loss: 4.831203937530518, Discriminator loss: 0.13867273926734924\n",
            "Generator loss: 4.538884162902832, Discriminator loss: 0.13837677240371704\n",
            "Generator loss: 5.701361179351807, Discriminator loss: 0.10091375559568405\n",
            "Generator loss: 5.358002662658691, Discriminator loss: 0.07479443401098251\n",
            "Generator loss: 4.416439056396484, Discriminator loss: 0.08478664606809616\n",
            "Generator loss: 4.506972312927246, Discriminator loss: 0.07481777667999268\n",
            "Generator loss: 5.220898628234863, Discriminator loss: 0.06923127174377441\n",
            "Generator loss: 4.6071457862854, Discriminator loss: 0.16945800185203552\n",
            "Generator loss: 3.1648197174072266, Discriminator loss: 0.19842758774757385\n",
            "Generator loss: 6.046562671661377, Discriminator loss: 0.20685705542564392\n",
            "Generator loss: 6.613384246826172, Discriminator loss: 0.037981998175382614\n",
            "Generator loss: 5.0128374099731445, Discriminator loss: 0.13834057748317719\n",
            "Generator loss: 3.6405797004699707, Discriminator loss: 0.08072464168071747\n",
            "Generator loss: 4.938414096832275, Discriminator loss: 0.08963285386562347\n",
            "Generator loss: 5.434360027313232, Discriminator loss: 0.04282130300998688\n",
            "Generator loss: 5.420712947845459, Discriminator loss: 0.029359156265854836\n",
            "Generator loss: 4.965109825134277, Discriminator loss: 0.0340023972094059\n",
            "Generator loss: 4.643895149230957, Discriminator loss: 0.03758550435304642\n",
            "Generator loss: 3.9937081336975098, Discriminator loss: 0.20458263158798218\n",
            "Generator loss: 4.073277950286865, Discriminator loss: 0.14072285592556\n",
            "Generator loss: 5.521363735198975, Discriminator loss: 0.0718684047460556\n",
            "Generator loss: 4.584036350250244, Discriminator loss: 0.200726717710495\n",
            "Generator loss: 4.653729438781738, Discriminator loss: 0.05726112052798271\n",
            "Generator loss: 5.066162109375, Discriminator loss: 0.048310019075870514\n",
            "Generator loss: 5.354362487792969, Discriminator loss: 0.028009265661239624\n",
            "Generator loss: 5.31358003616333, Discriminator loss: 0.026698939502239227\n",
            "Generator loss: 4.747027397155762, Discriminator loss: 0.057709816843271255\n",
            "Generator loss: 4.404138565063477, Discriminator loss: 0.08096058666706085\n",
            "Generator loss: 4.733477592468262, Discriminator loss: 0.06121348217129707\n",
            "Generator loss: 4.641002655029297, Discriminator loss: 0.08385517448186874\n",
            "Generator loss: 4.8587541580200195, Discriminator loss: 0.05877012014389038\n",
            "Generator loss: 5.197120666503906, Discriminator loss: 0.03674481809139252\n",
            "Generator loss: 4.8047075271606445, Discriminator loss: 0.07585036754608154\n",
            "Generator loss: 4.92683744430542, Discriminator loss: 0.047858141362667084\n",
            "Generator loss: 5.017580032348633, Discriminator loss: 0.059171516448259354\n",
            "Generator loss: 4.719743728637695, Discriminator loss: 0.09392759203910828\n",
            "Generator loss: 4.716315746307373, Discriminator loss: 0.09568143635988235\n",
            "Generator loss: 5.46368932723999, Discriminator loss: 0.06734192371368408\n",
            "Generator loss: 5.796172618865967, Discriminator loss: 0.04663971811532974\n",
            "Generator loss: 5.694539546966553, Discriminator loss: 0.041306983679533005\n",
            "Generator loss: 5.4776716232299805, Discriminator loss: 0.10079686343669891\n",
            "Generator loss: 4.947770118713379, Discriminator loss: 0.1948835849761963\n",
            "Generator loss: 5.571742534637451, Discriminator loss: 0.06318236887454987\n",
            "Generator loss: 5.549615859985352, Discriminator loss: 0.2783045172691345\n",
            "Generator loss: 5.561289310455322, Discriminator loss: 0.027583634480834007\n",
            "Generator loss: 4.834190368652344, Discriminator loss: 0.10574638843536377\n",
            "Generator loss: 4.772589683532715, Discriminator loss: 0.09990203380584717\n",
            "Generator loss: 4.9883713722229, Discriminator loss: 0.06542234867811203\n",
            "Generator loss: 5.393363952636719, Discriminator loss: 0.04332660883665085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "generator.eval()\n",
        "with torch.no_grad():\n",
        "    z = torch.randn(8, latent_dim, device=device)\n",
        "    generated = generator(z).detach().cpu()\n",
        "    grid = torchvision.utils.make_grid(generated, nrow=4, normalize=True)\n",
        "    plt.imshow(np.transpose(grid, (1, 2, 0)))\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "X9wxFjWHlEAt",
        "outputId": "a0d16ba7-337e-441c-dcac-6271a7d09ff9"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "# 1563 batches exist\n",
        "# for image_batch in train_dataloader:\n",
        "#     # train_step(image_batch)\n",
        "#     print(image_batch[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "z2_66bMvYynz",
        "outputId": "aca59c91-b1cc-47f9-aeab-7f1eac63222d"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([16, 3, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(generator.state_dict(), 'generator.pth')\n",
        "torch.save(discriminator.state_dict(), 'discriminator.pth')"
      ],
      "metadata": {
        "id": "MqsPuGGi9FV9"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # For one pass:\n",
        "# noise_dim = 100\n",
        "# real_images_batch = next(iter(train_dataloader))\n",
        "# noise = torch.randn(32, noise_dim)\n",
        "\n",
        "# # Training discriminator\n",
        "# optimizer_D.zero_grad()\n",
        "# fake_images = generator(noise)\n",
        "# real_output = discriminator(real_images_batch[0])\n",
        "# fake_output = discriminator(fake_images.detach())\n",
        "# disc_loss = discriminator_loss(real_output, fake_output)\n",
        "# disc_loss.backward()\n",
        "# optimizer_D.step()\n",
        "\n",
        "# # Training generator\n",
        "# optimizer_G.zero_grad()\n",
        "# generated_images = generator(noise)\n",
        "# fake_output = discriminator(generated_images)\n",
        "# gen_loss = generator_loss(fake_output)\n",
        "# gen_loss.backward()\n",
        "# optimizer_G.step()"
      ],
      "metadata": {
        "id": "L8fgRnoWifwV"
      },
      "execution_count": 130,
      "outputs": []
    }
  ]
}